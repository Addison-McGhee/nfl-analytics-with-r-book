[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to NFL Analytics with R",
    "section": "",
    "text": "1 Introduction\nOn April 27, 2020, Ben Baldwin hit send on a Tweet that announced the birth of nflfastR, an R package designed to scrape NFL play-by-play data, allowing the end-user to access it at speeds quicker than similar predecessors (hence the name).\nThanks to the work of multiple people (@mrcaseB, @benbbaldwin, @TanHo, @LeeSharpeNFL, and @thomas_mock … to just name a few), the process of getting started with advanced analytics using NFL data is now easier than ever.\nThat said, and without getting too far into the weeds of the history behind it all, the above-mentioned people are responsible in some shape or form for the current status of the nflverse, which is a superb collection of data and R-based packages that allows anybody the ability to access deeply robust NFL data as far back as the 1999 season.\nThe nflverse as we know it today was initially birthed from the nflscrapR project, which was started by the Carnegie Mellon University student and profess duo of Maksim Horowitz and Sam Ventura. After Horowitz graduated - and got hired by the Atlanta Hawks - the nflscrapR package was taken over by fellow CMU student Ron Yorko (who would go on to receive his Ph.D. from the Statistics and Data Science program). The trio’s work on nflscrapR ultimately led to a peer-reviewed paper titled “nflWAR: A Reproducible Method for Offensive Player Evaluation in Football.” Ultimately, the nflscrapR project came to an end when the specific .json feed used to gather NFL data changed. At this point, Ben Baldwin and Sebastian Carl had already built upon the nflscrapR project’s foundations to create nflfastR. Ron officially marked the end of the nflscrapR era and the beginning of the nflfastR era with a tweet on September 14, 2020:1\nAs a reply to his first tweet about the nflfastR project, Ben explained that he created the original function to scrape NFL data for the creation of his NFL analytics website. Thankfully, he and Seb did not keep the creation to themselves and released nflfastR to the public. Because of the “open source” nature of R and R packages, a laundry list of companion packages quickly developed alongside nflfastR. The original nflfastR package is now part of the larger nflverse of packages that drive the NFL analytics community on Twitter and beyond.\nThe creation of the nflverse allowed for anybody interested in NFL analytics to easily access data, manipulate it to their liking, and release their visualizations and/or metrics to the wider public. In fact, it is now a regular occurrence for somebody to advance their R programming ability because of the nflverse and then go on to win the Big Data Bowl. As of the 2022 version of the Big Data Bowl, over “30 participants have been hired to work in data and analytics roles in sports, including 22 that were hired in football” (“Big Data Bowl: The Annual Analytics Contest Explores Statistical Innovations in Football,” n.d.). Most recently, the Chargers hired 2020 participate Alex Stern and the Chiefs hired Marc Richards, a member of the winning 2021 team, as a Football Research Analyst.\nKevin Clark, in a 2018 article for The Ringer, explained that despite not being as obvious as the sabermetrics movement in baseball, the analytics movement in the NFL is “happening in front of you all the time.” The use of analytics in the NFL did, however, predate Clark’s article. In 2014, Eagles head coach Doug Pederson explained that all decisions made by the organization - from game planning to draft strategy - were to be informed by hard data and analytics. Leading this early adoption of analytics, and reporting directly to team Vice President Howie Roseman, were Joe Douglas and Alec Halaby, “a 31-year-old Harvard grad with a job description” that had an emphasis on “integrating traditional and analytical methods in football decision-making.” The result? A “blending of old-school scouting and newer approaches” that were often only seen in other leagues, such as the NBA and MLB (Rosenthal 2018). Pederson believed in and trusted the team’s approach to analytics so much that a direct line of communication was created between the two during games, with the analytics department providing the head coach with math-based recommendations for any scenario Pederson requested (Awbrey 2020).2\nIn just under five years time since the publishing of that article, it has become hard to ignore the analytic movement within the NFL. Yet, there is still so much growth to happen in the marriage between the NFL and advanced metrics. For example, there is no denying that the sabermetrics movement drastically “altered baseball’s DNA” Heifetz (2019)]. Moreover, as explained in Seth Partnow’s outstanding The Midrange Theory: Basketball’s Evolution in the Age of Analytics, the analytics movement in the NBA essentially killed the midrange shot (briefly: it is more beneficial to try to work the ball in directly under the basket (for a high-percentage shot) or to take the 3-pointer, as the possible additional point is statistically worth more despite the lower success probability as opposed a 2-point midrange shot) as well as the traditional, “old-school” center position.\nCompared to both the NBA and MLB, the NFL is playing catch up in analytics driving changes equivalent to the death of the midrange shot or the plethora of additional tactics and changes to baseball because of sabermetrics. Joe Banner, who served as the President of the Eagles from 2001-2012 and then the Chief Executive Officer of the Browns from 2012-2013, explained that some of the hesitation to incorporate analytics into NFL game planning was a result of the game being “very much driven by conventional wisdom to an extreme degree” (Fortier 2020). Perhaps nobody encapsulates this better than Pittsburgh Steelers Head Coach Mike Tomlin. When asked about his position on analytics during the 2015 season, Tomlin explained:\nGiven that Tomlin’s quote is from 2015, perhaps the Steelers pivoted since and are now more analytically inclined. That does not seem to be the case. In a poll of NFL analytics staffers conducted by ESPN, the Steelers were voted as one of the least analytically advanced teams in the league.\nThere is large gap between the least analytically inclined teams (Washington, Tennessee, Cincinnati, New York Giants, and Pittsburgh) and those voted as the most analytically inclined (Baltimore, Cleveland, Philadelphia, and Houston). In the ESPN poll, the Browns were voted as the analytics department producing the highest level of work. One of those polled spoke to the fact that much of this outstanding work is a result of General Manager Andrew Berry being a “true believer,” explaining that he is one of the “rare guys you’ll come across in life where you think to yourself, ‘Man, this guy thinks at a different level. Just pure genius.’ He’s one of them.”\nIn his article for the Washington Post, Sam Fortier argues that many teams became inspired to more intimately introduce analytics into game planning and on-field decisions after the 2017 season. On their run to becoming Super Bowl Champions, the Philadelphia Eagles were aggressive on 4th down, going for it 26 times during the season and converting on 17 of those for a conversion percentage of 65.4%. A quick examination and visualization of data highlights the absolutely staggering increase in 4th aggressiveness among NFL head coaches from 2017-2021:\nThere has been a 96.3% increase in the number of 4th down attempts from just 2017 to 2021. In fact, the numbers may actually be higher as I was quite conservative in building the above plot by only considering those 4th down attempts that took place when the offensive team had between a 5-to-95% winning probability and those prior to the two-minute warning of either half. Even with those conservative limitations, the increase is staggering. The numbers, however, support this aggression. During week one of both the 2020 and 2021 season, not going for it on 4th down “cost teams a cumulative 170 percentage points of win probability” (Bushnell 2021).\nBen Baldwin, using the nfl4th package that is part of the nflverse, tracked the shift in NFL coaching mentality regarding 4th down decisions by comparing 2014’s “go for it percentage” against the same for 2020. When compared to the 2014 season, NFL coaches are now much more in agreement with analytics on when to “go for it” on 4th down in relation to the expected gain in win probability.\nIt should not be surprising then, considering Mike Tomlin’s quote from above and other NFL analytics staffers voting the Steelers as one of the least analytically driven teams in the league, that Pittsburgh lost the most win probability by either kicking or punting in “go for it” situations during the 2020 NFL season. On the other end, the Ravens and Browns - two teams voted as the most analytically inclined - are the two best organizations at knowing when to “go for it” on 4th down based on win probability added. There seems to be a defined relationship between teams buying into analytics and those who do not:\nThe NFL’s turn towards more aggressive 4th-down decisions is just one of the many analytics-driven changes occurring in the league. Another significant example is Defense-Adjusted Value over Average (or DVOA), a formula created by Aaron Schatz, now the editor in chief of Football Outsiders, that sought to challenge the notion that teams should, first, establish the running game in order to open up the passing game. Some of these changes are apparent on televisions screens on Sunday afternoons in the Fall, while others are occurring behind the scenes (analytics departments working on scouting and draft preparation, for example). Indeed, the use of analytics in the NFL is not as tightly ingrained as we see in other prominent leagues. And we must remember that there are certainly continued hold outs among some NFL coaches (like Mike Tomlin).\nDespite some coaching hold outs on fully embracing analytics, the “thirst for knowledge in football is as excessive as any other sport and the desire to get the most wins per dollar is just as high.” As the pipeline of data continues to grow, both internally in the league and data that becomes publicly accessible, “smart teams will continue to leave no rock unturned as they push the limits on how far data can take them.” Joe Banner explained that while the NFL has long been a league of coaches saying “well, that is the way we’ve always done it,” the league is ripe for a major shift (Bechtold 2021).\nBanner’s belief that those teams searching for every competitive advantage will “leave no rock unturned” is the driving force behind this book. For all intents and purposes, the age of analytics in the NFL is still in its infancy. Turning back, again, to the 2017 season, the Eagles’ management praised and credited the team’s analytics department as part of the reason they were able to win Super Bowl LII. Doing so Danney Heifetz argues, “changed the language of football.” The NFL, he explains, is a “copycat league” and, as witnessed with the increase in 4th down aggressiveness since 2017, teams immediately began to carbon copy Philadelphia’s approach to folding traditional football strategy with a new age analytics approach. Because of the modernity of this relationship between long-held football dogmas and analytics, nobody can be quite sure what other impacts it will create on the gamesmanship of football.\nHowever, as Heifetz opines, both the NBA and MLB can serve as a roadmap to where analytics will take the NFL. Importantly, the NFL’s relationship with analytics is still in its “first frontier of what will likely be a sweeping change over the next two decades.” Because of this, we cannot be sure what the next major impact analytics will make, nor when it may occur. But, with the ever-growing amount of publicly accessible data, it is only a matter of time until it is discovered. For example, in an interview with Heifetz, Brian Burke - one of the forefather’s of NFL analytics and now a writer for ESPN - expressed his belief that the next major impact will be “quantifying how often quarterbacks make the correct decision on the field.”\nIt seems that every new NFL season results in an amateur analyst bringing a groundbreaking model and/or approach to the table. Unlike, for example, MLB where there is little left to discover in terms of sabermetrics and new approaches to understanding the game and its associated strategy, the NFL is - for lack of a better phrase - an open playing field. With more and more data becoming available to the public, it is now easier than ever investigate your own ideas and suspicions and to create your own models to confirm your intuition.\nFor example, I am originally from the greater Pittsburgh area and am a big Steelers fan (which certainly explains some of the Steelers-centric examples I use in the writing of this book). I was adamant in my belief that Pittsburgh’s TJ Watt should win the 2021 Defensive Player of the Year award, despite many others calling for Los Angeles’ Aaron Donald to claim the title. In effort to prove my point, I sought out to design what I coined Adjusted Defensive Impact. To begin, I wanted to explore the idea that not all defensive sacks are created equal, as a player’s true impact is not always perfectly represented by top-level statistics.\nTo account for that, I opted to adjust and add statistical weight to sack statistics. This was done over multiple areas. For instance, not all players competed in all 17 regular-season games in 2021. To adjust for this, I took the total of game played in the data (2,936) and divided by 17 (a full season) to achieve a weighted adjustment of 0.0058. TJ Watt played in just 15 games in 2021. His adjusted equation, therefore, is (17-‘games’) * 0.0058. The result? He gets a bit more credit for this adjustment than, say, Myles Garrett who played all 17 regular-season games.\nGoing further with the model, I created a weighted adjustment for solo sacks (0.90), a negative weighted adjustment (-0.14) for any sack charted as “unblocked,” and a weighted adjustment to account for how many times a player actually rushed the QB compared to how many defensive snaps they played. Using data from the SIS Data Hub, the full code is below:\nThe end result? Taking into account the above adjusted defensive impact, TJ Watt was absolutely dominant during the 2021 season:\nAll of these examples - from Ben Baldwin’s 4th-down model, to Football Outsiders’ DVOA, to my attempt to further quantify defensive player impact - are just the leading edge of the burgeoning analytics movement in the NFL. Moreover, the beauty of analytics is that you do not have to be a mathematician or statistics buff in order to enter the fray. All it takes is a genuine curiosity to explore what Bob Carroll, Pete Palmer, and John Thorn coined as the “Hidden Game of Football” and the desire to learn, if you have not already, the R programming language."
  },
  {
    "objectID": "index.html#overview-of-chapters",
    "href": "index.html#overview-of-chapters",
    "title": "An Introduction to NFL Analytics with R",
    "section": "\n1.1 Overview of Chapters",
    "text": "1.1 Overview of Chapters\n\nChapter 1 provides an overview of the nflverse with specific attention paid to the difference between using nflfastR versus nflreadr. Serving as the first dive into analytics, the chapter showcases how to use nflreadr to retrieve both compiled weekly NFL stats and the deeply robust play-by-play statistics. In both cases, exercises are provided. Readers will do their first coding by, first, using the weekly stats to determine the 2021 leaders in air yards per attempt. Second, readers will use the play-by-play statistics from the 2021 season to create a brand new metric (QB aggressiveness on 3rd down pass attempts). Afterward, readers will learn how to retrieve multiple seasons of data at once.\nChapter 2 covers the process of downloading both R and RStudio, as well as the necessary packages to do NFL analytics. As one of the most important chapters in the book (especially for those new to the R programming language), readers take a deep dive into wrangling NFL data with the tidyverse package. To begin, readers will learn about the dplyr pipe (%>%) and use, in exercises, the six most important verbs in the dplyr language: filer(), select(), arrange(), summarize(), mutate(), and group_by(). At the conclusion of the chapter, multiple exercises are provided to allow readers to practice using the dplyr verbs, relational operators within the filter() function and creating “new stats” by using the summarize() function. Moreover, readers will determine the relationship between the dplyr language and important variables within the nflverse data such as player_name and player_id, which is important for correct manipulation and cleaning of data.\nChapter 3 examines the numerous and, often, bewildering amount of functions “underneath the hood” of the packages that makes up the nflverse. For example, load_pbp() and load_player_stats() are included in both nflfastR and nflreadr. However, load_nextgen_stats(), load_pfr_stats(), and load_contracts() are all part of just nflreadr. Because of this complexity, readers will learn how to efficiently operate within the nflverse. Moreover, chapter 3 provides dozens of examples and exercises related to all of the various functions included. For example, readers will learn to use load_nextgen_stats() to determine which running backs get to the line of scrimmage the quickest and will use load_pfr_stats() to explore advanced defensive metrics across multiple seasons.\nChapter 4 moves readers from data cleaning and manipulation to an introduction to data visualization using ggplot2. As well, chapter 4 provides further instruction on nflverse functions such as clean_player_names(), clean_team_abbrs(), and clean_homeaway(). As well, to prep for data visualization, readers will be introduced to the teams_colors_logos and load_rosters functions as well as the nflplotR package, which provides “functions and geoms to help visualization of NFL related analysis” (Carl 2022). Readers will produce multiple types of visualizations, including geom_bar, geom_point, geom_density, and more. As well, readers will learn to use facet_wrap and facet_grid to display data over multiple seasons. For visualizations that include team logos or player headshots, instruction will cover both how to do the coding manually using teams_colors_logos or load_rosters and to use the nflplotr package to avoid the need to use left_join to merge teams_colors_logos to your dataframe.\nChapter 5 introduces advanced methods in R using nflverse data, with a specific focus on modeling and machine learning. To streamline the process of learning, readers will be introduced to tidymodels, a “collection of packages for modeling and machine learning using tidyverse principles” (Silge, n.d.). As an example, readers will first be introduced to Tej Seth’s Rushing Yards Over Expected model (GitHub, ShinyApp). The model will serve as a learning tool to help readers understand the relationship between nflfastR data and machine learning (in Tej’s case, an xgboost model). Afterward, specific attention is given to binary classification, multiclass classification, and regression computer learning models. At the conclusion of the chapter, readers will be provided exercises to allow them to develop their own supervised and unsupervised machine learning models.\nChapter 6 introduces data from sources outside of the nflverse, including premium statistics from Pro Football Focus and Sports Info Solutions. Readers will learned to use various functions, such as clean_team_names, in order to prepare the data to merge with data from the nflverse. As well, this chapter will introduce readers to working with player tracking data. To do so, data will be provided from the NFL’s Big Data Bowl. To highlight the work being completed using player tracking, this chapter will discuss the Big Data Bowl entries of Matt Ploenzke (The Importance of Ball Carrier Downfield Acceleration and Unblocked Tackler Distance and Spacing) and the team of Kellin Rumsey & Brandon DeFlon (The Battle Between Blocker and Defender Is Often Decided by Leverage)."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "An Introduction to NFL Analytics with R",
    "section": "About The Author",
    "text": "About The Author\nI (Bradley Congelio) am currently an Assistant Professor in the College of Business at Kutztown University of Pennsylvania. Aside from my core area of instruction, I also instruct the very popular Sport Analytics (SPT 313) course.\nI earned my Ph.D. from the University of Western Ontario and received a specialized certificate in R for Data Analytics from the University of California, San Diego in 2021. I am a proud undergraduate alumni of West Liberty University and am a strong advocate of a broad-based liberal arts education.\nMy research focuses on using big data, the R programming language, and analytics to explore the impact of professional stadiums on neighboring communities. I use the proprietary Zillow ZTRAX database as well as U.S. Census and other forms of data to create robust, applied, and useful insight into how best to protect those living in areas where stadiums are proposed for construction.\nAs well, my work in sport analytics, specifically the NFL, has been featured on numerous media outlets, including the USA Today and Sports Illustrated.\nFinally, my most recent academic, peer-reviewed publications include:\n\nCongelio, B. (2022). ’Examining the Impact of New Stadium Construction on Local Property Prices Using Data Analytics and the Zillow ZTRAX Database.” Journal of Business, Economics, and Technology Spring 2022, 39-55.\nCongelio, B. (2021). “Monitoring the Policing of Olympic Host Cities: A Novel Approach Using Data Analytics and the LA2028 Olympic Summer Games.” Journal of Olympic Studies 2(2), 129-145.\nCongelio, B. “Predicting the Impact of a New Stadium on Surrounding Neighborhoods Through the Use of a k-means Unsupervised Clustering Algorithm.” Currently under peer review.\nCongelio, B. “Examining Megaevent’s Impact on Foot Traffic to Local Businesses Using Mobility and Demographic Aggregation Data.” Currently under peer review and funded by a $15,000 grant.\n\n\n1.1.1 Why A Book Instead of Working in Analytics?\nI am sometimes asked why I spend time in the classroom teaching this material rather than taking my domain knowledge to the “industry side” and working in the NFL or an otherwise NFL-connected outlet.\nThe honest and, likely boring, answer is this: I love teaching. My favorite experience in the classroom yet is always in my Sport Analytics course. The frustration and sense of helplessness is palpable in the first weeks of the semester as students attempt to wrap their head around, what a former student called, “this [censored] foreign language.” I insist that they keep pushing through the exercises and assignments. Often, there is line out my door and extending down the hallway during office hours comprised of just students from the Sport Analytics class.\nAnd then something amazing happens.\nTypically about halfway through the semester, I start seeing the light bulbs go off. Instead of cursing in anger at the “foreign language,” students begin randomly cursing in excitement as the flow of the tidyverse language “clicks.” Once that happens, it is off to the races because, once they understand speaking in tidyverse, learning more difficult packages (like tidymodels) seems doable.\nAnd that is why I teach. That moment where I realize my lecturing, assisting, explaining, and gentle nudging are all finally paying dividends - not for me, though. For the students.\nThis book serves as an extension of that classroom experience. As a reader of this book, you are now a “student” and I hope you do not hesitate to reach out to me if you ever have any questions or, more importantly, when (not if) you have that “light bulb moment” and everything begins to click for you."
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "An Introduction to NFL Analytics with R",
    "section": "Technical Details",
    "text": "Technical Details\nThis book was written using RStudio’s Visual Editor for R Markdown. It was published using the Quarto publishing system built on Pandoc. As well, the following packages were used in this book:\n\n\n\n\nPackages Used In This Book\n \n package \n    version \n    source \n  \n\n\n arrow \n    10.0.1 \n    CRAN (R 4.1.3) \n  \n\n bonsai \n    0.2.1 \n    CRAN (R 4.1.3) \n  \n\n doParallel \n    1.0.17 \n    CRAN (R 4.1.3) \n  \n\n dplyr \n    1.1.0 \n    CRAN (R 4.1.3) \n  \n\n ggimage \n    0.3.1 \n    CRAN (R 4.1.3) \n  \n\n ggplot2 \n    3.4.0 \n    CRAN (R 4.1.3) \n  \n\n ggpmisc \n    0.5.2 \n    CRAN (R 4.1.3) \n  \n\n gt \n    0.8.0 \n    CRAN (R 4.1.3) \n  \n\n lightGBM \n    3.3.5 \n    CRAN (R 4.1.3) \n  \n\n nflfastR \n    4.5.1 \n    CRAN (R 4.1.3) \n  \n\n nflreadr \n    1.3.2 \n    CRAN (R 4.1.3) \n  \n\n nflverse \n    1.0.2 \n    https://nflverse.r-universe.dev (R 4.1.3) \n  \n\n scales \n    1.2.1 \n    CRAN (R 4.1.3) \n  \n\n tidymodels \n    1.0.0 \n    CRAN (R 4.1.3) \n  \n\n tidyverse \n    1.3.2 \n    CRAN (R 4.1.3) \n  \n\n webshot \n    0.5.4 \n    CRAN (R 4.1.3) \n  \n\n\n\n\nFinally, please note that this book uses the dplyr pipe operator (%>%) as opposed to the new, built-in pipe operator released with version 4.1 of R (|>). It is likely that you can work through the exercises and examples in this book by using either operator. I maintain my use of the dplyr pipe operator for no other reason than personal preference."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "An Introduction to NFL Analytics with R",
    "section": "\n1.2 License",
    "text": "1.2 License\nThe online version of this book is published with the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license.\n\n\n\n\nAwbrey, Jake. 2020. “The Future of NFL Analytics.” https://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics.\n\n\nBechtold, Taylor. 2021. “How the Analytics Movement Has Changed the NFL and Where It Has Fallen Short.” https://theanalyst.com/na/2021/04/evolution-of-the-analytics-movement-in-the-nfl/.\n\n\n“Big Data Bowl: The Annual Analytics Contest Explores Statistical Innovations in Football.” n.d. https://operations.nfl.com/gameday/analytics/big-data-bowl/.\n\n\nBushnell, Henry. 2021. “NFL Teams Are Taking 4th-down Risks More Than Ever - but Still Not Often Enough.” https://sports.yahoo.com/nfl-teams-are-taking-4th-down-risks-more-than-ever-but-still-not-often-enough-163650973.html.\n\n\nCarl, Sebastian. 2022. “nflplotR.” https://nflplotr.nflverse.com/.\n\n\nFortier, Sam. 2020. “The NFL’s Analytics Movement Has Finally Reached the Sport’s Mainstream.” https://www.washingtonpost.com/sports/2020/01/16/nfls-analytics-movement-has-finally-reached-sports-mainstream/.\n\n\nHeifetz, Danney. 2019. “We Salute You, Founding Father of the NFL’s Analytics Movement.” https://www.theringer.com/nfl-preview/2019/8/15/20806241/nfl-analytics-pro-football-focus.\n\n\nKozora, Alex. 2015. “Tomlin Prefers \"Feel over Analytics\".” http://steelersdepot.com/2015/09/tomlin-prefers-feel-over-analytics/.\n\n\nRosenthal, Gregg. 2018. “Super Bowl LII: How the 2017 Philadelphia Eagles Were Built.” https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753.\n\n\nSilge, Julia. n.d. “Tidymodels.” https://tidymodels.org."
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#introduction",
    "href": "01-nfl-analytics-and-r.html#introduction",
    "title": "\n2  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\nIt might seem odd to begin an introductory book with coding and visualization in Chapter 1, while placing information about learning the basics of the tidyverse in a later chapter. But there is good reason why I adopted this pedagogical approach in this book. As explained by Hadley Wickham and Garrett Grolemund in their outstanding book R for Data Science, the process of reading in and then cleaning data is not exactly the most exciting part of doing analytics. As evidence suggest, early excitement about and integration into a topic increases the likelihood of following up and learning the “boring” material.\nBecause of this, I follow the approach of Wickham and Grolemund and provide data that is already, for the most part, “tidied” and ready to be used. We will however, in later chapters, pull raw data directly from it source (such as nflreadr, Pro Football Reference, and Sports Info Solutions) that requires manipulation and cleaning before any significant analysis can begin.\n\n\n\n\n\n\nImportant\n\n\n\nI am assuming, while you may not have a full grasp of the tidyverse yet, that you do currently have base R and RStudio installed. If you do not, more detailed instructions are provided in Chapter 2. If you would rather jump right into this material, you can download base R and RStudio at the following links. Once both are installed, you can return to this point in the chapter to follow along.\nTo download/install base R: cran.rstudio.com\nTo download/install RStudio: RStudio Desktop (scroll to bottom of page for Mac options)\n\n\nMoreover, as briefly outlined in the Preface, we move through the process of learning NFL analytics via a close relationship with investigative inquiry. In this instance, we will define the process of investigative inquiry as one that seeks both knowledge and information about a problem/question through data-based research. To that end, we will routinely use the process throughout this book to uncover insights, patterns, and trends relating to both players and teams that serve to help us answer the problem/question we are examining.\nWhile it can - and should - be entertaining to develop visualization and models around arbitrarily picked statistics and metrics, it is important to remember that the end goal of the process is to glean useful insights that, ultimately, can be shared with the public. Much like the work done by a data analyst for a Fortune 500 company, the work you produce as a result of this book should do two things: (1.) provide deeper insight and knowledge about NFL teams and players and (2.) effectively communicate a story.\nThis is why the process of investigative inquiry is ingrained, as much as possible, into every example provided in this book. In doing so, the standard outline for completing an investigate inquiry is modified to fit the needs of this book - specifically, the addition of communicating your findings to the public at the end."
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#the-investigate-inquiry-outline",
    "href": "01-nfl-analytics-and-r.html#the-investigate-inquiry-outline",
    "title": "\n2  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n2.2 The Investigate Inquiry Outline",
    "text": "2.2 The Investigate Inquiry Outline\n\nIdentify the problem or question. The first step in any investigative inquiry is to clearly define the problem or question that you are trying to answer. Many times, fans have questions about their individuals favorite team and/or players. For example, the 2022 Los Angeles Rams - the defending Super Bowl Champions - were eliminated from playoff contention with three weeks remaining in the season. With the early exit, the Rams tied the 1999 Denver Broncos for the earliest elimination from playoff contention for any prior Super Bowl Champion. The Rams’ early elimination can be explained by the high number of injuries during the season, including Matthew Stafford, Cooper Kupp, and Aaron Donald. Perhaps the biggest factor, though, was the inability to keep a healthy offensive line. In this specific example, in terms of identifying the problem or question, a potential problem or question to explore is: how many unique combinations of offensive linemen did the 2022 LA Rams use and where does it rank in NFL history? Have other teams in recent history faced the same amount of offensive line turnover yet still make the playoffs? As you can see, there are a number of different avenues in which the problem or question surrounding the Rams’ offensive line injury issues can be explored.\nGather data. With a question or problem determined, we now turn to the process of finding and gathering the necessary data to find answers. Unfortunately, data is not always going to be available to answer your investigate inquiry. For example, NFL’s tracking data is only released in partial form during the annual Big Data Bowl (explored later in Chapter ##). In the event that your question or problem requires data that is not available, you must loop back to Step 1 and reconfigure your question to match available data. In the case of the 2022 LA Rams’ offensive line, access to data that can answer the question is available through two cores avenues: the load_participation and load_snap_counts functions within the nflverse family of packages.\nClean and prepare the data. It is not often that the data you obtain to answer your question will be perfectly prepared to immediate analysis. As will be explored below, the data collected to explore the Rams’ offensive line combinations required both (1.) a critical thought process on how to best solve oddities in the data while still producing correct and reliable information (2.) the cleaning and preparation to make the changes as a result of that critical thinking process. As you progress through the many examples and exercises in this book, you will often be presented with prepared datasets that require you to determine the best approach to data manipulation through this critical thinking and cleaning/preparation process.\nAnalyze the data. After problem solving to ensure the data is as reliable and consistent as possible, we can turn to analyzing the data. In this case, since we are concerned with unique combinations of offensive linemen, we can quickly get results by using the n_distinct function within dplyr.\n\nVisualize the data. There are generally two options for visualizing data: plotting with ggplot or creating a table with gt and the outstanding companion package gtExtras. To that end, considering the following can help determine whether to present your findings in char or table format.\n\nThe type of data you are working with. If you have a large amount of numerical data that needs to be compared or analyzed, a table may be the most effective way to present this information. On the other hand, if you want to highlight trends or patterns in your data, a chart can help illustrate the information in a more clear manner.\nThe purpose of your visualization. You must consider what you ultimately want to communicate with your visualization. If you want to provide a detailed breakdown of your data, a table is usually more appropriate. However, if you want to show the overall trend or pattern in your data, a chart is going to be more effective.\nThe audience for your visualization. As you determine whether to use a chart or a table, think about who will be viewing your visualization and what level of detail they need. If your audience is familiar with the data and needs to see precise value, a table may be a better choice. If your audience is not as familiar with the data and you want to highlight the main trends or patterns, a chart my be more effective.\n\nBelow, we will explore visualizing our offensive linemen combinations in both chart and table format using multiple different variables for sake of comparison.\n\nInterpret and communicate the results. Lastly, it is time to communicate your results to the public. Whether this be through Twitter, a team blog, or a message board, there are numerous items to consider when preparing to build your story/narrative for sharing. This will be covered further in Chapter ## (Visualization) as well.\n\nWith a clear direction via the investigative inquiry process, we can turn to taking a deeper dive into the LA Rams’ 2022 offensive linemen issue."
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#investigating-the-rams-2022-offensive-line",
    "href": "01-nfl-analytics-and-r.html#investigating-the-rams-2022-offensive-line",
    "title": "\n2  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n2.3 Investigating the Rams’ 2022 Offensive Line",
    "text": "2.3 Investigating the Rams’ 2022 Offensive Line\nThe “Super Bowl hangover” is real.\nAt least for the loser of the big game.\nSince the AFL and NFL merged in 1970, a total of 15 of the 51 losers of the Super Bowl went on to miss the playoffs in the following season, while 13 failed to even achieve a winning record. Teams coming off a Super Bowl victory have generally fared better, with the winners putting together a .500 record or better 45 out of 51 times.\nOf those six teams to not achieve a .500 record after winning the Super Bowl, only a few have been as downright terrible as the 2022 Los Angeles Rams.\nAs explained by Mike Ehrmann, the Rams’ poor Super Bowl defense is “what happens when a laundry list of things go wildly wrong at the same time” (Kirschner 2022). As outlined above in our investigative inquiry outline, one of the core items on the Rams’ laundry list of bad luck was the absurd amount of offensive linemen ending up on the injured list. This, combined with losing Andrew Whitworth to retirement after the Super Bowl, led to quarterback Matthew Stafford being sacked on 8.6-percent of his dropback attempts (a rate that nearly doubled from the previous season).\nGiven that context, just how historically bad was the Rams’ 2022 offensive line turnover? We can being diving into the data to find our results and build our story.\n\n2.3.1 Unique Offensive Line Combinations: How to Collect The Data?\nTo begin obtaining and preparing the data to determine the number of unique offensive line combinations the Rams had in the 2022 season, I turned to two possible options: the load_participation and load_snap_counts functions within the nflreadr package. The load_participation function will return, if include_pbp = TRUE a list of every player ID number that was on the field for each play, whereas load_snap_counts returns - on a per game basis - the percentage of snaps each player was on the field for.\nIn the end, using load_snap_counts created the most accurate, reliable, and straightforward way in each to collect unique offensive line combinations. The load_participation function resulted in several oddities in the data (not with the collection of it by the nflverse maintainers, but with individual NFL team strategies and formations). To highlight this, the following code will select the first offensive play for each team, in each game, up to week 15 of the 2022 season.\n\nparticipation <- nflreadr::load_participation(2022, include_pbp = TRUE)\nrosters <- nflreadr::load_rosters(2022) %>%\n  select(full_name, gsis_id, depth_chart_position)\n\noline_participation <- participation %>%\n  filter(play_type %in% c(\"pass\", \"run\")) %>%\n  group_by(nflverse_game_id, possession_team, fixed_drive) %>%\n  filter(fixed_drive == 1 | fixed_drive == 2) %>%\n  filter(row_number() == 1) %>%\n  select(nflverse_game_id, play_id, possession_team, \n         offense_personnel, offense_players) %>%\n  dplyr::mutate(gsis_id = stringr::str_split(offense_players, \";\")) %>%\n  tidyr::unnest(c(gsis_id)) %>%\n  left_join(rosters, by = c(\"gsis_id\" = \"gsis_id\"))\n\noline_participation <- oline_participation %>%\n  filter(depth_chart_position %in% c(\"T\", \"G\", \"C\")) %>%\n  group_by(nflverse_game_id, possession_team) %>%\n  mutate(starting_line = toString(full_name)) %>%\n  select(nflverse_game_id, possession_team, \n         offense_personnel, starting_line) %>%\n  distinct()\n\nWhile the output using the load_participation function is correct, a quick examination of the offense_personnel column causes concern about the viability of this approach to calculate the total number of unique offensive line combinations. A grouping and summing of the offense_personnel column highlights the issue.\n\noline_participation %>%\n  group_by(offense_personnel) %>%\n  summarize(total = n())\n\n# A tibble: 14 x 2\n   offense_personnel                        total\n   <chr>                                    <int>\n 1 1 RB, 0 TE, 4 WR                             4\n 2 1 RB, 1 TE, 3 WR                           240\n 3 1 RB, 2 TE, 2 WR                           171\n 4 1 RB, 3 TE, 1 WR                            19\n 5 2 QB, 1 RB, 1 TE, 2 WR                       4\n 6 2 RB, 0 TE, 3 WR                             1\n 7 2 RB, 1 TE, 2 WR                            89\n 8 2 RB, 2 TE, 1 WR                            14\n 9 3 RB, 1 TE, 1 WR                             1\n10 6 OL, 1 RB, 0 TE, 3 WR                       2\n11 6 OL, 1 RB, 1 TE, 2 WR                      12\n12 6 OL, 1 RB, 2 TE, 1 WR                       1\n13 6 OL, 2 RB, 0 TE, 2 WR                       1\n14 7 OL, 0 RB, 0 TE, 0 WR,1 P,1 LS,1 DL,1 K     1\n\n\nOf concern are rows 10 through 14. In 15 different cases, a team ran its first play of the game with six offensive linemen. And, in one case, the resulting data indicates that the Dallas Cowboys ran their first play in week 5 against the LA Rams with seven offensive linemen, one punter, one long snapper, and a kicker.\nIn the first case, the data is correct that the teams ran their first offensive play with six offensive linemen. For example, in its week 3 game against the Steelers, the data indicates the Cleveland Browns as having started Jack Conklin (tackle), Jedrick Wills Jr. (tackle), Joel Bitonio (guard), Michael Dunn (guard), Wyatt Teller (guard), and Ethan Pocic (center). A view of the NFL’s All-22 film confirms that, indeed, all six offensive linemen were on the field for the Browns’ first snap of the game.\n\n\nSteelers vs. Browns: 6 Offensive Linemen\n\n\nIn the second case, Dallas’ offense personnel on its “first play” from scrimmage is the result of the Cowboys returning a fumble for a touchdown on the Rams’ first offensive possession with a botched snap on the ensuing extra point attempt. Because of that, the extra point attempt is no longer scored as an extra_point in the play_type variable within the play-by-play data, but a rushing attempt. As a result of this oddity, the data is correct in listing Dallas’ first offensive play as coming out of an extra point personnel grouping.\nBoth of these examples are problematic as a team’s “starting offensive line” is considered to be just five players: the left tackle, the left guard, the center, the right guard, and the right tackle. In order to correctly determine the number of combinations used, we need to first determine the five most-commonly used offensive linemen for each team. Because of the off-the-wall situations that can occur in football, building offensive line combinations through personnel groupings in the play-by–play data is tricky, at best.\nBecause of this, we can turn to the load_snap_counts function with the nflreadr package to determine the number of unique offensive line combinations. The process to do so occurs over several steps and involves decision-making on our end on how best to accurately represent the five core offensive linemen for each team.\n\noline_snap_counts <- nflreadr::load_snap_counts(seasons = 2022)\n\noline_snap_counts <- oline_snap_counts %>%\n  select(game_id, week, player, position, team, offense_pct) %>%\n  filter(position %in% c(\"T\", \"G\", \"C\")) %>%\n  group_by(game_id, team) %>%\n  arrange(-offense_pct) %>%\n  dplyr::slice(1:5) %>%\n  ungroup()\n\nFirst, we obtain snap count data from the 2022 season and write it into a dateframe titled oline_snap_counts. After, we select just the necessary columns and then filter out the position information to include only tackles, guards, and centers. After grouping each individual offensive line by game_id and its respective team, we arrange each player’s snap count in descending order using offense_pct.\nAnd this is where a decision needs to be made on how to best construct the five starting offensive linemen for each team. By including slice(1:5), we are essentially selecting just the five offensive linemen with the most snap counts in that singular game.\nAre these five plays necessarily the same five that started the game as the two tackles, two guards, and one center? Perhaps not. But, hypothetically, a team’s starting center could have been injured a series or two into the game and the second-string center played the bulk of the snaps in that game.\nBecause of such situations, we can make the argument that the five offensive line players with the highest percentage of snap counts for each unique game_id are to be considered the combination of players used in that game.\nNext, let’s make the decision to arrange each team’s offensive line, by game, in alphabetical order. Since we do not have a reliable way to include specific offensive line positions (that is, we have tackle instead of left tackle or right tackle), we can build our combination numbers strictly based on the five downed linemen, regardless of specific position on the line of scrimmage.\nAfter, we use the toString function to place all five names into a single column (starting_line) and then filter out the data to include just one game_id for the linemen.\n\noline_snap_counts <- oline_snap_counts %>%\n  group_by(game_id, team) %>%\n  arrange(player, .by_group = TRUE)\n\noline_final_data <- oline_snap_counts %>%\n  group_by(game_id, week, team) %>%\n  mutate(starting_line = toString(player)) %>%\n  select(game_id, week, team, starting_line) %>%\n  distinct(game_id, .keep_all = TRUE)\n\nThe end result includes the game_id, the week, the team abbreviation, and the starting_line column that includes the names of the five offensive line players with the highest snap count percentage for that specific game.\n\nhead(oline_final_data)\n\n# A tibble: 6 x 4\n# Groups:   game_id, week, team [6]\n  game_id          week team  starting_line                                     \n  <chr>           <int> <chr> <chr>                                             \n1 2022_01_BAL_NYJ     1 BAL   Ben Powers, Kevin Zeitler, Morgan Moses, Patrick ~\n2 2022_01_BAL_NYJ     1 NYJ   Alijah Vera-Tucker, Connor McGovern, George Fant,~\n3 2022_01_BUF_LA      1 BUF   Dion Dawkins, Mitch Morse, Rodger Saffold, Ryan B~\n4 2022_01_BUF_LA      1 LA    Brian Allen, Coleman Shelton, David Edwards, Jose~\n5 2022_01_CLE_CAR     1 CAR   Austin Corbett, Brady Christensen, Ikem Ekwonu, P~\n6 2022_01_CLE_CAR     1 CLE   Ethan Pocic, James Hudson, Jedrick Wills Jr., Joe~\n\n\nWith the data cleaned and prepared, we are now able to take our first look at the results. In the code below, we are grouping by all 32 NFL team and then summarizing the total number of unique offensive line combinations used through the first 15 weeks of the 2022 season.\n\ntotal_combos <- oline_final_data %>%\n  group_by(team) %>%\n  summarize(combos = n_distinct(starting_line)) %>%\n  arrange(-combos)\n\nDespite much of the media focus being on the Rams’ poor performance, given their title of defending Super Bowl Champions, the Arizona Cardinals had nearly as many unique offensive line combinations at the conclusion of the 2022 regular season.\nWith the data cleaned and prepared, let’s use it to create a ggplot graph and, to do so, compare the relationship between a team’s number of unique offensive lines against its winning percentage. To complete this, we first need to join the winning percentages to our existing total_combos dataframe.\n\n2.3.2 Unique Offensive Line Combinations vs. Win Percentage\nTo bring in the individual winning percentages, we will use the get_nfl_standings function from espnscrapeR and then combine the two sets of data on team abbreviations via a left_join. Unfortunately, the team abbreviation returned from espnscrapeR does not match up with those used in the nflverse for both the Los Angeles Rams and the Washington Commanders (LAR vs. LA and WSH vs. WAS). As evidenced in the below code, correcting this issue is simple with the clean_team_abbrs function in the nflreadr package.\n\nrecords <- espnscrapeR::get_nfl_standings(season = 2022) %>%\n  select(team_abb, win_pct)\n\nrecords$team_abb <- nflreadr::clean_team_abbrs(records$team_abb)\n\ntotal_combos <- total_combos %>%\n  left_join(records, by = c(\"team\" = \"team_abb\"))\n\nAfter collecting team records and merging them into the offensive line combination data, we can use ggplot to visualize the data. Individual team logos are used in place of the typical geom_point by using the nflplotR package.\n\n\n\n\n\n\nImportant\n\n\n\nPlease note in the below ggplot coding that we are using a custom theme, nfl_analytics_theme(). If you wish to replicate the below visualization using the theme, run the below code to place the theme into your RStudio environment allowing you to call it within ggplot. Be sure to install and run the ggtext package as the theme uses the package’s element_markdown() function.\nIf you do not have the “Roboto Condensed” font installed, ggplot will give an error message but ultimately replace it with another font on your system. If you wish to install “Roboto Condensed,” you can do so with the showtext package.\n\n\n\nnfl_analytics_theme <- function(..., base_size = 12) {\n  \n  theme(\n    text = element_text(family = \"Roboto Condensed\", size = base_size, color = \"black\"),\n    axis.ticks = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.title = element_markdown(size = 16,\n                                  vjust = .02,\n                                  hjust = 0.5),\n    plot.subtitle = element_markdown(hjust = 0.5),\n    plot.caption = element_markdown(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major =  element_line(color = \"#d0d0d0\"),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    plot.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_blank())\n}\n\n\nggplot(data = total_combos, aes(x = combos, y = win_pct)) +\n  geom_smooth(se = FALSE, formula = 'y ~ x', method = \"lm\") +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065, alpha = 0.7) +\n  scale_x_reverse(breaks = scales::pretty_breaks(n = 12)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_number(accuracy = 0.001)) +\n  nfl_analytics_theme() +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Win Percentage\") +\n  labs(title = \"**Unique Offensive Line Combinations vs. Win Percentage**\",\n       subtitle = \"*Through Week #15*\",\n       caption = \"*An Introduction to NFL Analytics With R*<br>Brad J. Congelio\")\n\n\n\n\n\n\n\nAs can be seen in the resulting graph - which shows little, if any, statistical correlation - there are still teams with worse records than the Rams and Cardinals that have fewer unique offensive line combinations (the Houston Texans and the Chicago Bears, for example). Perhaps there is a better metric that correlates more strongly with a team’s number of offensive line combinations?\nTo begin exploring that, we can hypothesize that more offensive line combinations leads to more quarterback pressures and an increased pressure rate as the various member of the offensive line never have time to properly “gel.”\n\n2.3.3 Unique Offensive Line Combinations vs. Pressure Rate\nRather than calculate the data ourselves (which is the total number dropbacks divided by the total number of pressures), we can turn away from nflverse data and retrieve the information from the SIS Data Hub. After downloading the spreadsheet, we can read it into the RStudio environment using vroom and then merge the information into our existing total_combos by matching on the individual team abbreviations.\n\npressure_rate <- vroom::vroom(\"./example_data/csv/ch1_pressurerate.csv\")\n\ntotal_combos <- total_combos %>%\n  left_join(pressure_rate, by = c(\"team\" = \"team_abbr\"))\n\n\n\n# A tibble: 6 x 3\n  season team_abbr pressure_percent\n   <dbl> <chr>                <dbl>\n1   2022 KC                    34.2\n2   2022 PHI                   30.7\n3   2022 SEA                   34.8\n4   2022 CIN                   29.4\n5   2022 SF                    32.4\n6   2022 MIA                   35  \n\n\nWith the pressure rate now merged with our unique offensive line combination data, we can make slight adjustments to our prior ggplot code to examine any potential relationship.\n\nggplot(data = total_combos, aes(x = combos, y = pressure_percent)) +\n  geom_smooth(se = FALSE, formula = 'y ~ x', method = \"lm\") +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065, alpha = 0.7) +\n  scale_x_reverse(breaks = scales::pretty_breaks(n = 12)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::percent_format(scale = 1, accuracy = 0.1)) +\n  nfl_analytics_theme() +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Pressure Rate (per Dropback)\") +\n  labs(title = \"Unique Offensive Line Combinations vs. Pressure Rate\",\n       subtitle = \"2022 Season\")\n\n\n\n\n\n\n\nAgain, there does not seem to be any statistical correlation between a team’s number of unique offensive line combinations and the pressure rate per dropback allowed. Rather than examining the pressure per dropback rate, perhaps there is a correlation between a the number of unique offensive line combinations and the total number of quarterback hits allowed through the season? To determine the validity of that hypothesis, we can use data from nflreadr to collect total QB hits.\n\n2.3.4 Unique Offensive Line Combinations vs. QB Hits Allowed\nTo begin, we can collect the QB hit data from nflreadr being sure to group_by the posteam variable in order to calculate the number of QB hits allowed by each team’s offensive line. After, we can merge the data into our existing total_combos dataset and then produce the plot.\n\npbp <- nflreadr::load_pbp(2022)\n\nqb_hits <- pbp %>%\n  filter(!is.na(posteam)) %>%\n  group_by(posteam) %>%\n  summarize(total_qb_hits = sum(qb_hit == 1, na.rm = TRUE))\n\nqb_hits_combined <- left_join(\n  total_combos, qb_hits, by = c(\"team\" = \"posteam\"))\n\nAfter first filtering out any data that does not include posteam information, we can group_by each individual offensive unit and then calculate the total sum of QB hits for each. We can then take our new dataframe titled qb_hits_combined and plot the results.\n\nggplot(data = qb_hits_combined, aes(x = combos, y = total_qb_hits)) +\n  geom_smooth(se = FALSE, formula = 'y ~ x', method = \"lm\") +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065, alpha = 0.7) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Total QB Hits Allowed\") +\n  labs(title = \"Unique Offensive Line Combinations vs. QB Hits Allowed\",\n       subtitle = \"2022 Season\")"
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#exploring-explosive-plays-per-touchdown-drive",
    "href": "01-nfl-analytics-and-r.html#exploring-explosive-plays-per-touchdown-drive",
    "title": "\n2  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n2.4 Exploring Explosive Plays per Touchdown Drive",
    "text": "2.4 Exploring Explosive Plays per Touchdown Drive\n\npbp <- nflreadr::load_pbp(2022)\n\nexplosive <- pbp %>%\n  filter(!is.na(posteam) & !is.na(yards_gained) & fixed_drive_result == \"Touchdown\") %>%\n  filter(play_type != \"kickoff\" & play_type != \"extra_point\" & !is.na(down)) %>%\n  select(posteam, game_id, drive, yards_gained)\n  \nexplosive_per_td <- explosive %>%\n  group_by(posteam, game_id, drive) %>%\n  summarize(max_yards = max(yards_gained)) %>%\n  mutate(explosive_play = if_else(max_yards >= 20, 1, 0))\n\nexplosive_final <- explosive_per_td %>%\n  group_by(posteam) %>%\n  summarize(\n    tds_no_explosive = sum(explosive_play == 0),\n    tds_explosive = sum(explosive_play == 1)\n  )\n\ntotal_drives_calc <- pbp %>%\n  group_by(posteam, week) %>%\n  filter(!is.na(posteam) & fixed_drive_result == \"Touchdown\") %>%\n  summarize(total_drives = n_distinct(fixed_drive))\n\ntotal_drives_calc <- total_drives_calc %>%\n  group_by(posteam) %>%\n  summarize(total_drives = sum(total_drives))\n\nexplosive_final <- explosive_final %>%\n  left_join(total_drives_calc, by = c(\"posteam\" = \"posteam\")) %>%\n  mutate(percent_no_exp = tds_no_explosive / total_drives,\n         percent_w_exp = tds_explosive / total_drives) %>%\n  select(posteam, percent_no_exp, percent_w_exp)\n\nggplot(explosive_final, aes(y = reorder(posteam, percent_w_exp), x = percent_w_exp)) +\n  ggplot2::geom_col(aes(color = posteam, fill = posteam), width = 0.5) +\n  nflplotR::scale_color_nfl(type = \"secondary\") +\n  nflplotR::scale_fill_nfl(alpha = 0.4) +\n  nfl_analytics_theme() +\n  theme(axis.text.y = element_nfl_logo(size = .65)) +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = scales::pretty_breaks(),\n                     label = scales::percent_format()) +\n  xlab(\"Percent of TD Drives with an Explosive Play (20+ Yards)\") +\n  ylab(\"\")\n\n\n\n\n\nKirschner, Alex. 2022. “The Rams’ Super Bowl Afterparty Turned into a Historic Hangover.” https://fivethirtyeight.com/features/the-rams-super-bowl-afterparty-turned-into-a-historic-hangover/."
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#downloading-r-and-rstudio",
    "href": "02-nfl-analytics-tidyverse.html#downloading-r-and-rstudio",
    "title": "\n3  Wrangling NFL Data in the tidyverse\n",
    "section": "\n3.1 Downloading R and RStudio",
    "text": "3.1 Downloading R and RStudio\nPrior to downloading R and RStudio, it is important to explain the difference between R and RStudio, as they are two separate pieces of our analytics puzzle that are used for differing purposes. R is the core programming language used for statistical computing and graphics. R provides a wide range of statistical and graphical techniques and has a large community of users who develop and maintain a multitude of packages (essentially libraries of pre-written functions) that extend the capabilities and ease of coding. While R can be run from your computer’s command line, it is also has an integrated development environment (IDE) in RStudio that provides a graphical user interface for working with R scripts, data files, and packages.\nRStudio is free to download and use and provides a user-friendly interface for writing R code, organizing projects and files, and working with both big and small data. Regularly updated by the Posit team, RStudio includes many features that are specifically designed to making working with R easier, including syntax highlighting, code suggestions, robust debugging tools, and a built-in package manager.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important that you download and successfully install R before proceeding to install RStudio on your computer.\n\n\n\n3.1.1 Downloading R\n\nTo download R, visit the official R website at https://www.r-project.org/\n\nClick on the ‘CRAN’ link at the top of the page, directly underneath the word ‘Download.’ This will bring you to the Comprehensive R Archive Network.\nEach mirror that hosts a copy of R is sorted by country. Select a mirror that is geographically close to you.\nClick on the version of R that is appropriate for your operating system (Linux, macOS, or Windows).\nSelect the ‘base’ option to download an updated version of R to your computer.\nOpen the downloaded file and follow the provided installation instructions.\n\n3.1.2 Downloading RStudio\n\nTo download RStudio, visit the official RStudio website at https://posit.co\n\nClick on the ‘Download RStudio’ button in the top-right of the page.\nScroll down and select ‘Download’ within the ‘RStudio Desktop - Free’ box.\nOpen the downloaded file and follow the provided installation instructions.\n\n3.1.3 The Layout of RStudio\nWhen you open RStudio for the first time, you will see the interface laid out as in the picture below (sourced from the official RStudio documentation from posit).\n\n\n\n\nAs you can see, RStudio provides a graphical interface for working with R and is sorted into four main panes, each of which serves a specific purpose.\n\n\nThe source pane: In the upper-left, the source pane is where you write and run your R code. It serves as the main working pane within RStudio.\n\nThe console pane: the console pane serves multiple functions, including allowing you to interact directly with R by typing commands and receiving the output. Additionally, any errors outputted by code ran in the source pane will be detailed in the console, allowing you to troubleshoot and debug.\n\nEnvironment and history pane: this pane, in the upper-right, displays information about the current R environment and command history. Perhaps more important, it displays the information of all the R-created objects currently stored in your computer’s memory including datasets, vectors, lists, etc.\n\nFiles, plots, packages, and help pane: in the bottom-right, this pane provides access to numerous RStudio tools and resources including the ability to browse and navigate through the files and folders on your computer and view the output of plots and graphics. As well, the ‘Packages’ tab gives you the ability to manage any R packages that you have installed on your system and to view each packages’ help documentation.\n\n3.1.4 Running Code in RStudio\nTo begin writing code in RStudio you first need to open a new R script. To do so, select ‘File -> New File -> R Script.’ A new script tab, titled Untitled 1 will open in your RStudio’s source pane. To run a line of code - or multiple lines of code - you can do one of two options:\n\nPlace your cursor directly at the end of the last line of code, or highlight all the code you wish to run, and press ‘Ctrl + Enter’ (Windows) or ‘Command + Enter’ (Mac).\nPlace your cursor directly at the end of the last line of code, or highlight all the code you wish to run, and then use your mouse to click the ‘Run’ button in the source pane’s toolbar.\n\nAs a working example, let’s do a simple addition problem within the the source pane:\n\n2 + 2\n\nAfter following one of the two above options for running the addition problem, the output in your console should appear like below:\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is important to notice the > after the output in the console. That indicates that the coding process has completely run and RStudio is ready to run the next task that you submit. If you have an error in your code, you will notice that the > is replaced by what my students refer to as the ‘hanging plus sign’, +.\nYou receive the + sign when RStudio is expecting a continuation of the code. Several issues can cause this to happen, including forgetting to provide an equal number of opening and closing parenthesis or mistakenly including a pipe, %>%, after the last line of code.\nIn any case, when you see the + in the console, simply use your mouse to click within the console and hit your keyboard’s escape key. Doing so exits the prompt and resets the console to include the > symbol."
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#installing-and-loading-necessary-packages",
    "href": "02-nfl-analytics-tidyverse.html#installing-and-loading-necessary-packages",
    "title": "\n3  Wrangling NFL Data in the tidyverse\n",
    "section": "\n3.2 Installing and Loading Necessary Packages",
    "text": "3.2 Installing and Loading Necessary Packages\nInstalling and loading packages is an important part of working with RStudio, as they provide additional functionality that allow you to more efficiently conduct data analysis and/or modeling. Several packages are widely used through this book, including the ever-important tidyverse, the nflverse family of packages to retrieve NFL statistics and data, and many others such as tidymodels when we tackle building and testing advanced models in Chapter 5.\nTo begin, let’s install both tidyverse and nflverse. In your source pane, you can install a package by using the install.packages() function. To complete the code, you simply supply the package name within quotation marks.\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"nflverse\")\n\nAfter running the code, your console pane will output what is going on “behind the scenes.” When complete, you will again see the > symbol within the console. At this point, you are able to load the packages you just installed.\n\nlibrary(tidyverse)\nlibrary(nflverse)"
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#the-tidyverse-and-its-verbs",
    "href": "02-nfl-analytics-tidyverse.html#the-tidyverse-and-its-verbs",
    "title": "\n3  Wrangling NFL Data in the tidyverse\n",
    "section": "\n3.3 The tidyverse and Its Verbs",
    "text": "3.3 The tidyverse and Its Verbs\nThe tidyverse, now installed and loaded within RStudio, is a collection of R packages designed for data manipulation, visualization, and analysis. It was developed by Hadley Wickham, the Chief Scientist at RStudio, and a varied team of contributors. The goal of the tidyverse is to provide a consistent, easy-to-understand set of functions and syntax for working with data in R.\nThe core principle of the tidyverse is “tidy data,” which is the development team’s belief in creating a standard way of organizing data sets so that they can be easily manipulated, visualized, and analyzed. To that end, a “tidy” data set is one that is comprised of observations (rows) and variables (columns) with each variable being a distinct piece of information and each observation being a unit of analysis.\nInstalling and loading the tidyverse results eight of the core packages automatically being loaded and ready to use:\n\n\ndplyr: “dplyr provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.”\n\ntidyr: “tidyr provides a set of functions that help you get to tidy data. Tidy data is data with a consistent form: in brief, every variable goes in a column, and every column is a variable.”\n\nreadr: “readr provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is deigned to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes.”\n\npurrr: “purrr enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. Once you master the basic concepts, purrr allows you to replace many for loops with code that is easier to write and more expressive.”\n\ntibble: “tibble is a modern re-imagining of the data frame, keeping what time has proven to be effective, and throwing out what it has not. Tibbles are data.frames that are lazy and surly; they do less and complain more forcing you to confront problems earlier, typically leading to cleaner, more expressive code.”\n\nstringr: “stringr provides a cohesive set of functions designed to make working with strings as easy as possible. It is built on top of stringi, which uses the ICU C library to provide fast, correct implementations of common string manipulations.”\n\nforcats: “forcats provides a suite of useful tools that solve common problems with factors. R uses factors to handle categorical variables, variables that have a fixed and known set of possible values.”\n\nggplot2: “ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map the variables to aesthetics, what graphical primitives to use, and it takes care of the details” (Wickham 2022).\n\nAside from the core eight packages, the tidyverse will also install a multiple of other packages such as rvest (for web scraping), readxl (for reading Excel sheets in the RStudio environment), lubridate (a very powerful tool for working with times and dates), and magrittr (the package that provides the pipe %>%). As well, prior versions of the tidyverse utilized the modelr. Modeling is now handled in the tidyverse by the tidymodels package."
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#the-flow-of-the-tidyverse",
    "href": "02-nfl-analytics-tidyverse.html#the-flow-of-the-tidyverse",
    "title": "\n3  Wrangling NFL Data in the tidyverse\n",
    "section": "\n3.4 The Flow of the tidyverse\n",
    "text": "3.4 The Flow of the tidyverse\n\nThe underlying design of coding in the tidyverse, aside from the dplyr verbs are both the assignment statement (<-) and the pipe (%>%). Please note, as mentioned in the book’s Preface, that I still use the pipe (%>%) that is part of the magrittr package and not the native pipe operator (|>) included in the 4.1 release of R. The choice of pipe operator you use is your decision to make, as either will work seamlessly within the examples and activities provided in this book.\nAs I explain to my Sports Analytics students, the language and flow of the tidyverse can seem like a foreign language at first. But, it is important that you stick with it because, sooner rather than later, the light bulb above your head will go off. Before detailing the in’s and out’s of the tidyverse in the below section, let’s first dissect an example of the tidyverse workflow.\n\npbp <- nflreadr::load_pbp(2022) %>%\n  filter(posteam == \"PHI\" & rush == 1) %>%\n  group_by(rusher) %>%\n  summarize(success_rate = mean(success))\n\nThe given example involves multiple iterations of the tidyverse paradigm. At the outset of my Sport Analytics course, when introducing the concepts of the tidyverse, I emphasize that it is possible to “talk your way through” the process from the beginning to your end goal (especially once you have you have a more comprehensive understanding of the dplyr verbs, which are expounded upon in the subsequent section). The following stepwise method illustrates this using the above example of code:\n\nWe first create a dataset, denoted by pbp, by utilizing the load_pbp function from the nflreadr package. To talk through this, you can say “pbpis an copy of nflreadr::load_pbp(2022.” While R language purists may laugh at teaching the tidyverse in such language, it does indeed work. Going forward, the assignment operator (<-) simply implies that “something is.” In this case, our pbp dataframe is the containers for the play-by-play data we are collecting from nflreadr.\nWe then move into our first pipe operator (%>%). Again, R language purist will likely develop a eye twitch upon reading this, but I explain to my students that the pipe operator serves as a “… and then” command. In terms of the “talk it out” method above, the flow would be: “pbp is a copy of nflreadr::load_pbp(2022) and then …”\nAfter the pipe operator (or the first “and then …” command), we move into our first dplyr verb. In this case, we are using the filter verb to select just the Philadelphia Eagles as the offensive team and just offensive plays that are rush attempts. With another pipe operator, we are including a second “… and then” command.\nTo finish the example, we are grouping by each individual rusher on the Eagles “and then” summarize the average success rate for each rusher.\n\nTo put it together, “talking it out” from beginning to end results in:\n“First, create a dataset called pbp that is a copy of nflreadr::load_pbp(2022) and then filter for all instances where the posteam is PHI and rush == 1 and then group_by each individual rusher, and then summarize the average success rate for each rusher into a new column titled success_rate.”\nTo showcase this visually, the “talking through” method is inputted into the example code below:\n\npbp <- \"is\" nflreadr::load_pbp(2022) %>% \"... and then\"\n  filter(posteam == \"PHI\" & rush == 1) %>% \"... and then\"\n  group_by(rusher) %>% \"... and then\"\n  summarize(success_rate = mean(success))"
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#working-with-nfl-data-and-the-dplyr-verbs",
    "href": "02-nfl-analytics-tidyverse.html#working-with-nfl-data-and-the-dplyr-verbs",
    "title": "\n3  Wrangling NFL Data in the tidyverse\n",
    "section": "\n3.5 Working With NFL Data and the dplyr Verbs",
    "text": "3.5 Working With NFL Data and the dplyr Verbs\nOf the packages nestled within the tidyverse, dplyr is perhaps the most important in terms of wrangling and cleaning data. As mentioned above, dplyr is a powerful tool for data manipulation in R as it provides a key set of functions, known as verbs, that are designed to be easy to user and understand. The verbs can be used to filter, group, summarize, rearrange, and transform all types of data sets. For those just starting their NFL analytics endeavors in the R programming language, the following four dplyr verbs are perhaps the most important. Specific examples of working with these verbs, as well as others, follow below.\n\n\nfilter: the filter verb allows you to subset data based on certain criteria. For example, you can use filter() to keep only those rows in a data set where a certain variable meets a certain conditions (ie., more than 100 completed passes). Moreover, the filter verb can be used in conjunction with logical operators such as & and | to create more complex criteria.\n\ngroup_by: the group_by() verb allows you to group a data set by one or more variables. It is a useful tool when you want to perform an operation on each group, such as calculating a summary statistic (ie, intended air yards per quarterback) or when creating a plot.\n\nsummarize: the summarize() verb allows you to reduce a data set to a single summary value. The summarize() verb is often used in conjunction with the group_by function, allowing you to group the data by one or more variables. The summarize() verb allows for a wide range of summary statistics, including means, medians, standard deviations, and more. You can also use it to calculate custom summary statistics.\n\n`mutate`: the mutate verbs allows you to create new variables within your data while also preserving existing ones.\n\n\n3.5.1 NFL Data and the filter() verb\nThe filter verb allows you to extract specific rows from your dataset based on one, or multiple, supplied conditions. The conditions are supplied to the filter verb by using logical operators, listed in the below table, that ultimately evaluate to either TRUE or FALSE in each row of the dataset. The filter process returns a dataset that includes only those rows that meet the specified conditions.\n\n\n\nLogical Operator\nMeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n<\nless than\n\n\n<=\nless than or equal to\n\n\n>\ngreater than\n\n\n>=\ngreater than or equal to\n\n\n!\nnot\n\n\n&\nand\n\n\n|\nor\n\n\n%in%\nincludes\n\n\nc()\nused to combine arguments into a vector\n\n\nis.na\nchecks for missing values\n\n\n!is.na\nis not missing specific values\n\n\n\n\nIn order to work through specific examples of the above logical operators, we will use 2022 play-by-play data from nflreadr. To begin, let’s read in the data:\n\n\n\n\n\n\nNote\n\n\n\nPlease note that a more detailed overview of reading in nflreadr data is provided in Chapter 3. For the purposes of learning about the filter verb, please make sure you have both the tidyverse and nflreadr loaded by running the following:\nlibrary(tidyverse)library(nflreadr)\nIf you have difficult with the above step, please see the Installing and Loading Necessary Packages section above.\n\n\n\npbp <- nflreadr::load_pbp(2022)\n\nAfter running the above code, you will have a dataset titled pbp placed into your RStudio environment consisting over 50,147 observations over 372 variables. With the dataset loaded, let’s create a new dataset titled ne_offense that contains only those plays where the New England Patriots are the offensive team.\n\nne_offense <- pbp %>%\n  filter(posteam == \"NE\")\n\nas_tibble(ne_offense)\n\n# A tibble: 1,323 x 372\n   play_id game_id old_g~1 home_~2 away_~3 seaso~4  week posteam poste~5 defteam\n     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>   <int> <chr>   <chr>   <chr>  \n 1      44 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 2      59 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 3      83 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 4     109 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 5     130 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 6     154 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 7     175 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 8     196 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n 9     236 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n10     571 2022_0~ 202209~ MIA     NE      REG         1 NE      away    MIA    \n# ... with 1,313 more rows, 362 more variables: side_of_field <chr>,\n#   yardline_100 <dbl>, game_date <chr>, quarter_seconds_remaining <dbl>,\n#   half_seconds_remaining <dbl>, game_seconds_remaining <dbl>,\n#   game_half <chr>, quarter_end <dbl>, drive <dbl>, sp <dbl>, qtr <dbl>,\n#   down <dbl>, goal_to_go <dbl>, time <chr>, yrdln <chr>, ydstogo <dbl>,\n#   ydsnet <dbl>, desc <chr>, play_type <chr>, yards_gained <dbl>,\n#   shotgun <dbl>, no_huddle <dbl>, qb_dropback <dbl>, qb_kneel <dbl>, ...\n\n\nThe as_tibble() output shows that the posteam variable contains only NE, which is the abbreviation for the New England Patriots in the nflreadr play-by-play data. In the code that produced ne_offense, it is important to notice that:\n\nthe “equal to” logical operator consist of TWO equal signs, not one.\nthe team abbreviation (NE) is in quotation marks.\n\nIn the first, a single equal sign (=) can be used as a synonym for assignment (<-) but is most often used when passing values into functions (which is covered in Chapter 5). To avoid confusion with these possibility, the test for equality when using filter is always ==.\nIn the second, you must use quotation marks around the character-based value you are placing into the filter() verb because, if not, R will interpret posteam incorrectly and ultimately generate an error. On the other hand, you do not need to include quotation marks if you are filtering out numeric-based variables. Below are incorrect and correct examples of both:\n\nne_offense <- pbp %>%\n  filter(posteam == NE) #this is incorrect. Character values must be in quotation marks.\n\nne_offense <- pbp %>%\n  filter(posteam == \"NE\") #this is correct for character values.\n\nne_offense <- pbp %>%\n  filter(air_yards >= \"5\") #this is incorrect. Numeric values do not need quotation marks.\n\nne_offense <- pbp %>%\n  filter(air_yards >= 5) #this is correct for numeric values.\n\nHow do we approach the logical operators if we want to retrieve every offensive team in the pbp data except for New England? In that case, we can use the “not equal to” (!=) operator:\n\nnot_ne_offense <- pbp %>%\n  filter(posteam != \"NE\")\n\nThe resulting dataset titled not_ne_offense will still include all 372 variables housed within the nflreadr play-by-play data, but will not include any row in which New England is the offensive (posteam) team.\nContinuing with examples, how do we use the filter verb on multiple teams at once? For instance, let’s use the above filtering process for offensive teams but only retrieve information from the play-by-play data for the four teams that comprise the AFC East (Buffalo Bills, Miami Dolphins, New England Patriots, and the New York Jets). There are, in fact, two logical operators that can produce the results we are looking for: the “or” logical operator (|) or by using the “includes” logical operator (%in%) combined with the “concatenate” operator (c()). Let’s start with using the “or” operator.\n\nafc_east <- pbp %>%\n  filter(posteam == \"NE\" | posteam == \"MIA\" | posteam == \"NYJ\" | posteam == \"BUF\")\n\nBy using the | logical operator, which translates to the word “or”, we can string together for separate filters for posteam within the play-by-platy data. That said, it probably seems odd to have to include the posteam argument four different times, rather than being able to do this:\n\nafc_east <- pbp %>%\n  filter(posteam == \"NE\" | \"MIA\" | \"NYJ\" | \"BUF\")\n\nWhile the above example logically makes sense (verbally saying “posteam equals NE or MIA or NYJ or BUF”), it unfortunately results in an error. To that end, if you’d rather avoid the need to type posteam four different times, as in the above example, you can switch to using the %in% operator combined with c(). It is possible to combine just filter() and the %in% operator to retrieve one specific team. But, as in the above example where, we will receive an error if we try to do it for multiple teams without including the c() operator, as such:\n\nafc_east <- pbp %>%\n  filter(posteam %in% c(\"NE\", \"MIA\", \"NYJ\", \"BUF\"))\n\nas_tibble(afc_east)\n\n# A tibble: 5,625 x 372\n   play_id game_id old_g~1 home_~2 away_~3 seaso~4  week posteam poste~5 defteam\n     <dbl> <chr>   <chr>   <chr>   <chr>   <chr>   <int> <chr>   <chr>   <chr>  \n 1      43 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 2      68 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 3      89 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 4     115 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 5     136 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 6     172 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 7     391 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 8     412 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n 9     436 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n10     469 2022_0~ 202209~ NYJ     BAL     REG         1 NYJ     home    BAL    \n# ... with 5,615 more rows, 362 more variables: side_of_field <chr>,\n#   yardline_100 <dbl>, game_date <chr>, quarter_seconds_remaining <dbl>,\n#   half_seconds_remaining <dbl>, game_seconds_remaining <dbl>,\n#   game_half <chr>, quarter_end <dbl>, drive <dbl>, sp <dbl>, qtr <dbl>,\n#   down <dbl>, goal_to_go <dbl>, time <chr>, yrdln <chr>, ydstogo <dbl>,\n#   ydsnet <dbl>, desc <chr>, play_type <chr>, yards_gained <dbl>,\n#   shotgun <dbl>, no_huddle <dbl>, qb_dropback <dbl>, qb_kneel <dbl>, ...\n\n\nThe above approach is a simplified version of first creating a vector of the team abbreviations and then passing that into the filter() argument. For example, we can create the following vector that appears in the “Values” area of your RStudio environment, and then use that to retrieve the same results in did in the above code:\n\nafc_east_teams_vector <- c(\"NE\", \"MIA\", \"NYJ\", \"BUF\")\n\nafc_east <- pbp %>%\n  filter(posteam %in% afc_east_teams_vector)\n\nas_tibble(afc_east_teams_vector)\n\n# A tibble: 4 x 1\n  value\n  <chr>\n1 NE   \n2 MIA  \n3 NYJ  \n4 BUF  \n\n\nIn other cases, we may need to do the opposite of above - that is, select all teams except for specific ones. For example, using data from Pro Football Focus regarding a quarterback’s average time to throw, let’s determine how to remove all AFC North teams (Pittsburgh, Cleveland, Baltimore, and Cincinnati) from the dataset while leaving all other teams intact. Such a procedure is necessary, for example, if we wanted to explore the average time to throw for the combined AFC North teams against the rest of the NFL.\nWe cannot leave the data as is and find the averages outright. We must remove the AFC North teams in order to avoid “baking them into” the league-wide average, thus skewing the results of our analysis. To complete this process, we turn to the ! (“is not”) logical operator.\nTo begin, we will use the vroom package to read in the data.\n\ntime_in_pocket <- vroom::vroom(\"https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/time_in_pocket.csv\")\n\nas_tibble(time_in_pocket)\n\n# A tibble: 30 x 5\n   player          position team_name player_game_count avg_time_to_throw\n   <chr>           <chr>    <chr>                 <dbl>             <dbl>\n 1 Patrick Mahomes QB       KC                       20              2.85\n 2 Tom Brady       QB       TB                       18              2.31\n 3 Justin Herbert  QB       LAC                      18              2.75\n 4 Joe Burrow      QB       CIN                      19              2.51\n 5 Josh Allen      QB       BUF                      18              2.92\n 6 Kirk Cousins    QB       MIN                      18              2.7 \n 7 Trevor Lawrence QB       JAX                      19              2.51\n 8 Geno Smith      QB       SEA                      18              2.79\n 9 Daniel Jones    QB       NYG                      18              3.03\n10 Jalen Hurts     QB       PHI                      18              2.86\n# ... with 20 more rows\n\n\nThe structure and organization of the data is quite similar to the datasets we worked with in our above examples, despite coming from a different source (PFF vs. nflreadr). Our time_in_pocket data includes a quarterback’s name on each row with his corresponding team, player_game_count, and avg_time_to_throw. Again, we are seeking to compare the average time to release - combined - for the AFC North against the rest of the NFL. In order to avoid skewing the results by “baking in data” (that is, not removing AFC North teams prior to finding the NFL average), we can use the ! logical operator to remove all four teams at once (as oppose to structuring the filter() to include four separate team_name != for all four different AFC North teams.\n\ntime_in_pocket_no_afcn <- time_in_pocket %>%\n  filter(!team_name %in% c(\"PIT\", \"BAL\", \"CIN\", \"CLE\"))\n\nas_tibble(time_in_pocket_no_afcn)\n\n# A tibble: 26 x 5\n   player          position team_name player_game_count avg_time_to_throw\n   <chr>           <chr>    <chr>                 <dbl>             <dbl>\n 1 Patrick Mahomes QB       KC                       20              2.85\n 2 Tom Brady       QB       TB                       18              2.31\n 3 Justin Herbert  QB       LAC                      18              2.75\n 4 Josh Allen      QB       BUF                      18              2.92\n 5 Kirk Cousins    QB       MIN                      18              2.7 \n 6 Trevor Lawrence QB       JAX                      19              2.51\n 7 Geno Smith      QB       SEA                      18              2.79\n 8 Daniel Jones    QB       NYG                      18              3.03\n 9 Jalen Hurts     QB       PHI                      18              2.86\n10 Jared Goff      QB       DET                      17              2.7 \n# ... with 16 more rows\n\n\nWith Pittsburgh, Baltimore, Cincinnati, and Cleveland removed from the dataset, we can - for the sake of completing the analysis - take our new time_in_pocket_afcn dataset and find the league-wide average for avg_time_to_throw.\n\ntime_in_pocket_no_afcn %>%\n  summarize(combined_average = mean(avg_time_to_throw))\n\n# A tibble: 1 x 1\n  combined_average\n             <dbl>\n1             2.76\n\ntime_in_pocket %>%\n  filter(team_name %in% c(\"PIT\", \"BAL\", \"CIN\", \"CLE\")) %>%\n  summarize(combined_average = mean(avg_time_to_throw))\n\n# A tibble: 1 x 1\n  combined_average\n             <dbl>\n1             2.89\n\n\nThe AFC North quarterbacks had an average time to throw of 2.76 seconds while the rest of the NFL averaged 2.89 seconds, meaning the AFC North QBs released the ball, on average, 0.13 seconds quicker than the rest of the quarterbacks in the league.\nSo far, all our filter() operations have include just one logical operator, or multiple values built into one operator using the %in% option. However, because of the structure of the logical operators, we are able to construct a single filter() combined with various operator requests. For example, let’s gather only those rows that fit the following specifications:\n\nthat play occurs on 2nd down.\nthe play must be a pass play.\nthe play must include a charted QB hit.\nthe play must result in a complete pass.\nthe pass must have an air yards amount that is greater than or equal to the yards to go.\n\n\n\n\n\n\n\nTip\n\n\n\nCompiling the above filter() requires knowledge of the variable names within nflreadr play-by-play data. This is covered in Chapter 3.\nThat said, it is important to point out that many of the above variables have data values that are structured in binary - that is, the value is either 1 or 0.\nIn the above list, the qb_hit variable is binary. A play with a QB hit is denoted by the inclusion of a numeric 1 in the data, while the inclusion of a numeric 0 indicates that a QB hit did not occur.\n\n\nBuilding the filter() based on the above specifications requires the use of both character and numeric values:\n\nmultiple_filters <- pbp %>%\n  filter(down == 2 & play_type == \"pass\" & qb_hit == 1 & complete_pass == 1 & \n           air_yards >= ydstogo)\n\nThe resulting dataset, multiple_filters, indicates that exactly 105 such instances took place during the 2002 NFL season.\nLastly, in some instances, it is necessary to use filter() to remove rows that may provide you with incorrect information if include. To showcase this, it is necessary to also include the summarize() verb, which is covered in greater detail in the following two section. For example, let’s gather information from the play-by-play data that will provide the total amount of yards per scrimmage - combined - for the 2022 regular season. Using our knowledge from the prior example, we can write code that first uses the filter() verb to retrieve just those rows that took place during the regular season and then uses the summarize() verb to distill the information down to the singular, combined total:\n\ntotal_yards <- pbp %>%\n  filter(season_type == \"REG\") %>%\n  summarize(yards = sum(yards_gained, na.rm = TRUE))\n\nas_tibble(total_yards)\n\n# A tibble: 1 x 1\n   yards\n   <dbl>\n1 184444\n\n\nHow many total scrimmage yards did NFL teams earn during the 2022 NFL regular season? According to the results from our above code, the answer is 184,444. However, that is incorrect.\nThe right answer, in fact, is 184,332 total scrimmage yards. How did the results of our above coding come up 112 shorts of the correct answer (as provided by the 2022 Team Stats page on Pro Football Reference)?\nThe difference is the result of the above code including all plays, including 2-point conversion attempts. Because a 2-point conversion is technically considered an extra point attempt - not a play from scrimmage or a official down - no player stats are officially record for the play. We can verify this is the case by using the filter() verb to include only those rows that include regular-season games and those plays that do not include information pertaining to down:\n\ntwo_points <- pbp %>%\n  filter(season_type == \"REG\" & is.na(down)) %>%\n  summarize(total = n(),\n            total_yards = sum(yards_gained, na.rm = TRUE))\n\nas_tibble(two_points)\n\n# A tibble: 1 x 2\n  total total_yards\n  <int>       <dbl>\n1  8137         112\n\n\nThe resulting as_tibble() indicates that exactly 112 scrimmage yards were gained on places missing down information. There was, of course, not 8,137 2-point conversion attempts during the 2022 regular season. There are many other instances within the data that do not include information for down, including: kickoffs, extra points, time outs, the end of a quarter, etc. We can be more specific with our filter() argument in the above code to include only the binary argument for the two_point_attempt variable:\n\ntwo_points_true <- pbp %>%\n  filter(season_type == \"REG\" & two_point_attempt == 1) %>%\n  summarize(total = n(),\n            total_yards = sum(yards_gained, na.rm = TRUE))\n\ntwo_points_true\n\n# A tibble: 1 x 2\n  total total_yards\n  <int>       <dbl>\n1   119         112\n\n\nBy altering the `is.na(down) argument to the binary indicator for two_point_attempt, we see that there were 119 2-point conversion attempts in the 2022 regular season while maintain the correct 112 yards that explains our difference from above. To account for our new-found knowledge that we must remove 2-point conversions to calculate the correct amount of total scrimmage yards, we can use the !is.na() operator (where !is.na() means “is not missing this specific variable) to remove all rows that do not include information pertaining to down:\n\ntotal_yards_correct <- pbp %>%\n  filter(season_type == \"REG\" & !is.na(down)) %>%\n  summarize(yards = sum(yards_gained, na.rm = TRUE))\n\nas_tibble(total_yards_correct)\n\n# A tibble: 1 x 1\n   yards\n   <dbl>\n1 184332\n\n\nBy filtering out any row that does not include the down for a given play, we correctly calculated the total amount of scrimmage yards - 184,332 - for the 2022 NFL regular season.\n\n3.5.2 NFL Data and the group_by() verb\nAs mentioned above, the group_by verb allows you to group data by one or more specific variables in order to conducted, among other actions, summary statistics. To showcase how group_by is used within the nflverse data, let’s first gather the 2022 regular season statistics and then use the summarize verb to get the average success rate on rushing plays.\nAs well, we immediately make use of the filter function to do sort the data : (1.) we first instruct to filter the data to include just those instances where the play_type equals run, (2.) we then say it must also be play == 1, meaning there was no penalty or other interruption that “cancelled out” the play, and (3.) we lastly pass the argument that the down cannot be missing by using !is.na as a missing down is indicative of a two-point conversion attempt.\n\n\n\n\nrushing_success_ungrouped <- pbp %>%\n  filter(play_type == \"run\" & play == 1 & !is.na(down)) %>%\n  summarize(success_rate = mean(success))\n\nrushing_success_ungrouped\n\n# A tibble: 1 x 1\n  success_rate\n         <dbl>\n1        0.431\n\n\nWithout including the group_by verb within the above code, the output is the average success rate for rushing plays for all 32 NFL teams, wherein success rate is the percentage of rushing plays that resulted in an EPA above zero. In this case, approximately 43% of NFL rushes had a positive success rate.\nThat said, we are interested in examining the success rate by team, not league-wide average. To do so, we add the posteam variable into the group_by verb.\n\nrushing_success_grouped <- pbp %>%\n  filter(play_type == \"run\" & play == 1 & !is.na(down)) %>%\n  group_by(posteam) %>%\n  summarize(success_rate = mean(success)) %>%\n  arrange(-success_rate)\n\nrushing_success_grouped %>%\n  slice(1:10)\n\n# A tibble: 10 x 2\n   posteam success_rate\n   <chr>          <dbl>\n 1 PHI            0.526\n 2 BUF            0.488\n 3 BAL            0.478\n 4 GB             0.476\n 5 PIT            0.470\n 6 ATL            0.463\n 7 KC             0.459\n 8 NYG            0.454\n 9 CIN            0.454\n10 LV             0.449\n\n\nIn the above example, we have added the offensive team into the group_by verb, while also arranging the data in descending order by success_rate, and then used slice to gather just the ten teams with the highest rushing success rate. The Philadelphia Eagles led the NFL in rushing success rate during the 2022 NFL regular season at 52.3%. By removing the slice function in the above example, we can see that Tampa Bay maintained the worst rushing success rate in the league at 37.3%.\nWhile determining the rushing success rate of teams is interesting, we can also determine the same metric for individual running backs as well. To do so, we simply replace the variable in the group_by verb. In the below example, we replace the posteam variable with the rusher variable to see which running backs have the highest success rate.\n\nrunning_back_success <- pbp %>%\n  filter(play_type == \"run\" & play == 1 & !is.na(down)) %>%\n  group_by(rusher) %>%\n  summarize(success_rate = mean(success)) %>%\n  arrange(-success_rate)\n\nrunning_back_success %>%\n  slice(1:10)\n\n# A tibble: 10 x 2\n   rusher      success_rate\n   <chr>              <dbl>\n 1 A.Davis                1\n 2 B.Aiyuk                1\n 3 B.Allen                1\n 4 B.Skowronek            1\n 5 C.Kmet                 1\n 6 C.Sutton               1\n 7 C.Wilson               1\n 8 D.Bellinger            1\n 9 D.Brown                1\n10 D.Gray                 1\n\n\nThe output, unfortunately, is not all that helpful. Because we did not use the filter verb to stipulate a minimum number of rushing attempts, the output is saying that - for example, Daniel Bellinger, a tight end, has among the most successful rushers in the league with a 100% rushing success rate. To correct this, we must add a second metric to our summarize verb (we will call it n_rushes) and then use the filter verb afterwards to include a minimum number of rushes required to be included in the final output.\nAs well, we will provide an additional argument in the first filter verb that stops the output from including any rushing attempt that does not include the running back’s name. The n_rushes() in the summarize verb allows use to now include the number of attempts, per individual rusher, that fall within the first filter parameter. Afterwards, we include a second filter argument to include just those rushers with at least 200 attempts.\n\nrunning_back_success_min <- pbp %>%\n  filter(play_type == \"run\" & play == 1 & !is.na(down) & !is.na(rusher)) %>%\n  group_by(rusher) %>%\n  summarize(success_rate = mean(success), n_rushes = n()) %>%\n  filter(n_rushes >= 200) %>%\n  arrange(-success_rate)\n\nrunning_back_success_min %>%\n  slice(1:10)\n\n# A tibble: 10 x 3\n   rusher      success_rate n_rushes\n   <chr>              <dbl>    <int>\n 1 M.Sanders          0.483      294\n 2 I.Pacheco          0.469      207\n 3 A.Jones            0.465      213\n 4 J.Jacobs           0.444      340\n 5 N.Chubb            0.434      302\n 6 T.Allgeier         0.429      210\n 7 Ja.Williams        0.427      262\n 8 T.Etienne          0.424      250\n 9 J.Mixon            0.422      249\n10 B.Robinson         0.420      205\n\n\nUnsurprisingly, Miles Sanders - a running back for the Eagles, who lead the NFL in team success rate - is the leader in rushing success among individual players with 49% of his attempts gaining positive EPA.\nMuch like the filter() verb, we are able to supply multiple arguments within one group_by() verb. This is necessary, for example, when you want to examine results over the course of multiple seasons. Let’s explore this by determining each team’s yards after catch between 2010 and 2022. To start, we will retrieve all NFL play-by-play data from those seasons.\n\n\n\n\n\n\nDanger\n\n\n\nLoading in multiple seasons of play-by-play data can be taxing on your computer’s memory. RStudio’s memory usage monitor, located in ‘Environment’ toolbar tell you how much data is currently being stored in memory. As needed, you can click on the usage monitor and select ‘Free Unused Memory’ to purge any data that is no longer also in your ‘Environment.’\n\n\n\nmultiple_seasons <- nflreadr::load_pbp(2010:2022)\n\nOnce the dataset is loaded, we can think ahead to what is needed in the group_by() verb. In this case, we are exploring team results on a season basis. Because of this, we will include both posteam and season in the group_by() verb.\n\nyac_seasons <- multiple_seasons %>%\n  filter(season_type == \"REG\" & !is.na(posteam) & !is.na(yards_after_catch)) %>%\n  group_by(season, posteam) %>%\n  summarize(total_yac = sum(yards_after_catch, na.rm = TRUE))\n\n\n3.5.3 NFL Data and the summarize() verb\nAs we’ve seen, the summarize function can be used to find summary statistics based whichever option we pass to it via the group_by verb. However, it can also be used to create new metrics built off data included in the nflverse play-by-play data.\nFor example, let’s examine which teams were the most aggressive on 3rd and short passing attempts during the 2022 season. Of course, determining our definition of both what “short” is on 3rd down and “aggressive” is quite subjective. For the purposes of this example, however, let’s assume that 3rd and short is considered 3rd down with five or less yards to go and that “aggressive” is a quarterback’s air yards being to, at minimum, the first-down marker.\nMust like our above examples working with rushing success rate, we begin constructing the metric with the filter argument. In this case, we are filtering for just pass plays, we want the down to equal 3, the yards to go to be equal to or less than 5, we want it to be an official play, and we do not want it to be missing the down information. After the initial filter process, we include the posteam variable within our group_by verb.\nIn our summarize section, we are first getting the total number of times each team passed the ball on 3rd down with no more than five yards to go. After, we are creating a new aggressiveness column that counts the number of times a quarterback’s air yards were, at minimum, the required yards for a first down. Next, we create another new column titled percentage that takes aggressiveness and divides it by total.\n\nteam_aggressiveness <- pbp %>%\n  filter(play_type == \"pass\" & down == 3 & ydstogo <= 5 & play == 1 & !is.na(down)) %>%\n  group_by(posteam) %>%\n  summarize(total = n(),\n            aggressiveness = sum(air_yards >= ydstogo, na.rm = TRUE),\n            percentage = aggressiveness / total) %>%\n  arrange(-percentage)\n\nteam_aggressiveness %>%\n  slice(1:10)\n\n# A tibble: 10 x 4\n   posteam total aggressiveness percentage\n   <chr>   <int>          <int>      <dbl>\n 1 LV         60             50      0.833\n 2 BUF        61             47      0.770\n 3 ARI        58             44      0.759\n 4 SF         67             50      0.746\n 5 PIT        79             58      0.734\n 6 SEA        60             44      0.733\n 7 NE         54             39      0.722\n 8 TB         92             66      0.717\n 9 MIA        62             43      0.694\n10 CHI        42             29      0.690\n\n\nThe Las Vegas Raiders, based on our definitions, are the most aggressive passing team in the league on 3rd and short as just over 83% of their air yards were at - or past - the required yardage for a first down. On the other end of the spectrum, the New York Giants were the least aggressive team during the 2022 regular season, at 49.1%.\n\n3.5.4 NFL Data and the mutate() verb\nIn the our example above working with the summarize verb, our output includes only the information contained in our group_by and then whatever information we provided in the summarize() (such as total, aggressiveness, and percentage).\nWhat if, however, you wanted to create new variables and then summarize() those? That is where the mutate verb is used.\nAs an example, let’s explore individual quarterback’s average completion percentage over expected for specific air yard distances. To start, we can attempt to do this simply by including both passer and air_yards in the group_by verb.\n\nairyards_cpoe <- pbp %>%\n  group_by(passer, air_yards) %>%\n  summarize(avg_cpoe = mean(cpoe, na.rm = TRUE))\n\nYour output is going to include the avg_cpoe for each quarterback at each and every distance of air_yards. Not only is it difficult to find meaning in, but it would prove to be difficult - if not impossible - to visualize with ggplot. To correct this issue, we must use the mutate verb.\nRather than summarize the completion percentage over expected for each distance of air_yards, we can use the mutate verb to bundle together a grouping of distances. In the below example, we are using the mutate verb to create a new variable titled ay_distance using the case_when verb.\n\nairyards_cpoe_mutate <- pbp %>%\n  filter(!is.na(cpoe)) %>%\n  mutate(\n    ay_distance = case_when(\n      air_yards < 0 ~ \"Negative\",\n      air_yards >= 0 & air_yards < 10 ~ \"Short\",\n      air_yards >= 10 & air_yards < 20 ~ \"Medium\",\n      air_yards >= 20 ~ \"Deep\")) %>%\n  group_by(passer, ay_distance) %>%\n  summarize(avg_cpoe = mean(cpoe))\n\nWith the air_yards data now binned into four different groupings, we can examine quarterbacks at specific distances.\n\nairyards_cpoe_mutate %>%\n  filter(ay_distance == \"Medium\") %>%\n  arrange(-avg_cpoe) %>%\n  slice(1:10)\n\n# A tibble: 82 x 3\n# Groups:   passer [82]\n   passer     ay_distance avg_cpoe\n   <chr>      <chr>          <dbl>\n 1 A.Brown    Medium        -6.09 \n 2 A.Cooper   Medium       -43.1  \n 3 A.Dalton   Medium         5.01 \n 4 A.Rodgers  Medium         2.02 \n 5 B.Allen    Medium        44.5  \n 6 B.Hoyer    Medium       -44.0  \n 7 B.Mayfield Medium       -15.4  \n 8 B.Perkins  Medium         0.266\n 9 B.Purdy    Medium         9.72 \n10 B.Rypien   Medium        18.0  \n# ... with 72 more rows"
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#core-skills-for-tidy-data",
    "href": "02-nfl-analytics-tidyverse.html#core-skills-for-tidy-data",
    "title": "\n3  Wrangling NFL Data in the tidyverse\n",
    "section": "\n3.6 Core Skills for Tidy Data",
    "text": "3.6 Core Skills for Tidy Data\nWith your new understanding of the tidyverse flow in the R programming language, we are now going to hone these core skills by taking a dataset from ingestion through the cleaning and prepping process so that it is prepared for eventual data visualization (which will be done in Chapter 4 of this book). To do so, we are going to use data provided by Sports Info Solutions. Based out of Allentown, Pennsylvania, SIS collects its play-by-play and then does a manual hand-charting process to provide weekly data that covers a vast array of specific occurrences in football (defensive scheme, pre-snap motion, wide receivers formations, offensive personnel, drop type, and more).\nIn this example, we are going to compare the Boom% and the Bust% for each quarterback in the data, wherein the Boom% correlates to any pass attempt by the quarterback that resulted in an expected points add (EPA) of at least 1 and Bust% is any pass attempt that resulted in an EPA of at least -1. In order to prepare the data for visualization in ggplot, we will:\n\nImport and examine the data.\nDeal with missing values in the data.\nCharge variable types to match what it needed for visualization.\nUsing the mutate verb to correct and create new metrics within the data.\nMerging the data with a secondary dataset.\n\n\n3.6.1 Importing and Conducting Exploratory Data Analysis\nTo start, let’s examine data from Sports Info Solutions by reading in the data from this book’s Git repository using the Vroom package.\n\nsis_data <- vroom::vroom(\"https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/sis_boom_bust.csv\")\n\nUsing vroom::vroom, we are reading in the .csv file into our environment and assigning it the name sis_data. With access to the data, we can begin our exploratory data analysis - the importance of which cannot be understated. The process of EDA results in several key items:\n\nEDA helps to identify potential issues in the dataset, such as missing or erroneous data, outliers, or inconsistent values - all of which must be addressed prior to further analysis or visualization.\nConducting an EDA allows you to gain a deeper understanding of the distribution, variability, and relationships between all the variables contained in the dataset, thus allowing you to make informed decisions on appropriate statistical techniques, models, and visualizations that can best analyze and communicate the data effectively.\nThe EDA process can help you discover patterns or trends in the data that may be of interest or relevance to your research question, or help you discover to questions to answer.\n\nTo start the process of EDA on our newly created sis_data dataset, let’s examine the current status fo the dataset using the as_tibble() prompt:\n\nas_tibble(sis_data)\n\n# A tibble: 107 x 14\n   Season Player      Team    Att Point~1 PE Pe~2 Point~3 PAA P~4    EPA Posit~5\n    <dbl> <chr>       <chr> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl> <chr>  \n 1   2022 Patrick Ma~ Chie~   648    180.   0.267   101.    0.149 119.   52.70% \n 2   2022 Justin Her~ Char~   699    138.   0.187    54.2   0.074  -8.11 46.30% \n 3   2022 Trevor Law~ Jagu~   584    123.   0.202    48.0   0.079  32.8  48.90% \n 4   2022 Jared Goff  Lions   587    121.   0.199    48.4   0.079  64.7  49.00% \n 5   2022 Jalen Hurts Eagl~   460    106.   0.213    48.1   0.097  10.9  45.80% \n 6   2022 <NA>        <NA>    439    104.   0.12     41.2   0.087   8.92 45.70% \n 7   2022 Kirk Cousi~ Viki~   643    104.   0.151    24.0   0.035 -12.7  45.70% \n 8   2022 Tua Tagova~ Dolp~   400    102.   0.243    44.9   0.107  66.2  50.40% \n 9   2022 Joe Burrow  Beng~   606    102.   0.158    25.7   0.04   19.4  49.50% \n10   2022 Josh Allen  Bills   567    100.   0.167    29.4   0.049  47.9  50.30% \n# ... with 97 more rows, 4 more variables: PAR <dbl>, WAR <dbl>, `Boom%` <chr>,\n#   `Bust%` <chr>, and abbreviated variable names 1: `Points Earned`,\n#   2: `PE Per Play`, 3: `Points Above Avg`, 4: `PAA Per Play`, 5: `Positive%`\n\n\nThe results immediately bring to light several issues in the data:\n\nThere are several instances of missing data (as indicated by NA values in the 6th row).\nMany of the column names are not in tidy format. Specifically, the Boom% and Bust% columns, because of the inclusion of the percentage sign, require the small ticks at either end of the word. The same issue is apparent in Points Earned as it includes a space between the word and, finally, many of the variable names are in full caps.\nThe Boom% and Bust% columns are both listed as <chr> which indicates that they are currently character-based columns, rather than the needed numeric-based column.\nThe above issue is caused by all the values within Boom% and Bust% containing a percentage sign at the tail end.\nLastly, the team variable for Baker Mayfield’s has a value of “2 teams” since he played for both the Carolina Panthers and Los Angeles Rams during the 2022 season.\n\nTo begin dealing with these issues, let’s first tackle cleaning the variable names and then selecting just those columns relevant to our forthcoming data visualization in Chapter 4:\n\nsis_data <- sis_data %>%\n  janitor::clean_names() %>%\n  select(player, team, att, boom_percent, bust_percent)\n\nas_tibble(sis_data)\n\n# A tibble: 107 x 5\n   player          team       att boom_percent bust_percent\n   <chr>           <chr>    <dbl> <chr>        <chr>       \n 1 Patrick Mahomes Chiefs     648 25.10%       12.50%      \n 2 Justin Herbert  Chargers   699 21.20%       15.90%      \n 3 Trevor Lawrence Jaguars    584 23.70%       15.20%      \n 4 Jared Goff      Lions      587 23.60%       13.30%      \n 5 Jalen Hurts     Eagles     460 23.50%       16.10%      \n 6 <NA>            <NA>       439 <NA>         10.10%      \n 7 Kirk Cousins    Vikings    643 21.30%       17.40%      \n 8 Tua Tagovailoa  Dolphins   400 28.50%       15.70%      \n 9 Joe Burrow      Bengals    606 23.60%       14.80%      \n10 Josh Allen      Bills      567 23.20%       12.70%      \n# ... with 97 more rows\n\n\nIn the above code, we are using the clean_names() function within the janitor package to automatically rework the column names into a tidy format. After, we use the select verb to keep only five of the included columns of data (player, team, att, boom_percent, and bust_percent). With the column names clean and easy to implement within our code, and with the relevant columns selected, we can move to the process of dealing with the missing data.\n\n3.6.2 Dealing with Missing Data\nDealing with missing data is an important step in the exploratory data analysis process, as its inclusion can introduce bias, reduce statistical power, and ultimately influence the validity of your research findings. In any case, a missing value within an RStudio dataset is represented by NA values and there are several different approaches to handling the missing data in an effective manner.\nA dataset can present missing data for a variety of reasons, including participant non-responses (such in social science research), measurement error, or simple data processing issues. The process of handling missing data is ultimately the result of the end goal of the analysis. In some case, conducting a imputation (such as replacing all NA values with the mean of all existing data) is used. In other cases, a more complex method can be utilized such as the mice package, which can impute missing data based on several different machine learning approaches (predictive mean matching, logistic regression, Bayesian polytomous regression, proportional odds model, etc.). Ultimately, the choice of method for handling missing data depends on the attributes of the dataset, your research question, and your end goals for the data.\nTo begin exploring missing values in our sis_data dataset, we can run the following code:\n\nsis_data_missing <- sis_data %>%\n  filter_all(any_vars(is.na(.)))\n\nsis_data_missing\n\n# A tibble: 1 x 5\n  player team    att boom_percent bust_percent\n  <chr>  <chr> <dbl> <chr>        <chr>       \n1 <NA>   <NA>    439 <NA>         10.10%      \n\n\nBy using filter_all across all of our variables (any_vars), we can filter for any missing value, indicated by including is.na in the filter. The output shows each row within the dataset that includes at least missing value. In this case, we only have one row with missing data, with values for player, team, and boom_percent all missing. Given the nature of our data, it is not prudent to impute a value in place of the missing data as we have no ability to accurately determine who the player is.\nInstead, we will return to our original dataset (sis_data) and use the na.omit function to drop any row that includes missing data:\n\nsis_data <- na.omit(sis_data)\n\nIn the above code, we are simply “recreating” our existing dataset but using na.omit to drop rows that include NA values. Once complete, we can move onto the next step of the data preparation/cleaning process.\n\n3.6.3 Changing Variable Types\nAs discovered in the exploratory data analysis process, we know that four columns (player, team, boom_percent, and bust_percent) are listed as a character data type while each quarterback’s number of attempts (att) is listed as a numeric. While we do want both player and team names to be a character-based datatype, we need both the boom and bust percentage for each quarterback to be a numeric value rather than a character.\nMoreover, the values in each boom_percent and bust_percent include a percentage sign at the tail-end.\nBecause of this, we have two issues we need to correct in order to eventually bring the data over to Chapter 4 for visualization:\n\ncorrectly changing the variable type for boom_percent and bust_percent\n\nremoving the % from the end of each value in boom_percent and bust_percent\n\n\nMuch like many things while working in the R programming language, there exists more than one way to tackle the above issues. In this specific case, we will first look at a method that uses the stringr package before switching the columns from character to numeric using an approach rooted in base R. After, we will use a much easier method to do both at one time.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note here that I do not necessarily endorse using the first method below to change variable types and to drop the percentage sign, as the second option is ideal.\nHowever, it is important to see how both methods work as there could be a case, somewhere down the road when you are exploring and preparing data yourself, that the first option is the only or best option.\nAs mentioned, there are typically multiple ways to get to the same endpoint in the R programming language. Showcasing both methods below simply provides you more options in your toolkit for tidying data.\n\n\n\n3.6.3.1 Method #1: Using stringr and Changing Variables\nIn the below example, we are creating an “example” database titled sis_data_stringr from our current iteration of SIS data and then pipe into the mutate verb (covered in more depth in the below Creating New Variables section). It is within this mutate verb that we can drop the percentage sign.\nWe first indicate that we are going to mutate our already existing columns (both boom_percent and bust_percent). After, we use an = sign to indicate that the following string is the argument to create or, in our case, edit the existing values in the column.\nWe then use the str_remove function from the stringr package to locate the % sign in each value and remove it.\nAfterwards, we dive into base R (that is, not tidyverse structure) and use the as.numeric function to change both of the boom and bust columns to the correct data type (using the $ sign to notate that we are working on just one specific column in the data.\n\nsis_data_stringr <- sis_data %>%\n  mutate(boom_percent = str_remove(boom_percent, \"%\"),\n         bust_percent = str_remove(bust_percent, \"%\"))\n\nsis_data_stringr$boom_percent <- as.numeric(as.character(sis_data_stringr$boom_percent))\nsis_data_stringr$bust_percent <- as.numeric(as.character(sis_data_stringr$bust_percent))\n\nas_tibble(sis_data_stringr)\n\n# A tibble: 106 x 5\n   player          team       att boom_percent bust_percent\n   <chr>           <chr>    <dbl>        <dbl>        <dbl>\n 1 Patrick Mahomes Chiefs     648         25.1         12.5\n 2 Justin Herbert  Chargers   699         21.2         15.9\n 3 Trevor Lawrence Jaguars    584         23.7         15.2\n 4 Jared Goff      Lions      587         23.6         13.3\n 5 Jalen Hurts     Eagles     460         23.5         16.1\n 6 Kirk Cousins    Vikings    643         21.3         17.4\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7\n 8 Joe Burrow      Bengals    606         23.6         14.8\n 9 Josh Allen      Bills      567         23.2         12.7\n10 Geno Smith      Seahawks   572         20.6         17.6\n# ... with 96 more rows\n\n\nOur output using as_tibble() now shows that att, boom_percent, and bust_percent are now all three numeric and, moreover, the tailing percentage sign in each boom and bust value is now removed. At this point, the data is prepped and properly structured for the visualization process.\n\n3.6.3.2 Method #2: Using the parse_number Function in readr\n\nWhile the above example using stringr and base R is suitable for our needs, we can use the readr package - and its parse_number function - to achieve the same results in a less verbose manner.\n\nsis_data <- sis_data %>%\n  mutate(boom_percent = readr::parse_number(boom_percent),\n         bust_percent = readr::parse_number(bust_percent))\n\nas_tibble(sis_data)\n\n# A tibble: 106 x 5\n   player          team       att boom_percent bust_percent\n   <chr>           <chr>    <dbl>        <dbl>        <dbl>\n 1 Patrick Mahomes Chiefs     648         25.1         12.5\n 2 Justin Herbert  Chargers   699         21.2         15.9\n 3 Trevor Lawrence Jaguars    584         23.7         15.2\n 4 Jared Goff      Lions      587         23.6         13.3\n 5 Jalen Hurts     Eagles     460         23.5         16.1\n 6 Kirk Cousins    Vikings    643         21.3         17.4\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7\n 8 Joe Burrow      Bengals    606         23.6         14.8\n 9 Josh Allen      Bills      567         23.2         12.7\n10 Geno Smith      Seahawks   572         20.6         17.6\n# ... with 96 more rows\n\n\nThe above example, using readr, is quite similar to the first version that used stringr in that we are using the mutate verb to edit our existing boom_percent and bust_percent columns. In this case, however, we are replacing our stringer argument with the parse_number function from the readr package. When finished, you can see in the str() output that not only are both boom and bust correctly listed as numeric, but the parse_number function automatically recognized and dropped the tailing percentage sign.\nWhile both examples get us to the same endpoint, using the readr package allows us to get there with less work.\n\n3.6.4 Correct and Create New Variables Using mutate\n\nThe mutate verb is a powerful and widely used tool that allows us to create new variables (columns) based on existing ones within our dataset. To that end, the mutate verb applies a user-supplied argument to each row of the dataset to create the new metric. Additionally, the mutate verb - when combined with the case_when() argument - allows us to make correction to the data (such as the aforementioned issues with Mayfield’s teams).\nThe syntax of the mutate verb follows the tidyverse flow like many of the other dplyr verbs:\n\nmutate(new_column_name = user_supplied_argument(existing_column_name))\n\nThe mutate verb is highly flexible, allowing us to create a new column based on a wide-range of functions and arguments, including mathematical operations, logical operations, string operation, and many more - including applying functions from other packages.\nImportantly, the mutate verb allows us to create new variables without changing the original dataset. While the summarize verb can also create new columns based on various functions and argument, it also distills the dataset down to just the columns created within the summarize function (plus any column including in the group_by()). Conversely, the mutate verb creates the new column and adds it on to the current iteration of the dataset.\nTo showcase this, let’s create a new column in our sis_data that calculate the difference between a quarterback’s boom_percent and bust_percent. This is also a great time to limit the quarterbacks we include based on the number of attempts for each. Not doing so results in oddities in the data, such as Christian McCaffrey (a running back) having a 100-percent boom percentage on one passing attempt. To avoid such occurrences, let’s add the filter verb prior to our mutate and limit the data to just those quarterbacks with at least 200 passing attempts:\n\nsis_data <- sis_data %>%\n  filter(att >= 200) %>%\n  mutate(difference = boom_percent - bust_percent)\n\nas_tibble(sis_data)\n\n# A tibble: 33 x 6\n   player          team       att boom_percent bust_percent difference\n   <chr>           <chr>    <dbl>        <dbl>        <dbl>      <dbl>\n 1 Patrick Mahomes Chiefs     648         25.1         12.5      12.6 \n 2 Justin Herbert  Chargers   699         21.2         15.9       5.3 \n 3 Trevor Lawrence Jaguars    584         23.7         15.2       8.5 \n 4 Jared Goff      Lions      587         23.6         13.3      10.3 \n 5 Jalen Hurts     Eagles     460         23.5         16.1       7.4 \n 6 Kirk Cousins    Vikings    643         21.3         17.4       3.90\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7      12.8 \n 8 Joe Burrow      Bengals    606         23.6         14.8       8.8 \n 9 Josh Allen      Bills      567         23.2         12.7      10.5 \n10 Geno Smith      Seahawks   572         20.6         17.6       3   \n# ... with 23 more rows\n\n\nIn the above example, by using the mutate verb, we created a new column in our existing sis_data dataset that calculates the different between a quarterback’s boom_percent and bust_percent. Importantly, as mentioned, this is done while keeping the rest of the dataset in place unlike using the summarize verb.\nAs mentioned in the introduction to this section, it is possible to include other function within the mutate verb. In fact, doing so is necessary if we wish to make the correction to Baker Mayfield’s team. Currently, Mayfield’s value for team is listed as 2 teams. Instead, let’s replace that with his most recent team in the 2022 NFL season (Rams).\nTo make this correction, we will begin by using the mutate verb on the existing team column in our dataset. However, where we calculated the different between boom and bust percent in the above example of mutate, we will now use the case_when() function.\nThe case_when() function allows us to either correct values, or create new columns, based on multiple conditions and values. The syntax of case_when() is as follows:\n\nmutate(column_name = case_when(\n  condition_1 == value_1 ~ new_value,\n  TRUE ~ default_value))\n\nIn the above code, we are essentially saying: “if condition_1 equals value_1, then replace the column_name’s existing value with this new_value.” The final argument, TRUE ~ default_value, specifies the default value to be used if none of the prior conditions are TRUE.\nTo make this more concrete, let’s working through it using our issue with Baker Mayfield. In order to make the correction, we need to supply the case_when() argument three items from the data: the column name that we are applying the mutate to (in this case, team), the first condition (in this case, player), and the value of that condition (in this case, Baker Mayfield). Next, we need to include the new value (in this case, Rams), and then supply what to do if the above condition is not TRUE. In other words, if the player is not Baker Mayfield then simply keep the existing team in place.\nFollowing that verbal walk through of the process, we can place the conditions and values into the correct spot in the relevant code:\n\nsis_data <- sis_data %>%\n  mutate(team = case_when(\n    player == \"Baker Mayfield\" ~ \"Rams\",\n    TRUE ~ team))\n\nsis_data %>%\n  filter(player == \"Baker Mayfield\") %>%\n  as_tibble()\n\n# A tibble: 1 x 6\n  player         team    att boom_percent bust_percent difference\n  <chr>          <chr> <dbl>        <dbl>        <dbl>      <dbl>\n1 Baker Mayfield Rams    335           17         18.9      -1.90\n\n\nBy filtering the data down to include just Baker Mayfield, we can use as_tibble() to make sure that his team is now listed as the Rams. Further, a continued look at the data shows no other players’ teams were changed in the process:\n\nas_tibble(sis_data)\n\n# A tibble: 33 x 6\n   player          team       att boom_percent bust_percent difference\n   <chr>           <chr>    <dbl>        <dbl>        <dbl>      <dbl>\n 1 Patrick Mahomes Chiefs     648         25.1         12.5      12.6 \n 2 Justin Herbert  Chargers   699         21.2         15.9       5.3 \n 3 Trevor Lawrence Jaguars    584         23.7         15.2       8.5 \n 4 Jared Goff      Lions      587         23.6         13.3      10.3 \n 5 Jalen Hurts     Eagles     460         23.5         16.1       7.4 \n 6 Kirk Cousins    Vikings    643         21.3         17.4       3.90\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7      12.8 \n 8 Joe Burrow      Bengals    606         23.6         14.8       8.8 \n 9 Josh Allen      Bills      567         23.2         12.7      10.5 \n10 Geno Smith      Seahawks   572         20.6         17.6       3   \n# ... with 23 more rows\n\n\nWith that, our sis_data dataset is nearly complete and ready for taking over to ggplot in Chapter 4. Before doing so, though, we can use the load_teams function within nflreadr to combine the appropriate team colors to each quarterback in the data, allowing us to customize the appearance of the data visualization.\n\n3.6.5 Merging Datasets with dplyr and Mutating Joins\nMutating joins are a manipulation technique that allows us to add, modify, or remove columns in a dataset based on a shared variable or a set of variables. The process is done by matching rows in the datasets based on an matching identifier between the two. Once completed, the matching data from the second dataset is added to the first or, conversely, the data within the first dataset is modified as needed to include the incoming data from the second dataset.\nThere are four common mutating joins in the dplyr package:\n\n\nleft_join(): a left_join adds all the columns from the first dataset by only matching columns from the second dataset.\n\nright_join(): a right_join is the opposite of the left_join process in that it adds all the columns from the second dataset and only matches those columns from the first.\n\ninner_join(): a inner_join will only match rows found in both datasets.\n\nfull_join(): a full_join will include all the rows and columns from both datasets.\n\nThe picture below, from William Surles’ instructional Joining Data in R with dplyr, showcases the above in graphical form.\n\n\nMutating Joins in dplyr\n\n\nIn this instance, using our sis_data dataset, we will need to conduct a left_join() to bring in NFL team color information. To start, we can use the load_teams function in nflreadr to load the team information and then use select to keep just the information we needed for the merging process:\n\n\n\n\n\n\nNote\n\n\n\nPlease note that a much deeper dive into the nflverse functions is included in Chapter 3, including a more detailed approach to working with individual team data. The below code will be expanded upon in the next chapter but, for now, allows us to walk through the merging process.\n\n\n\nteams <- nflreadr::load_teams(current = TRUE) %>%\n  select(team_abbr, team_nick, team_name, team_color, team_color2)\n\nas_tibble(teams)\n\n# A tibble: 32 x 5\n   team_abbr team_nick team_name          team_color team_color2\n   <chr>     <chr>     <chr>              <chr>      <chr>      \n 1 ARI       Cardinals Arizona Cardinals  #97233F    #000000    \n 2 ATL       Falcons   Atlanta Falcons    #A71930    #000000    \n 3 BAL       Ravens    Baltimore Ravens   #241773    #9E7C0C    \n 4 BUF       Bills     Buffalo Bills      #00338D    #C60C30    \n 5 CAR       Panthers  Carolina Panthers  #0085CA    #000000    \n 6 CHI       Bears     Chicago Bears      #0B162A    #C83803    \n 7 CIN       Bengals   Cincinnati Bengals #FB4F14    #000000    \n 8 CLE       Browns    Cleveland Browns   #FF3C00    #311D00    \n 9 DAL       Cowboys   Dallas Cowboys     #002244    #B0B7BC    \n10 DEN       Broncos   Denver Broncos     #002244    #FB4F14    \n# ... with 22 more rows\n\n\nFor educational purposes, we use the above code to bring in each NFL’s team abbreviation, nickname, and full name. Along with identifying information for each team, we also have selected team_color and team_color2 (which correspond to each team’s primary and secondary colors).\nAs mentioned, we will use the left_join() function to merge this information into our sis_data. The basic syntax of a left_join includes a call to the original dataset, the dataset to be merged in, and then an argument to stipulate which variable the join should match on:\n\ndataset1 <- dataset1 %>%\n  left_join(dataset2, by = c(\"variable_1\" = \"variable_2\"))\n\nIn terms of talking through it, the above code is stating: “use dataset1, and then conduct a left_join with dataset2, by matching variable_1 from dataset1 to corresponding variable_2 from dataset2.”\nTo conduct the join, we must determine which variable in teams matching a corresponding variable in sis_data. We can use the slice function to compare the first row of data from each:\n\nsis_data %>%\n  slice(1)\n\n# A tibble: 1 x 6\n  player          team     att boom_percent bust_percent difference\n  <chr>           <chr>  <dbl>        <dbl>        <dbl>      <dbl>\n1 Patrick Mahomes Chiefs   648         25.1         12.5       12.6\n\nteams %>%\n  slice(1)\n\n# A tibble: 1 x 5\n  team_abbr team_nick team_name         team_color team_color2\n  <chr>     <chr>     <chr>             <chr>      <chr>      \n1 ARI       Cardinals Arizona Cardinals #97233F    #000000    \n\n\nIn sis_data, the team column includes the nickname of the NFL team (in this case, the Chiefs). The corresponding variable in our teams dataset is the column titled team_nick. Knowing that, we can conduct the left_join using team for variable_1 from the above example syntax and team_nick for variable_2.\n\n\n\n\n\n\nTip\n\n\n\nNotice that the columns containing the corresponding data do not have the same name (team vs. team_nick).\nIt is important to remember that the name of the columns do not have to match. It is the values contained within the column that must match.\n\n\n\nsis_data <- sis_data %>%\n  left_join(teams, by = c(\"team\" = \"team_nick\"))\n\nas_tibble(sis_data)\n\n# A tibble: 33 x 10\n   player    team    att boom_~1 bust_~2 diffe~3 team_~4 team_~5 team_~6 team_~7\n   <chr>     <chr> <dbl>   <dbl>   <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>  \n 1 Patrick ~ Chie~   648    25.1    12.5   12.6  KC      Kansas~ #E31837 #FFB612\n 2 Justin H~ Char~   699    21.2    15.9    5.3  LAC     Los An~ #007BC7 #ffc20e\n 3 Trevor L~ Jagu~   584    23.7    15.2    8.5  JAX     Jackso~ #006778 #000000\n 4 Jared Go~ Lions   587    23.6    13.3   10.3  DET     Detroi~ #0076B6 #B0B7BC\n 5 Jalen Hu~ Eagl~   460    23.5    16.1    7.4  PHI     Philad~ #004C54 #A5ACAF\n 6 Kirk Cou~ Viki~   643    21.3    17.4    3.90 MIN     Minnes~ #4F2683 #FFC62F\n 7 Tua Tago~ Dolp~   400    28.5    15.7   12.8  MIA     Miami ~ #008E97 #F58220\n 8 Joe Burr~ Beng~   606    23.6    14.8    8.8  CIN     Cincin~ #FB4F14 #000000\n 9 Josh All~ Bills   567    23.2    12.7   10.5  BUF     Buffal~ #00338D #C60C30\n10 Geno Smi~ Seah~   572    20.6    17.6    3    SEA     Seattl~ #002244 #69be28\n# ... with 23 more rows, and abbreviated variable names 1: boom_percent,\n#   2: bust_percent, 3: difference, 4: team_abbr, 5: team_name, 6: team_color,\n#   7: team_color2\n\n\nYou can see in the as_tibble() output that our two datasets correctly merged on the team and team_nick columns. Moreover, to illustrate what happens doing the joining process, please note these two items:\n\nThe team_nick column from our teams dataset is not included in our merged sis_data, as it is dropped after matching/merging with the team information from our original dataset.\nWhile the team_nick column was dropped, the other team identifiers (team_abbr and team_name) were kept intact, as they were not used in the matching process. As well, team_color and team_color2 are now correctly match with each quarterback’s team.\n\nWe can use the seecolor package to illustrate how the hex codes in team_color and team_color2 correspond to each team:\n\nseecolor::print_color(sis_data$team_color)\n\n\n ------ $ sis_data team_color ------\n                                                                                                   \n\nseecolor::print_color(sis_data$team_color2)\n\n\n ------ $ sis_data team_color2 ------\n                                                                                                   \n\n\n\n\n\n\nWickham, Hadley. 2022. “Tidyverse Packages.” https://www.tidyverse.org/packages/."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#nflreadr-an-introduction-to-the-data",
    "href": "03-nfl-analytics-functions.html#nflreadr-an-introduction-to-the-data",
    "title": "\n4  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n4.1 nflreadr: An Introduction to the Data",
    "text": "4.1 nflreadr: An Introduction to the Data\nThe most important part of the nflverse is, of course, the data. To begin, we will examine the core data that underpins the nflverse: weekly player stats and the more detailed play-by–play data. Using nflreadr, the end user is able to collect weekly top-level stats via the load_player_stats() function or the much more robust play-by-play numbers by using the load_pbp() function.\nAs you may imagine, there is a very important distinction between the load_player_stats() and load_pbp(). As mentioned, load_player_stats() will provide you with weekly, pre-calculated statistics for either offense or kicking. Conversely, load_pbp() will provide over 350 metrics for every single play of every single game dating back to 1999.\nThe load_player_stats() function includes the following offensive information:\n\noffensive.stats <- nflreadr::load_player_stats(2021)\nls(offensive.stats)\n\n [1] \"air_yards_share\"             \"attempts\"                   \n [3] \"carries\"                     \"completions\"                \n [5] \"dakota\"                      \"fantasy_points\"             \n [7] \"fantasy_points_ppr\"          \"headshot_url\"               \n [9] \"interceptions\"               \"pacr\"                       \n[11] \"passing_2pt_conversions\"     \"passing_air_yards\"          \n[13] \"passing_epa\"                 \"passing_first_downs\"        \n[15] \"passing_tds\"                 \"passing_yards\"              \n[17] \"passing_yards_after_catch\"   \"player_display_name\"        \n[19] \"player_id\"                   \"player_name\"                \n[21] \"position\"                    \"position_group\"             \n[23] \"racr\"                        \"receiving_2pt_conversions\"  \n[25] \"receiving_air_yards\"         \"receiving_epa\"              \n[27] \"receiving_first_downs\"       \"receiving_fumbles\"          \n[29] \"receiving_fumbles_lost\"      \"receiving_tds\"              \n[31] \"receiving_yards\"             \"receiving_yards_after_catch\"\n[33] \"recent_team\"                 \"receptions\"                 \n[35] \"rushing_2pt_conversions\"     \"rushing_epa\"                \n[37] \"rushing_first_downs\"         \"rushing_fumbles\"            \n[39] \"rushing_fumbles_lost\"        \"rushing_tds\"                \n[41] \"rushing_yards\"               \"sack_fumbles\"               \n[43] \"sack_fumbles_lost\"           \"sack_yards\"                 \n[45] \"sacks\"                       \"season\"                     \n[47] \"season_type\"                 \"special_teams_tds\"          \n[49] \"target_share\"                \"targets\"                    \n[51] \"week\"                        \"wopr\"                       \n\n\nAs well, switching the stat_type to “kicking” provides the following information:\n\nkicking.stats <- nflreadr::load_player_stats(2021, stat_type = \"kicking\")\nls(kicking.stats)\n\n [1] \"fg_att\"              \"fg_blocked\"          \"fg_blocked_distance\"\n [4] \"fg_blocked_list\"     \"fg_long\"             \"fg_made\"            \n [7] \"fg_made_0_19\"        \"fg_made_20_29\"       \"fg_made_30_39\"      \n[10] \"fg_made_40_49\"       \"fg_made_50_59\"       \"fg_made_60_\"        \n[13] \"fg_made_distance\"    \"fg_made_list\"        \"fg_missed\"          \n[16] \"fg_missed_0_19\"      \"fg_missed_20_29\"     \"fg_missed_30_39\"    \n[19] \"fg_missed_40_49\"     \"fg_missed_50_59\"     \"fg_missed_60_\"      \n[22] \"fg_missed_distance\"  \"fg_missed_list\"      \"fg_pct\"             \n[25] \"gwfg_att\"            \"gwfg_blocked\"        \"gwfg_distance\"      \n[28] \"gwfg_made\"           \"gwfg_missed\"         \"pat_att\"            \n[31] \"pat_blocked\"         \"pat_made\"            \"pat_missed\"         \n[34] \"pat_pct\"             \"player_id\"           \"player_name\"        \n[37] \"season\"              \"season_type\"         \"team\"               \n[40] \"week\"               \n\n\nWhile the data returned is not as rich as the play-by-play data we will covering next, the load_player_stats() function is extremely helpful when you need to quickly (and correctly!) recreate the official stats listed on either the NFL’s website or on Pro Football Reference.\nAs an example, let’s say you need to get Ben Roethlisberger’s total passing yard and attempts from the 2021 season. You could do so via load_pbp() but, if you do not need further context, using load_player_stats() is much more efficient.\n\n4.1.1 Getting Weekly Player Stats via load_player_stats()\n\nIf you are familiar with R, it might seem logical to do the following to get Roethlisberger’s total passing yards and number of attempts from the 2021 regular season:\n\nweekly.data <- nflreadr::load_player_stats(2021)\n\nben.weekly <- weekly.data %>%\n  group_by(player_id, player_name) %>%\n  filter(season_type == \"REG\" & player_name == \"B.Roethlisberger\") %>%\n  summarize(total.yards = sum(passing_yards),\n            n.attempts = sum(attempts))\n\ntibble(ben.weekly)\n\n# A tibble: 1 x 4\n  player_id  player_name      total.yards n.attempts\n  <chr>      <chr>                  <dbl>      <int>\n1 00-0022924 B.Roethlisberger        3740        605\n\n\nAs you can see in the ben.weekly output, we have matched his official 2021 regular stats perfectly with 3,740 passing yards on 605 attempts. The code we just created is doing several things. First, we are using nflreadr::load_player_stats(2021) to place the data into our R environment in a DF titled weekly.data.\nNext, we group the data together by alike player_id (as every individual player has a unique ID number) as well as the player’s actual name. At the filtering level, we are looking for just the regular season (REG) within season_type and also removing all quarterbacks except for Ben Roethlisberger. It is important to note that player names are just first initial and last name, without a space after the period.\nAfter filtering for the regular season, we are able to summarize all of the weekly data into combined statistics by summing the weekly totals of passing yards and attempts.\nHowever, filtering by player_name can lead to significant issues with your results. An excellent example of this is Josh Allen. Let’s recreate the code above that successfully provided Roethlisberger’s stats, but replace Ben with Josh Allen:\n\njosh.allen <- weekly.data %>%\n  group_by(player_name) %>%\n  filter(player_name == \"J.Allen\" & season_type == \"REG\") %>%\n  summarize(total.yards = sum(passing_yards),\n            n.attempts = sum(attempts))\n\ntibble(josh.allen)\n\n# A tibble: 1 x 3\n  player_name total.yards n.attempts\n  <chr>             <dbl>      <int>\n1 J.Allen            4407        646\n\n\nThe output tells us Allen threw for 4,049 yards on 603 attempts during the 2021 regular season. A check of his Pro Football Reference page tells us those numbers are incorrect. In fact, he had 4,407 passing yards on 646 attempts. How did we end up 358 passing yards and 43 attempts short?\nThe answer comes from Aaron Schatz, the creator of Football Outsiders, who explained in a Tweet that the official Buffalo Bills’ scorer, during week 3 of the NFL season, decided to refer to Allen as “Jos.Allen” as a result of the Washington Commanders having a player named “Jonathan Allen.”\nTo double check this, we can run the same code as above, but remove the player_name filter and switch to searching for just those players on the Buffalo Bills by using recent_team.\n\ntwo.josh.allens <- weekly.data %>%\n  group_by(player_id, player_name) %>%\n  filter(season_type == \"REG\" & recent_team == \"BUF\") %>%\n  summarize(total.yards = sum(passing_yards),\n            n.attempts = sum(attempts))\n\ntibble(two.josh.allens)\n\n# A tibble: 16 x 4\n   player_id  player_name  total.yards n.attempts\n   <chr>      <chr>              <dbl>      <int>\n 1 00-0027685 E.Sanders              0          0\n 2 00-0029000 C.Beasley              0          1\n 3 00-0031588 S.Diggs                0          0\n 4 00-0031787 J.Kumerow              0          0\n 5 00-0033308 M.Breida               0          0\n 6 00-0033466 I.McKenzie             0          0\n 7 00-0033550 D.Webb                 0          0\n 8 00-0033869 M.Trubisky            43          8\n 9 00-0033904 D.Dawkins              0          0\n10 00-0034857 J.Allen             4407        646\n11 00-0035250 D.Singletary           0          0\n12 00-0035308 T.Sweeney              0          0\n13 00-0035689 D.Knox                 0          0\n14 00-0036187 R.Gilliam              0          0\n15 00-0036196 G.Davis                0          0\n16 00-0036251 Z.Moss                 0          0\n\n\nGrouping by player_id and player_name (as well as filtering down to Buffalo), we can see that, indeed, Josh Allen is in the data twice under the same player_id. Moreover, if you do the math, you can see that the numbers from his two entries add up to the official statistics on his Pro Football Reference page.\n\n4.1.1.1 Using load_player_stats() Correctly\nTo avoid these situations, you could load up NFL rosters via the nflreadr::load_rosters() function, but that would require unnecessary code in order to merge the two DFs together by matching the player_id to the gsis_id number found within the roster information. Doing so would correct the above issue of Josh Allen appearing in the data under different spellings. Instead, and to write the minimal amount of code to complete the task, we can do the following:\n\njosh.allen <- weekly.data %>%\n  filter(season_type == \"REG\") %>%\n  group_by(player_id) %>%\n  summarize(player_name = first(player_name),\n            total.yards = sum(passing_yards),\n            n.attempts = sum(attempts)) %>%\n  filter(player_name == \"J.Allen\")\n\nThe most efficient way to gather correct player statistics is to do the group_by with ONLY the player_id as, despite the variation in name, the player_id remained the same for Josh Allen. In order to include his correct name in the output, we can gather QB names within the summarize prior to calculating the sum of passing_yards and attempts. After, if you desire to see only Josh Allen’s number, you can filter out to just his name.\n\n4.1.2 Using load_player_stats() To Find Leaders\nWhile using load_player_stats() does not provide the ability to add context to your analysis as we will soon see with load_pbp(), it does provide an easy and efficient way to determine weekly or season-long leaders over many top-level, widely-used NFL statistics. In the below example, we will determine the 2021 leaders in air yards per attempt.\n\n4.1.2.1 An Example: 2021 QB Air Yards per Attempt Leaders\n\ndata <- nflreadr::load_player_stats(2021)\n\nay.per.attempt <- data %>%\n  group_by(player_id) %>%\n  filter(season_type == \"REG\") %>%\n  summarize(player_name = first(player_name),\n            n.attempts = sum(attempts),\n            n.airyards = sum(passing_air_yards),\n            ay.attempt = n.airyards / n.attempts) %>%\n  filter(n.attempts >= 400) %>%\n  select(player_name, ay.attempt) %>%\n  arrange(-ay.attempt)\n\nIn the above example, we are using group_by to combine the desired statistics based on each unique player_id to, again, avoid any issues with player names within the data. After filtering to include just those statistics for the regular season, we first use the summarize function to grab the first player_name associated with the player_id. After, we find two items: (1.) the total number of passing attempts by each QB which is outputted into a new row titled n.attempts and the regular season total of each QB’s air yards, again outputted into a new row titled n.airyards.\nIt is important to note that the final row created with the summarize function is not a statistic included within load_player_stats(). In order to find a QB’s average air yards per attempt, we must use the first two items we’ve created and do some simple division (the created n.airyards divided by n.attempts).\nFinally, to “clear the noise” of those QBs with minimal attempts through the season, we included a filter to include only those passers with at least 400 attempts. After, we arrange the new DF by sorting the QBs in descending order by average air yards per attempt.\nThe end results look like this:\n\n\n# A tibble: 25 x 2\n   player_name   ay.attempt\n   <chr>              <dbl>\n 1 R.Wilson            9.89\n 2 J.Hurts             8.99\n 3 B.Mayfield          8.73\n 4 M.Stafford          8.48\n 5 J.Allen             8.20\n 6 K.Cousins           8.16\n 7 J.Burrow            8.12\n 8 D.Carr              8.12\n 9 T.Brady             8.10\n10 T.Bridgewater       8.04\n# ... with 15 more rows\n\n\nRussell Wilson led the NFL in 2021 with 9.89 air yards per attempt."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#using-load_pbp-to-add-context-to-statistics",
    "href": "03-nfl-analytics-functions.html#using-load_pbp-to-add-context-to-statistics",
    "title": "\n4  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n4.2 Using load_pbp() to Add Context to Statistics",
    "text": "4.2 Using load_pbp() to Add Context to Statistics\nAs just mentioned above, using the load_pbp() function is preferable when you are looking to add context to a player’s statistics, as the load_player_stats() function is, for all intents and purposes, aggregated statistics that limit your ability to find deeper meaning.\nThe load_pbp() function provides over 350 various metrics, as listed below:\n\npbp.data <- nflreadr::load_pbp(2021)\nls(pbp.data)\n\n  [1] \"aborted_play\"                        \n  [2] \"air_epa\"                             \n  [3] \"air_wpa\"                             \n  [4] \"air_yards\"                           \n  [5] \"assist_tackle\"                       \n  [6] \"assist_tackle_1_player_id\"           \n  [7] \"assist_tackle_1_player_name\"         \n  [8] \"assist_tackle_1_team\"                \n  [9] \"assist_tackle_2_player_id\"           \n [10] \"assist_tackle_2_player_name\"         \n [11] \"assist_tackle_2_team\"                \n [12] \"assist_tackle_3_player_id\"           \n [13] \"assist_tackle_3_player_name\"         \n [14] \"assist_tackle_3_team\"                \n [15] \"assist_tackle_4_player_id\"           \n [16] \"assist_tackle_4_player_name\"         \n [17] \"assist_tackle_4_team\"                \n [18] \"away_coach\"                          \n [19] \"away_score\"                          \n [20] \"away_team\"                           \n [21] \"away_timeouts_remaining\"             \n [22] \"away_wp\"                             \n [23] \"away_wp_post\"                        \n [24] \"blocked_player_id\"                   \n [25] \"blocked_player_name\"                 \n [26] \"comp_air_epa\"                        \n [27] \"comp_air_wpa\"                        \n [28] \"comp_yac_epa\"                        \n [29] \"comp_yac_wpa\"                        \n [30] \"complete_pass\"                       \n [31] \"cp\"                                  \n [32] \"cpoe\"                                \n [33] \"def_wp\"                              \n [34] \"defensive_extra_point_attempt\"       \n [35] \"defensive_extra_point_conv\"          \n [36] \"defensive_two_point_attempt\"         \n [37] \"defensive_two_point_conv\"            \n [38] \"defteam\"                             \n [39] \"defteam_score\"                       \n [40] \"defteam_score_post\"                  \n [41] \"defteam_timeouts_remaining\"          \n [42] \"desc\"                                \n [43] \"div_game\"                            \n [44] \"down\"                                \n [45] \"drive\"                               \n [46] \"drive_end_transition\"                \n [47] \"drive_end_yard_line\"                 \n [48] \"drive_ended_with_score\"              \n [49] \"drive_first_downs\"                   \n [50] \"drive_game_clock_end\"                \n [51] \"drive_game_clock_start\"              \n [52] \"drive_inside20\"                      \n [53] \"drive_play_count\"                    \n [54] \"drive_play_id_ended\"                 \n [55] \"drive_play_id_started\"               \n [56] \"drive_quarter_end\"                   \n [57] \"drive_quarter_start\"                 \n [58] \"drive_real_start_time\"               \n [59] \"drive_start_transition\"              \n [60] \"drive_start_yard_line\"               \n [61] \"drive_time_of_possession\"            \n [62] \"drive_yards_penalized\"               \n [63] \"end_clock_time\"                      \n [64] \"end_yard_line\"                       \n [65] \"ep\"                                  \n [66] \"epa\"                                 \n [67] \"extra_point_attempt\"                 \n [68] \"extra_point_prob\"                    \n [69] \"extra_point_result\"                  \n [70] \"fantasy\"                             \n [71] \"fantasy_id\"                          \n [72] \"fantasy_player_id\"                   \n [73] \"fantasy_player_name\"                 \n [74] \"fg_prob\"                             \n [75] \"field_goal_attempt\"                  \n [76] \"field_goal_result\"                   \n [77] \"first_down\"                          \n [78] \"first_down_pass\"                     \n [79] \"first_down_penalty\"                  \n [80] \"first_down_rush\"                     \n [81] \"fixed_drive\"                         \n [82] \"fixed_drive_result\"                  \n [83] \"forced_fumble_player_1_player_id\"    \n [84] \"forced_fumble_player_1_player_name\"  \n [85] \"forced_fumble_player_1_team\"         \n [86] \"forced_fumble_player_2_player_id\"    \n [87] \"forced_fumble_player_2_player_name\"  \n [88] \"forced_fumble_player_2_team\"         \n [89] \"fourth_down_converted\"               \n [90] \"fourth_down_failed\"                  \n [91] \"fumble\"                              \n [92] \"fumble_forced\"                       \n [93] \"fumble_lost\"                         \n [94] \"fumble_not_forced\"                   \n [95] \"fumble_out_of_bounds\"                \n [96] \"fumble_recovery_1_player_id\"         \n [97] \"fumble_recovery_1_player_name\"       \n [98] \"fumble_recovery_1_team\"              \n [99] \"fumble_recovery_1_yards\"             \n[100] \"fumble_recovery_2_player_id\"         \n[101] \"fumble_recovery_2_player_name\"       \n[102] \"fumble_recovery_2_team\"              \n[103] \"fumble_recovery_2_yards\"             \n[104] \"fumbled_1_player_id\"                 \n[105] \"fumbled_1_player_name\"               \n[106] \"fumbled_1_team\"                      \n[107] \"fumbled_2_player_id\"                 \n[108] \"fumbled_2_player_name\"               \n[109] \"fumbled_2_team\"                      \n[110] \"game_date\"                           \n[111] \"game_half\"                           \n[112] \"game_id\"                             \n[113] \"game_seconds_remaining\"              \n[114] \"game_stadium\"                        \n[115] \"goal_to_go\"                          \n[116] \"half_sack_1_player_id\"               \n[117] \"half_sack_1_player_name\"             \n[118] \"half_sack_2_player_id\"               \n[119] \"half_sack_2_player_name\"             \n[120] \"half_seconds_remaining\"              \n[121] \"home_coach\"                          \n[122] \"home_opening_kickoff\"                \n[123] \"home_score\"                          \n[124] \"home_team\"                           \n[125] \"home_timeouts_remaining\"             \n[126] \"home_wp\"                             \n[127] \"home_wp_post\"                        \n[128] \"id\"                                  \n[129] \"incomplete_pass\"                     \n[130] \"interception\"                        \n[131] \"interception_player_id\"              \n[132] \"interception_player_name\"            \n[133] \"jersey_number\"                       \n[134] \"kick_distance\"                       \n[135] \"kicker_player_id\"                    \n[136] \"kicker_player_name\"                  \n[137] \"kickoff_attempt\"                     \n[138] \"kickoff_downed\"                      \n[139] \"kickoff_fair_catch\"                  \n[140] \"kickoff_in_endzone\"                  \n[141] \"kickoff_inside_twenty\"               \n[142] \"kickoff_out_of_bounds\"               \n[143] \"kickoff_returner_player_id\"          \n[144] \"kickoff_returner_player_name\"        \n[145] \"lateral_interception_player_id\"      \n[146] \"lateral_interception_player_name\"    \n[147] \"lateral_kickoff_returner_player_id\"  \n[148] \"lateral_kickoff_returner_player_name\"\n[149] \"lateral_punt_returner_player_id\"     \n[150] \"lateral_punt_returner_player_name\"   \n[151] \"lateral_receiver_player_id\"          \n[152] \"lateral_receiver_player_name\"        \n[153] \"lateral_receiving_yards\"             \n[154] \"lateral_reception\"                   \n[155] \"lateral_recovery\"                    \n[156] \"lateral_return\"                      \n[157] \"lateral_rush\"                        \n[158] \"lateral_rusher_player_id\"            \n[159] \"lateral_rusher_player_name\"          \n[160] \"lateral_rushing_yards\"               \n[161] \"lateral_sack_player_id\"              \n[162] \"lateral_sack_player_name\"            \n[163] \"location\"                            \n[164] \"name\"                                \n[165] \"nfl_api_id\"                          \n[166] \"no_huddle\"                           \n[167] \"no_score_prob\"                       \n[168] \"old_game_id\"                         \n[169] \"opp_fg_prob\"                         \n[170] \"opp_safety_prob\"                     \n[171] \"opp_td_prob\"                         \n[172] \"order_sequence\"                      \n[173] \"out_of_bounds\"                       \n[174] \"own_kickoff_recovery\"                \n[175] \"own_kickoff_recovery_player_id\"      \n[176] \"own_kickoff_recovery_player_name\"    \n[177] \"own_kickoff_recovery_td\"             \n[178] \"pass\"                                \n[179] \"pass_attempt\"                        \n[180] \"pass_defense_1_player_id\"            \n[181] \"pass_defense_1_player_name\"          \n[182] \"pass_defense_2_player_id\"            \n[183] \"pass_defense_2_player_name\"          \n[184] \"pass_length\"                         \n[185] \"pass_location\"                       \n[186] \"pass_oe\"                             \n[187] \"pass_touchdown\"                      \n[188] \"passer\"                              \n[189] \"passer_id\"                           \n[190] \"passer_jersey_number\"                \n[191] \"passer_player_id\"                    \n[192] \"passer_player_name\"                  \n[193] \"passing_yards\"                       \n[194] \"penalty\"                             \n[195] \"penalty_player_id\"                   \n[196] \"penalty_player_name\"                 \n[197] \"penalty_team\"                        \n[198] \"penalty_type\"                        \n[199] \"penalty_yards\"                       \n[200] \"play\"                                \n[201] \"play_clock\"                          \n[202] \"play_deleted\"                        \n[203] \"play_id\"                             \n[204] \"play_type\"                           \n[205] \"play_type_nfl\"                       \n[206] \"posteam\"                             \n[207] \"posteam_score\"                       \n[208] \"posteam_score_post\"                  \n[209] \"posteam_timeouts_remaining\"          \n[210] \"posteam_type\"                        \n[211] \"punt_attempt\"                        \n[212] \"punt_blocked\"                        \n[213] \"punt_downed\"                         \n[214] \"punt_fair_catch\"                     \n[215] \"punt_in_endzone\"                     \n[216] \"punt_inside_twenty\"                  \n[217] \"punt_out_of_bounds\"                  \n[218] \"punt_returner_player_id\"             \n[219] \"punt_returner_player_name\"           \n[220] \"punter_player_id\"                    \n[221] \"punter_player_name\"                  \n[222] \"qb_dropback\"                         \n[223] \"qb_epa\"                              \n[224] \"qb_hit\"                              \n[225] \"qb_hit_1_player_id\"                  \n[226] \"qb_hit_1_player_name\"                \n[227] \"qb_hit_2_player_id\"                  \n[228] \"qb_hit_2_player_name\"                \n[229] \"qb_kneel\"                            \n[230] \"qb_scramble\"                         \n[231] \"qb_spike\"                            \n[232] \"qtr\"                                 \n[233] \"quarter_end\"                         \n[234] \"quarter_seconds_remaining\"           \n[235] \"receiver\"                            \n[236] \"receiver_id\"                         \n[237] \"receiver_jersey_number\"              \n[238] \"receiver_player_id\"                  \n[239] \"receiver_player_name\"                \n[240] \"receiving_yards\"                     \n[241] \"replay_or_challenge\"                 \n[242] \"replay_or_challenge_result\"          \n[243] \"result\"                              \n[244] \"return_team\"                         \n[245] \"return_touchdown\"                    \n[246] \"return_yards\"                        \n[247] \"roof\"                                \n[248] \"run_gap\"                             \n[249] \"run_location\"                        \n[250] \"rush\"                                \n[251] \"rush_attempt\"                        \n[252] \"rush_touchdown\"                      \n[253] \"rusher\"                              \n[254] \"rusher_id\"                           \n[255] \"rusher_jersey_number\"                \n[256] \"rusher_player_id\"                    \n[257] \"rusher_player_name\"                  \n[258] \"rushing_yards\"                       \n[259] \"sack\"                                \n[260] \"sack_player_id\"                      \n[261] \"sack_player_name\"                    \n[262] \"safety\"                              \n[263] \"safety_player_id\"                    \n[264] \"safety_player_name\"                  \n[265] \"safety_prob\"                         \n[266] \"score_differential\"                  \n[267] \"score_differential_post\"             \n[268] \"season\"                              \n[269] \"season_type\"                         \n[270] \"series\"                              \n[271] \"series_result\"                       \n[272] \"series_success\"                      \n[273] \"shotgun\"                             \n[274] \"side_of_field\"                       \n[275] \"solo_tackle\"                         \n[276] \"solo_tackle_1_player_id\"             \n[277] \"solo_tackle_1_player_name\"           \n[278] \"solo_tackle_1_team\"                  \n[279] \"solo_tackle_2_player_id\"             \n[280] \"solo_tackle_2_player_name\"           \n[281] \"solo_tackle_2_team\"                  \n[282] \"sp\"                                  \n[283] \"special\"                             \n[284] \"special_teams_play\"                  \n[285] \"spread_line\"                         \n[286] \"st_play_type\"                        \n[287] \"stadium\"                             \n[288] \"stadium_id\"                          \n[289] \"start_time\"                          \n[290] \"success\"                             \n[291] \"surface\"                             \n[292] \"tackle_for_loss_1_player_id\"         \n[293] \"tackle_for_loss_1_player_name\"       \n[294] \"tackle_for_loss_2_player_id\"         \n[295] \"tackle_for_loss_2_player_name\"       \n[296] \"tackle_with_assist\"                  \n[297] \"tackle_with_assist_1_player_id\"      \n[298] \"tackle_with_assist_1_player_name\"    \n[299] \"tackle_with_assist_1_team\"           \n[300] \"tackle_with_assist_2_player_id\"      \n[301] \"tackle_with_assist_2_player_name\"    \n[302] \"tackle_with_assist_2_team\"           \n[303] \"tackled_for_loss\"                    \n[304] \"td_player_id\"                        \n[305] \"td_player_name\"                      \n[306] \"td_prob\"                             \n[307] \"td_team\"                             \n[308] \"temp\"                                \n[309] \"third_down_converted\"                \n[310] \"third_down_failed\"                   \n[311] \"time\"                                \n[312] \"time_of_day\"                         \n[313] \"timeout\"                             \n[314] \"timeout_team\"                        \n[315] \"total\"                               \n[316] \"total_away_comp_air_epa\"             \n[317] \"total_away_comp_air_wpa\"             \n[318] \"total_away_comp_yac_epa\"             \n[319] \"total_away_comp_yac_wpa\"             \n[320] \"total_away_epa\"                      \n[321] \"total_away_pass_epa\"                 \n[322] \"total_away_pass_wpa\"                 \n[323] \"total_away_raw_air_epa\"              \n[324] \"total_away_raw_air_wpa\"              \n[325] \"total_away_raw_yac_epa\"              \n[326] \"total_away_raw_yac_wpa\"              \n[327] \"total_away_rush_epa\"                 \n[328] \"total_away_rush_wpa\"                 \n[329] \"total_away_score\"                    \n[330] \"total_home_comp_air_epa\"             \n[331] \"total_home_comp_air_wpa\"             \n[332] \"total_home_comp_yac_epa\"             \n[333] \"total_home_comp_yac_wpa\"             \n[334] \"total_home_epa\"                      \n[335] \"total_home_pass_epa\"                 \n[336] \"total_home_pass_wpa\"                 \n[337] \"total_home_raw_air_epa\"              \n[338] \"total_home_raw_air_wpa\"              \n[339] \"total_home_raw_yac_epa\"              \n[340] \"total_home_raw_yac_wpa\"              \n[341] \"total_home_rush_epa\"                 \n[342] \"total_home_rush_wpa\"                 \n[343] \"total_home_score\"                    \n[344] \"total_line\"                          \n[345] \"touchback\"                           \n[346] \"touchdown\"                           \n[347] \"two_point_attempt\"                   \n[348] \"two_point_conv_result\"               \n[349] \"two_point_conversion_prob\"           \n[350] \"vegas_home_wp\"                       \n[351] \"vegas_home_wpa\"                      \n[352] \"vegas_wp\"                            \n[353] \"vegas_wpa\"                           \n[354] \"weather\"                             \n[355] \"week\"                                \n[356] \"wind\"                                \n[357] \"wp\"                                  \n[358] \"wpa\"                                 \n[359] \"xpass\"                               \n[360] \"xyac_epa\"                            \n[361] \"xyac_fd\"                             \n[362] \"xyac_mean_yardage\"                   \n[363] \"xyac_median_yardage\"                 \n[364] \"xyac_success\"                        \n[365] \"yac_epa\"                             \n[366] \"yac_wpa\"                             \n[367] \"yardline_100\"                        \n[368] \"yards_after_catch\"                   \n[369] \"yards_gained\"                        \n[370] \"ydsnet\"                              \n[371] \"ydstogo\"                             \n[372] \"yrdln\"                               \n\n\nA bit overwhelming, right?\nLuckily, the nflfastR website includes a searchable directory of all the variables with a brief description of what each one means. You can visit that here: nflfastR Field Descriptions.\nAs seen above, we can use the load_player_stats() function to determine a QB’s average yards per attempt over the course of a season. But, what if we wanted to add context to that? For example, how do we explore a QB’s air yards in game-specific situations?\nTo showcase using load_pbp() to add context to your analysis, let’s explore QB performance via air yards on 3rd down.\n\n4.2.1 An Example: QB Aggressiveness on 3rd Down\nSticking with the air yards example from above, let’s examine a metric I created using load_pbp() that I coined QB 3rd Down Aggressiveness. The metric is designed to determine which QBs in the NFL are most aggressive in 3rd down situations by gauging how often they throw the ball to, or pass, the first down line. It is an interesting metric to explore as, just like many metrics in the NFL, not all air yards are created equal. For example, eight air yards on 1st and 10 are less valuable than the same eight air yards on 3rd and 5.\nFirst, let’s highlight the code used to create the results for this metric and then break it down line-by-line.\n\ndata <- nflreadr::load_pbp(2021)\n\naggressiveness <- data %>%\n  group_by(passer_id) %>%\n  filter(down == 3, play_type == \"pass\", ydstogo >= 5, ydstogo <= 10) %>%\n  summarize(player_name = first(passer),\n            team = first(posteam),\n            total = n(),\n            aggressive = sum(air_yards >= ydstogo, na.rm = TRUE),\n            percentage = aggressive / total) %>%\n  filter(total >= 50) %>%\n  arrange(-percentage)\n\ntibble(aggressiveness)\n\n# A tibble: 30 x 6\n   passer_id  player_name  team  total aggressive percentage\n   <chr>      <chr>        <chr> <int>      <int>      <dbl>\n 1 00-0033077 D.Prescott   DAL      84         53      0.631\n 2 00-0035228 K.Murray     ARI      60         37      0.617\n 3 00-0036389 J.Hurts      PHI      65         40      0.615\n 4 00-0033873 P.Mahomes    KC       93         56      0.602\n 5 00-0034855 B.Mayfield   CLE      59         35      0.593\n 6 00-0036971 T.Lawrence   JAX      78         46      0.590\n 7 00-0036355 J.Herbert    LAC      87         51      0.586\n 8 00-0035710 D.Jones      NYG      60         35      0.583\n 9 00-0036212 T.Tagovailoa MIA      60         35      0.583\n10 00-0026498 M.Stafford   LA       95         54      0.568\n# ... with 20 more rows\n\n\nAs you can see in the tibble() output of the results, Dak Prescott was the most aggressive quarterback in 3rd down passing situations in the 2021 season, passing to, our beyond, the line of gain just over 63% of the time.\nAfter creating a new dataframe called aggressiveness from the 2021 play-by-play we originally collected using data <- nflreadr::load_pbp(2021), we use group_by to ensure that the data is being collected per individual quarterback via passer_id.\nAfter using the group_by function to lump data with each individual QB, we then use filter() function. Of course, we only want those play_types that are “pass” on 3rd downs. However, in the above code, we are filtering for just those 3rd down situations where the yards to go are between five and ten yards.\nDoing so was a personal decision on my end when creating the metric, as there are certainly arguments to be made regarding how to “capture” scenarios in the data that require “aggressiveness.” My logic? If there were less than five yards to go on 3rd down, the opposing defense would not be able to “sell out” to the pass as it would not be out of the question for an offense to attempt to gain the first down on the ground. Conversely, anything over ten yards likely results in the defense selling out to the pass, thus leaving an imprint on the aggressiveness output of the quarterbacks.\nFor the sake of curiosity, we can edit the above code to include all passing attempts on 3rd down with under 10 yards to go for the first down:\n\naggressiveness.under.10 <- data %>%\n  group_by(passer_id) %>%\n  filter(down == 3, play_type == \"pass\", ydstogo <= 10) %>%\n  summarize(player_name = first(passer),\n            team = first(posteam),\n            total = n(),\n            aggressive = sum(air_yards >= ydstogo, na.rm = TRUE),\n            percentage = aggressive / total) %>%\n  filter(total >= 50) %>%\n  arrange(desc(percentage))\n\ntibble(aggressiveness.under.10)\n\n# A tibble: 33 x 6\n   passer_id  player_name  team  total aggressive percentage\n   <chr>      <chr>        <chr> <int>      <int>      <dbl>\n 1 00-0035228 K.Murray     ARI      98         67      0.684\n 2 00-0036389 J.Hurts      PHI     107         73      0.682\n 3 00-0036971 T.Lawrence   JAX     131         88      0.672\n 4 00-0033077 D.Prescott   DAL     136         89      0.654\n 5 00-0034857 J.Allen      BUF     138         88      0.638\n 6 00-0036355 J.Herbert    LAC     148         94      0.635\n 7 00-0036212 T.Tagovailoa MIA      93         59      0.634\n 8 00-0026498 M.Stafford   LA      172        109      0.634\n 9 00-0023459 A.Rodgers    GB      128         80      0.625\n10 00-0035710 D.Jones      NYG      85         53      0.624\n# ... with 23 more rows\n\n\nThe results are quite different from the first running of this metric, as Dak Prescott is now the 4th most aggressive QB, while Kyler Murray moves to the top by approaching a nearly 70% aggressiveness rate on 3rd down. This small change highlights an important element about analytics: much of the work is the result of the coder (ie., you) being able to justify your decision-making process when developing the filters for each metric you create.\nIn this case, I stand by my argument that including just those pass attempts on 3rd down with between 5 and 10 yards to go is a more accurate assessment of aggressiveness as, for example, 3rd down with 8 yards to go is an obvious passing situation in most cases.\nThat begs the question, though: in which cases is 3rd down with 8 yards to go not an obvious passing situation? An example of this falls under the guise of “garbage time.”\n\n4.2.1.1 QB Aggressiveness: Filtering for “Garbage Time?”\nIn our initial running of the QB Aggressiveness metric, Josh Allen is the 15th most aggressive QB in the NFL on 3rd down with between 5 and 10 yards to go. But how much does the success of the Buffalo Bills play into that 15th place ranking?\nThe Bills, at the conclusion of the 2021 season, had the largest positive point differential in the league at 194 (the Bills scored 483 points, while allowing just 289). Perhaps Allen’s numbers are skewed because the Bills were so often playing with the lead late into the game?\nTo account for this, we can add information into the filter() function to attempt to remove what are referenced to in the analytics community as “garbage time stats.”\nLet’s add the “garbage time” filter to the code we’ve already prepared:\n\naggressiveness.garbage <- data %>%\n  group_by(passer_id) %>%\n  filter(down == 3, play_type == \"pass\", ydstogo >= 5, ydstogo <= 10,\n         wp > .05, wp < .95, half_seconds_remaining > 120) %>%\n  summarize(player_name = first(passer),\n            team = first(posteam),\n            total = n(),\n            aggressive = sum(air_yards >= ydstogo, na.rm = TRUE),\n            percentage = aggressive / total) %>%\n  filter(total >= 50) %>%\n  arrange(desc(percentage))\n\ntibble(aggressiveness.garbage)\n\n# A tibble: 26 x 6\n   passer_id  player_name team  total aggressive percentage\n   <chr>      <chr>       <chr> <int>      <int>      <dbl>\n 1 00-0033077 D.Prescott  DAL      61         40      0.656\n 2 00-0036971 T.Lawrence  JAX      51         33      0.647\n 3 00-0035228 K.Murray    ARI      51         31      0.608\n 4 00-0026498 M.Stafford  LA       79         48      0.608\n 5 00-0033873 P.Mahomes   KC       68         41      0.603\n 6 00-0035710 D.Jones     NYG      50         30      0.6  \n 7 00-0036355 J.Herbert   LAC      67         38      0.567\n 8 00-0036972 M.Jones     NE       57         31      0.544\n 9 00-0034857 J.Allen     BUF      59         32      0.542\n10 00-0029263 R.Wilson    SEA      56         30      0.536\n# ... with 16 more rows\n\n\nWe are now using the same code, but have included three new items to the filter(). First, we are stipulating that, aside from the down and distance inclusion, we only want those plays that occurred when the offense’s win probability was between 5% and 95%, as well as ensuring that the plays did not happen after the two-minute warning of either half.\nThe decision on range of the win probability numbers is, again, a personal preference. When nflfastR was first released, analyst often used a 20-80% range for win probability. However, Sebastian Carl - one of the creators of the nflverse explained in the package’s Discord:\n\nSebastian Carl: “I am generally very conservative with filtering plays using wp. Especially the vegas wp model can reach >85% probs early in the game because it incorporates market lines. I never understood the 20% <= wp <= 80%”garbage time” filter. This is removing a ton of plays. My general advice is a lower boundary of something around 5% (i.e., 5% <= wp <= 95%).\n\nBen Baldwin followed up on Carl’s thoughts:\n\nBen Baldwin: “agree with this. 20-80% should only be used as a filter for looking at how run-heavy a team is (because outside of this range is when teams change behavior a lot). and possibly how teams behave on 4th downs. but not for team or player performance.”\n\nBased on that advice, I typically stick to the 5-95% range when filtering for win probability using play-by-play data. And, in this case, it did have an impact.\nAs mentioned, prior to filtering for garbage time, Allen was the 15th most aggressive QB in the league at nearly 52%. However, once filtering for garbage time, Allen rose to 9th most aggressive QB, with a slight increase of percentage to 54%.\nWhat is interesting about the above example, though, is Dak Prescott and the Cowboys. Dallas maintained the second largest point differential in the league (530 points for and 358 points against, for a 172 point difference). Without the garbage time filter, Prescott was tops in the NFL with an aggressiveness rating of 63%.\nOnce adjusted for garbage time? Prescott remained atop the NFL with an aggressiveness rating of 65.5%.\nAllen’s increase in the standings, and Prescott remaining best in the league, in this specific metric, is a possible indicator that the inclusion of the “garbage time” filters provides a slightly more accurate result.\n\n4.2.2 The Inclusion of Contextual Statistics\nAs seen in the above example regarding QB aggressiveness on 3rd down, the using of the load_pbp() function provides the ability to create situation specific metrics that would otherwise be lost in aggregated weekly statistics."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#retrieving-working-with-data-for-multiple-seasons",
    "href": "03-nfl-analytics-functions.html#retrieving-working-with-data-for-multiple-seasons",
    "title": "\n4  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n4.3 Retrieving & Working With Data for Multiple Seasons",
    "text": "4.3 Retrieving & Working With Data for Multiple Seasons\nIn the case of both load_pbp() and load_player_stats(), it is possible to load data over multiple seasons.\nIn our above example calculating average air yard per attempt, it is important to note that Russell Wilson’s league-leading average of 9.89 air yards per attempt is calculated using all passing attempts, meaning pass attempts that were both complete and incomplete.\nIn our first example of working with data across multiple seasons, let’s examine average air yards for only completed passes. To begin, we will retrieve the play-by-play data for the last five seasons:\n\nay.five.years <- nflreadr::load_pbp(2017:2021)\n\nTo retrieve multiple seasons of data, a colon : is placed between the years that you want. When you run the code, nflreadr will output the data to include the play-by-play data starting with the oldest season (in this case, the 2017 NFL season).\nOnce you have the data collected, we can run code that looks quite similar to our code above that explored 2021’s air yards per attempt leaders using load_player_stats(). In this case, however, we are including an additional filter to gather those passing attempts that resulted only in complete passes:\n\naverage.airyards <- ay.five.years %>%\n  group_by(passer_id) %>%\n  filter(season_type == \"REG\" & complete_pass == 1) %>%\n  summarize(player = first(passer_player_name),\n            completions = sum(complete_pass),\n            air.yards = sum(air_yards),\n            average = air.yards / completions) %>%\n  filter(completions >= 1000) %>%\n  arrange(-average)\n\ntibble(average.airyards)\n\n# A tibble: 22 x 5\n   passer_id  player      completions air.yards average\n   <chr>      <chr>             <dbl>     <dbl>   <dbl>\n 1 00-0031503 J.Winston          1008      8174    8.11\n 2 00-0033537 D.Watson           1186      8461    7.13\n 3 00-0029263 R.Wilson           1603     10939    6.82\n 4 00-0026143 M.Ryan             1954     13051    6.68\n 5 00-0034855 B.Mayfield         1185      7858    6.63\n 6 00-0034857 J.Allen            1245      8221    6.60\n 7 00-0033077 D.Prescott         1613     10449    6.48\n 8 00-0026498 M.Stafford         1668     10804    6.48\n 9 00-0029701 R.Tannehill        1049      6680    6.37\n10 00-0032950 C.Wentz            1505      9491    6.31\n# ... with 12 more rows\n\n\nOf those QBs with at least 1,000 complete passes since the 2017 season, Jameis Winston has the highest average air yards per complete pass at 8.11."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#working-with-the-various-nflreadr-functions",
    "href": "03-nfl-analytics-functions.html#working-with-the-various-nflreadr-functions",
    "title": "\n4  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n4.4 Working with the Various nflreadr Functions",
    "text": "4.4 Working with the Various nflreadr Functions\nThe nflreadr package comes with a multitude of “under the hood” functions designed to provide you with supplemental data, items for data visualization, and utilities for efficiently collecting and storing the data on your system. You can view the entire list of these options using the ls to output all the objects in the package.\n\nls(\"package:nflreadr\")\n\n [1] \"clean_homeaway\"               \"clean_player_names\"          \n [3] \"clean_team_abbrs\"             \"clear_cache\"                 \n [5] \"csv_from_url\"                 \"dictionary_combine\"          \n [7] \"dictionary_contracts\"         \"dictionary_depth_charts\"     \n [9] \"dictionary_draft_picks\"       \"dictionary_espn_qbr\"         \n[11] \"dictionary_ff_opportunity\"    \"dictionary_ff_playerids\"     \n[13] \"dictionary_ff_rankings\"       \"dictionary_injuries\"         \n[15] \"dictionary_nextgen_stats\"     \"dictionary_participation\"    \n[17] \"dictionary_pbp\"               \"dictionary_pfr_passing\"      \n[19] \"dictionary_player_stats\"      \"dictionary_rosters\"          \n[21] \"dictionary_schedules\"         \"dictionary_snap_counts\"      \n[23] \"dictionary_trades\"            \"ffverse_sitrep\"              \n[25] \"get_current_season\"           \"get_current_week\"            \n[27] \"get_latest_season\"            \"join_coalesce\"               \n[29] \"load_combine\"                 \"load_contracts\"              \n[31] \"load_depth_charts\"            \"load_draft_picks\"            \n[33] \"load_espn_qbr\"                \"load_ff_opportunity\"         \n[35] \"load_ff_playerids\"            \"load_ff_rankings\"            \n[37] \"load_from_url\"                \"load_injuries\"               \n[39] \"load_nextgen_stats\"           \"load_officials\"              \n[41] \"load_participation\"           \"load_pbp\"                    \n[43] \"load_pfr_advstats\"            \"load_pfr_passing\"            \n[45] \"load_player_stats\"            \"load_players\"                \n[47] \"load_rosters\"                 \"load_rosters_weekly\"         \n[49] \"load_schedules\"               \"load_snap_counts\"            \n[51] \"load_teams\"                   \"load_trades\"                 \n[53] \"most_recent_season\"           \"nflverse_download\"           \n[55] \"nflverse_game_id\"             \"nflverse_releases\"           \n[57] \"nflverse_sitrep\"              \"parquet_from_url\"            \n[59] \"player_name_mapping\"          \"progressively\"               \n[61] \"qs_from_url\"                  \"raw_from_url\"                \n[63] \"rbindlist_with_attrs\"         \"rds_from_url\"                \n[65] \"team_abbr_mapping\"            \"team_abbr_mapping_norelocate\"\n\n\nGoing forward in this chapter, we will be exploring specific use cases for the functions provided by nflreadr - but not all of them. For example, the dictionary_ functions can more easily be used directly on the nflreadr website where the package’s maintainers keep a copy of each. Many, like the dictionary for play-by-play data, includes a search feature to allow you to quickly find how the variables you are looking for is provided in the column name. Others, like the clear_cache() function is used only when you want to wipe any memoized data stored by nflreadr - often needed if you are troubleshooting a pesky error message - join_coalesce() which is an experimental function that is only used internally to help build player IDs into the data. The load_pbp() and load_player_stats() function will also not be covered here, as the first portion of this chapter explored the use of both in great detail. We will briefly discuss the use of load_players() and load_rosters() but a more detailed discussion of each is provided in Chapter 4: Data Visualization with NFL Analytics. The remaining functions will be presented in the order provided on the nflreadr function reference website.\n\n4.4.1 The load_participation() Function\nThe load_participation() function allows us to create a dataframe of player participation data dating back to 2016 with an option to infuse that information into the existing nflreadr play-by-play data. The resulting dataframe, when not including play-by-play data, includes information pertaining to: the individial game ID, the play ID, the possession team, what formation the offense was in, the layout of the offensive personnel, how many defenders were in the box, the defensive personnel, the number of rushers on pass players, and the unique ID number for each player on the field for that specific play.\nDespite the load_participation() function being one of the newest editions to the nflreadr package, it is already being used to create contextual analysis regarding a team’s use of its players out out formations. For example, Joseph Hefner uses the data to create tables (built with the gt packages) that calculates not only each player’s rate of usage out of different personnel packages, but how the team’s EPA per play and pass rate fluncuate with each. In the spirit of R’s open-source nature, he also created a Team Formation ShinyApp that allows anybody to explore the data and output the results in .png format.\n\n\nTo build a dataframe of 2022 participation data, that includes complete play-by-play, you must pass both the season and include_pbp argument with the load_participation() function.\n\nparticipation <- nflreadr::load_participation(season = 2022, include_pbp = TRUE)\n\nparticipation\n\n# A tibble: 49,969 x 383\n   nflverse_ga~1 play_id posse~2 offen~3 offen~4 defen~5 defen~6 numbe~7 playe~8\n   <chr>           <int> <chr>   <chr>   <chr>     <int> <chr>     <int> <chr>  \n 1 2022_01_BAL_~       1 \"\"      <NA>    <NA>         NA <NA>         NA \"\"     \n 2 2022_01_BAL_~      43 \"BAL\"   <NA>    <NA>         NA <NA>         NA \"47969~\n 3 2022_01_BAL_~      68 \"NYJ\"   SINGLE~ 1 RB, ~       7 3 DL, ~      NA \"53536~\n 4 2022_01_BAL_~      89 \"NYJ\"   SHOTGUN 1 RB, ~       6 3 DL, ~       4 \"53536~\n 5 2022_01_BAL_~     115 \"NYJ\"   SINGLE~ 1 RB, ~       7 3 DL, ~      NA \"53536~\n 6 2022_01_BAL_~     136 \"NYJ\"   SHOTGUN 1 RB, ~       7 3 DL, ~       4 \"53536~\n 7 2022_01_BAL_~     172 \"NYJ\"   <NA>    <NA>         NA <NA>         NA \"53059~\n 8 2022_01_BAL_~     202 \"BAL\"   SINGLE~ 2 RB, ~       7 4 DL, ~       3 \"44929~\n 9 2022_01_BAL_~     230 \"BAL\"   SHOTGUN 2 RB, ~       6 4 DL, ~       3 \"44929~\n10 2022_01_BAL_~     254 \"BAL\"   EMPTY   1 RB, ~       5 4 DL, ~      NA \"44929~\n# ... with 49,959 more rows, 374 more variables: offense_players <chr>,\n#   defense_players <chr>, n_offense <int>, n_defense <int>, old_game_id <chr>,\n#   home_team <chr>, away_team <chr>, season_type <chr>, week <int>,\n#   posteam <chr>, posteam_type <chr>, defteam <chr>, side_of_field <chr>,\n#   yardline_100 <dbl>, game_date <chr>, quarter_seconds_remaining <dbl>,\n#   half_seconds_remaining <dbl>, game_seconds_remaining <dbl>,\n#   game_half <chr>, quarter_end <dbl>, drive <dbl>, sp <dbl>, qtr <dbl>, ...\n\n\nRather than exploring the data at the league-level, let’s take a micro-approach to the participation data and examine the the 2022 Pittsburgh Steelers. It is important to note that the participation data, as gathered using the load_participation() function, is not fully prepped for immediate analysis as the players_on_play, offense_players, and defense_players information (which contains the unique player indentification numbers) are separated by a delimiter (in this case, a semicolon) in what are called “concatenated strings” or “delimited strings.” While this is a compact way to store the values in the core dataframe, it does require us to clean the information through the “splitting” or “tokenizing” process. As you will see in the below code, we are going to place the unique identifiers into separate rows, based on formation, by utilizing the separate_rows() function in tidyr.\n\n\n\n\n\n\nTip\n\n\n\nIf you use load_participation() but set include_pbp = FALSE it is important to remember that the posteam variable that is part of the play-by-play data will not exist. Instead, the offensive team is indicated by the possession_team column.\nIf you gather the participation data with play-by-play information, you can use either possession_team or posteam for any filtering.\nThe same is not true when looking at defensive participation data. A defensive team variable is not provided without included the play-by-play in the data. Once set to TRUE, the defensive team is housed under the typical defteam column.\n\n\n\nparticipation_split <- participation %>%\n  filter(!is.na(offense_formation)) %>%\n  filter(posteam == \"PIT\") %>%\n  tidyr::separate_rows(offense_players, sep = \";\") %>%\n  group_by(offense_personnel, offense_players) %>%\n  summarize(total = n()) %>%\n  ungroup()\n\nparticipation_split\n\n# A tibble: 157 x 3\n   offense_personnel offense_players total\n   <chr>             <chr>           <int>\n 1 1 RB, 1 TE, 3 WR  00-0032897          3\n 2 1 RB, 1 TE, 3 WR  00-0033869        262\n 3 1 RB, 1 TE, 3 WR  00-0034142         37\n 4 1 RB, 1 TE, 3 WR  00-0034347        781\n 5 1 RB, 1 TE, 3 WR  00-0034768        781\n 6 1 RB, 1 TE, 3 WR  00-0034785        744\n 7 1 RB, 1 TE, 3 WR  00-0034928        235\n 8 1 RB, 1 TE, 3 WR  00-0035216        745\n 9 1 RB, 1 TE, 3 WR  00-0035217         25\n10 1 RB, 1 TE, 3 WR  00-0035222        258\n# ... with 147 more rows\n\n\nWe are doing quite a few things in the above example:\n\n\nThose rows where the is an ‘NA’ value for offense_formation are removed. The participation data includes information for kickoffs, extra points, field goals, punts, QB kneels, no plays, and more. It is possible to remove items using play_type == \"run\" | play_type == \"pass\" but such an approach, if done with the Steelers, results in one fake punt being included in the data (as the play_type was listed as “run”). If you wish to include plays such as fake punts, you do so by filtering for only specific play types.\n\nOnly those rows that includes ‘PIT’ as the posteam are included.\n\n\nThe separate_rows() function from tidyr is used to conduct the splitting of the concatenated players identifiers in the offense_players column. The separate_rows() function needs just to arguments to complete the process - the column name and the delimiter (provided in the argument as sep =).\n\nThe newly split data is then grouped by offense_personnel and offense_players in order to calculate the number of times each specific ID was associated with personnel package. The resulting dataframe lists the offense formation type for each player that participated in it, along with the player’s respective participation count.\n\nAs is, the load_participation() data does not include any way to further identify players outside of the unique players IDs. Because of this, it is necessary to use the load_rosters() function to include this information (this process is included below in the load_rosters() section).\n\n4.4.2 The load_rosters() and load_rosters_weekly() Functions\nThe load_rosters() function houses a multitude of useful tools for use in the nflverse. With player information dating back to 1920, load_rosters() will provide you with basic demographic information about each player in the NFL such as their name, birth date, height, weight, the college and high school they attended, and items regarding primary position and depth chart position.\nImportantly, load_rosters() also provides the unique identifier for each player over nine different sources: the gsis_id (which is the core ID used in nflverse data), sleeper_id, espn_id, yahoo_id, rotowire_id, pff_id, fantasy_data_id, sportradar_id, and pfr_id. The different IDs become extremely useful when using data collected from outside the nflverse, or when combining data from two outside sources, matching the information can be done by matching the various IDs as needed. As well, as outlined in Chapter 4: Data Visualization with NFL Analytics, the URLs for player headshots are included in load_rosters() data.\nLet’s return to our participation_split dataframe that we built prior in this chapter while working with the load_participation() function. The the data is formatted and prepared for analysis, there is no information that allows for easy identification of each player (aside from unique IDs, which is not helpful). To correct this, we can bring in the 2022 Pittsburgh Steelers roster information.\n\nrosters <- nflreadr::load_rosters(2022)\n\nThe only argument you must provide the load_rosters() function is the years in which you want to collect roster information (in this specific case, 2022). While we know we only want the Steelers’ roster, it is not suggested to use filter() to gather any specific team at this point. Case in point: do so with this example will result in the gsis_id 00-0036326 having an “NA” value for a name, despite having over 380 snaps in the 1 RB, 1TE, 3 WR offensive formation. The player associated with that gsis_id is Chase Claypool, who the Steelers traded to the Bears on November 1 of 2022. Because of this, no player name will be associated with that specific gsis_id on the Steelers’ roster (with the end result being a missing value).\n\n\n\n\n\n\nNote\n\n\n\nYou may notice that the resulting dataframe, titled pit_roster, has 80 players on the roster despite NFL teams only being permitted to have 53 active players at a time.\nThis is because the information collected with load_rosters() includes not only active players, but those listed on the practice squad and injured reserve. Additionally, you may notice players listed as R/Retired (such as Stephon Tuitt in our dataframe, who retired at the end of the 2022 season). In these situations, this status indicated that the player is listed as ‘reserved-retired’ and that the team continues to hold the player’s rights until the official expiration of their contract. In this case, Tuitt could not come out of retirement prior to his contract ending and play with another team unless first released or traded by Pittsburgh.\n\n\nDespite the wealth of information in the roster information, only the gsis_id and the full_name is required to complete our participation_split dataframe. Rather than bringing unnecessary information over during the merge, we can select just the two columns we need and then use left_join() to merge each player’s name on the matching ID in the offense_players column and gsis_id from the roster information.\n\nrosters <- nflreadr::load_rosters(2022)\n\nrosters <- rosters %>%\n  select(gsis_id, full_name)\n\nparticipation_split <- participation_split %>%\n  left_join(rosters, by = c(\"offense_players\" = \"gsis_id\"))\n\nparticipation_split\n\n# A tibble: 157 x 4\n   offense_personnel offense_players total full_name        \n   <chr>             <chr>           <int> <chr>            \n 1 1 RB, 1 TE, 3 WR  00-0032897          3 Derek Watt       \n 2 1 RB, 1 TE, 3 WR  00-0033869        262 Mitchell Trubisky\n 3 1 RB, 1 TE, 3 WR  00-0034142         37 J.C. Hassenauer  \n 4 1 RB, 1 TE, 3 WR  00-0034347        781 James Daniels    \n 5 1 RB, 1 TE, 3 WR  00-0034768        781 Chukwuma Okorafor\n 6 1 RB, 1 TE, 3 WR  00-0034785        744 Mason Cole       \n 7 1 RB, 1 TE, 3 WR  00-0034928        235 Steven Sims      \n 8 1 RB, 1 TE, 3 WR  00-0035216        745 Diontae Johnson  \n 9 1 RB, 1 TE, 3 WR  00-0035217         25 Benny Snell      \n10 1 RB, 1 TE, 3 WR  00-0035222        258 Zach Gentry      \n# ... with 147 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe load_rosters() data is capable of providing data for multiple seasons at once. Through each season, a player’s gsis_id will remain static. Despite this, when merging multiple years of participation data with multiple years of roster information, the data must be matched on the season variable was well. This process involves including season in both the participation data group_by() and the roster information, as seen below.\n\nparticipation_multi_years <- load_participation(season = 2018:2022, include_pbp = TRUE)\n\nparticipation_2018_2022 <- participation_multi_years %>%\n  filter(!is.na(offense_formation)) %>%\n  filter(posteam == \"PIT\") %>%\n  tidyr::separate_rows(offense_players, sep = \";\") %>%\n  group_by(season, offense_personnel, offense_players) %>%\n  summarize(total = n())\n\nrosters_2018_2022 <- nflreadr::load_rosters(2018:2022) %>%\n  select(season, gsis_id, full_name)\n\nparticipation_2018_2022 <- participation_2018_2022 %>%\n  left_join(rosters_2018_2022, by = c(\"season\", \"offense_players\" = \"gsis_id\"))\n\n\n\nIf your analysis requires providing great granularity, the load_rosters_weekly() function provides the same information as load_rosters() but structures it in weekly format by season (or over multiple seasons, if needed).\n\n4.4.3 The load_teams() Function\nA tool heavily used in the data vizualization process, the load_teams() function provides information pertaining to each NFL team’s colors and logos, as well as providing a way to merge dataframes that have differing values for teams. The\n\n4.4.4 The load_schedules() Function\nfffff\n\n4.4.5 The load_officials() Function\nThe load_officials() data will return data, from 2015 to present, outlining which officials were assigned to which game. The data also includes information regarding each referee’s position, jersey number, and their official NFL ID number. Importantly, the data is structured to also include both a game_id and game_key that are sorted by season and week, allowing you to merge the information other dataframes.\nWith the data, we can - for example - examine which NFL officiating crews called the most penalties during the 2022 NFL season. Doing so requires a bit of work in order to assign a unique crew_id to each stable of officials “working” under a lead referee.\n\nnfl_officials <- nflreadr::load_officials(seasons = 2022)\n\nreferees <- nfl_officials %>%\n  filter(position == \"Referee\") %>%\n  mutate(crew_id = match(official_id, unique(official_id)))\n\nnfl_officials <- nfl_officials %>%\n  left_join(referees %>% select(game_id, crew_id), by = \"game_id\")\n\npenalty_pbp <- nflreadr::load_pbp(seasons = 2022)\n\npenalties <- penalty_pbp %>%\n  filter(penalty == 1) %>%\n  select(game_id, old_game_id, season, play_id, desc, home_team, away_team,\n         posteam, defteam, week, penalty_team, penalty_type, penalty, penalty_player_id,\n         penalty_player_name, penalty_yards)\n\ntotal_penalties <- penalties %>%\n  group_by(old_game_id) %>%\n  summarize(\n    total_called = sum(penalty == 1, na.rm = TRUE),\n    total_yards = sum(penalty_yards, na.rm = TRUE))\n\nnfl_officials <- nfl_officials %>%\n  left_join(total_penalties, by = c(\"game_id\" = \"old_game_id\")) %>%\n  select(game_id, official_name, position, crew_id, total_called, total_yards)\n  \ntallied <- nfl_officials %>%\n  group_by(game_id) %>%\n  filter(position == \"Referee\") %>%\n  ungroup() %>%\n  group_by(crew_id) %>%\n  reframe(\n    referee = unique(official_name),\n    all_pen = sum(total_called, na.rm = TRUE),\n    all_yards = sum(total_yards, na.rm = TRUE))\n\n\n4.4.6 The load_trades() Function\nffffff\n\n4.4.7 The load_draft_picks() Function\nfffffff\n\n4.4.8 The load_combine() Function\nfffffff\n\n4.4.9 The load_nextgen_stats() Function\nffffff\n\n4.4.10 The load_depth_charts() Function\nfffffff\n\n4.4.11 The load_injuries() Function\nfffffff\n\n4.4.12 The load_espn_qbr() Function\nffffff\n\n4.4.13 The load_pfr_advstats() Function\nfffffff\n\n4.4.14 The load_snap_counts() Function\nfffffff\n\n4.4.15 The load_contracts() Function\nfffffff\n\n4.4.16 The clean_player_names() Function\nffffff\n\n4.4.17 The clean_team_abbrs() Function\nffffff\n\n4.4.18 The clean_homeaway() Function"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#data-viz-must-understand-the-audience",
    "href": "04-nfl-analytics-visualization.html#data-viz-must-understand-the-audience",
    "title": "\n5  Data Visualization with NFL Analytics\n",
    "section": "\n5.1 Data Viz Must Understand the Audience",
    "text": "5.1 Data Viz Must Understand the Audience\nAs explained by Stikeleather, the core purpose of a data visualization is to take “great quantities of information” and then convey that information in such a way that it is “easily assimilated by the consumers of the information.” In other words, the process of data visualization should allow for a great quantity of data to be distilled into an easily consumable (and understandable!) format.\nSpeaking specifically to NFL analytics, when doing visualizations we must be conscious about whether or not the intended audience will understand the terminology and concepts we use in the plot. For example, most all NFL fans understand the “non-advanced” statistics in the sport. But, when plots start using metrics such as EPA or completion percentage over expected, for example, the audience looking at the plot may very well have little understanding of what is being conveyed.\nBecause of this, any data viz I create never fails to include “directables” within the plot. These “directables” may be arrows that indicate which trend on the plot are “good” or they can be text within a scatterplot that explains what each quadrant means. Or, for example, I sometimes include a textual explanation of the “equation” used to develop a metric as seen below:\n\n\n\n\n\nThe above plot explores which QBs, from the 2020 season, were most aggressive on 3rd down with between 5 to 10 yards to go. Since “aggressiveness” is not a typical, day-to-day metric discussed by NFL fans, I included a “directable” within the subtitle of the plot that explained that the plot, first, was examining just 3rd down pass attempts within a specific yard range. And, second, I made the decision to include how “aggressiveness” was calculated by including the simple equation within the subtitle as well. Doing so allows even the most casual of NFL fans to easily understand what the plot is showing - in this case, that Joe Burrow’s 3rd down pass attempts with between 5 to 10 yards to go made it to the line of gain, or more, on 68% of his attempts. On the other hand, Drew Lock and Drew Brees were the least aggressive QBs in the line based on the same metric.\nAs another example, below is what I deemed my “Uncle Rico Metric” (because who does not like a good Napoleon Dynamite reference?):"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#setting-up-for-data-viz",
    "href": "04-nfl-analytics-visualization.html#setting-up-for-data-viz",
    "title": "\n5  Data Visualization with NFL Analytics\n",
    "section": "\n5.2 Setting Up for Data Viz",
    "text": "5.2 Setting Up for Data Viz\nWhile most of your journey through NFL analytics in this book required you to use the tidyverse and a handful of other packages, the process of creating compelling and meaningful data visualizations will require you to utilize multitudes of other packages. Of course, the most important is ggplot2 which is already installed via the tidyverse. However, in order to recreate the visualizations included in this chapter, it is required that you install other R packages. To install the necessary packages, you can run the following code in RStudio:\nNeed to install: ggrepel"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#selecting-the-correct-type-of-plot",
    "href": "04-nfl-analytics-visualization.html#selecting-the-correct-type-of-plot",
    "title": "\n5  Data Visualization with NFL Analytics\n",
    "section": "\n5.3 Selecting The Correct Type of Plot",
    "text": "5.3 Selecting The Correct Type of Plot\nContent coming soon and testing flowchart.\n\n\n\n\n\nflowchart\n\nA[Data Type]\nA --> B(Continuous)\nA --> C(Discrete)\nC --> D(Two Discrete Variables)\nD --> E(Relationships)\nD --> F(Counts)\nE --> G{geom_point}\nE --> H{geom_jitter}\nC --> I(One Discrete, One Continuous)\nI --> J(Distribution)\nI --> K(Summary)\nJ --> L{geom_boxplot}\nJ --> M{geom_violin}\nK --> N{geom_bar}\nK --> O{geom_col}\nB --> P(Two Continuous Variables)\nP --> Q{geom_point}\nP --> R{geom_line}\nB --> S(One Continuous, One Discrete)\nS --> T(Distribution)\nS --> U(Summary)\nT --> V{geom_boxplot}\nT --> W{geom_violin}\nU --> X{geom_bar}\nB --> Y(One Continuous)\nY --> Z(Histogram)\nY --> AA(Density)\nZ --> BB{geom_histogram}\nZ --> CC{geom_density}"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#the-basics-of-using-ggplot2",
    "href": "04-nfl-analytics-visualization.html#the-basics-of-using-ggplot2",
    "title": "\n5  Data Visualization with NFL Analytics\n",
    "section": "\n5.4 The Basics of Using ggplot2\n",
    "text": "5.4 The Basics of Using ggplot2\n\nThe basics of any ggplot visualization involves three basic calls to information in a dataset as well as stipulation which type of geom you would like to use:\n\nthe dataset to be used in the visualization\nan aesthetic calls for the x-axis\nan aesthetic call for the y-axis\nyour desired geom type\n\n\nggplot(data = 'dataset_name', aes(x = 'x_axsis', y = 'y_axis')) +\n  geom_type()\n\nTo showcase this, let’s use data from Sports Info Solutions that pertains quarterback statistic when using play action versus when not using play action. To start, collect the data using the vroom function.\n\nplay_action_data <- vroom::vroom(\"https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/play_action_data.csv\")\n\nTo provide an easy-to-understand example of building a visualization with ggplot, let’s use each QB’s total yardage when using play action and when not. In this case, our two variable names are yds and pa_yds with the yds variable being place on the x-axis and the pa_yds variable being placed on the y-axis.\n\n\n\n\n\n\nTip\n\n\n\nIt is important to remember which axis is which as you begin to learn using ggplot:\nThe x-axis is the horizontal axis that runs left-to-right.\nThe y-axis is the vertical axis that runs top-to-bottom.\n\n\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point()"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#building-a-data-viz-a-step-by-step-process",
    "href": "04-nfl-analytics-visualization.html#building-a-data-viz-a-step-by-step-process",
    "title": "\n5  Data Visualization with NFL Analytics\n",
    "section": "\n5.5 Building A Data Viz: A Step-by-Step Process",
    "text": "5.5 Building A Data Viz: A Step-by-Step Process\nThe outputted scatter plot is an excellent starting point for a more finely detailed visualization. While we are able to see the relationship between non-play action passing yards and those attempts that included play action, we are unable to discern which specific point is which quarterback - among other issues. To provide more detail and to “prettify” the plot, let’s discuss doing the following:\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both the x-axis and y-axis\nChange theme elements to make data viz more appealing.\n\n\n\n\n5.5.1 Adding Team Colors to Each Point\n\n\n\n\n\n\nNote\n\n\n\nMuch like anything in the R language, there are multiple ways to go about add team colors (and logos, player headshots, etc.) to visualizations.\nFirst, we can merge team color information into our play_action_data and then manually set the colors in our geom_point call.\nSecond, we can conduct the same merge but then use the nflplotRr package (which is part of the nflverse) to bring the colors in.\nBoth examples will be included in the below example. However, please note that I typically do not use the nflplotR package in scatter plots as, in certain circumstances, adding the colors manually provides the necessary control that nflplotR sometimes cannot provide.\nThat said, the power of nflplotR is extremely evident when working with team logos and/or player headshots (which will be covered later in this chapter).\n\n\nTo start, we will load team information using the load_teams function within nflreadr. In this case, we are requesting that the package provide only the 32 current NFL teams with by including the current = TRUE argument. Conversely, setting the argument to current = FALSE will result in historical NFL teams being included in the data (the Oakland Raiders and St. Louis Rams, for example, will be included in the data). We will also use the select verb from dplyr to gather just the variables we know we will need (team_abbr, team_nick, team_color, and team_color2.\n\n\n\n\n\n\nImportant\n\n\n\nWe are only including the team_abbr variable in this example because we are going to create the plot both with and without the use of nflplotR. As of the writing of this book, the newest development version of the package is 1.1.0.9004 and does not yet (if ever) provide support to use team nicknames. Because of this, we must include team_abbr in our merge since it is the team name version that is standardized for use in nflplotr.\n\n\nAfter collecting the team information needed, we can conducted a left_join to match the information by team in play_action_data and team_nick in the team information from load_teams() and then confirm the merge was successful by viewing the columns names in play_action_data with colnames().\n\nteams <- nflreadr::load_teams(current = TRUE) %>%\n  select(team_abbr, team_nick, team_color, team_color2)\n\nplay_action_data <- play_action_data %>%\n  left_join(teams, by = c(\"team\" = \"team_nick\"))\n\ncolnames(play_action_data)\n\nWith the team color information now built into our play_action_data, we can incorporate the team color specific to each point by including the color argument within our geom_point.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(color = play_action_data$team_color)\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou may notice that we used the the $ special operator to extract the team_color information within our play_action_data dataframe. This is an extremely important distinction, as using the aes() argument from ggplot and not using the $ operator will result in a custom scale color being applied to each team, without the correct team colors associating with the correct team.\nTo see this for yourself, you can run the example following code. Remember, this is an incorrect approach and serves to only highlight why the $ special operator was used in the above code.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(aes(color = team_color))\n\n\n\n\n\n\n\n\n\nAs mentioned, the same result can be achieved using the nflplotR package. The following code will do so:\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(aes(color = team_abbr)) +\n  nflplotR::scale_color_nfl(type = \"primary\")\n\n\n\n\n\n\n\nIn the above example, you will notice that we are including the team color information in an aes() call within the geom_point() function. This is because we ultimately control the specific of the custom scale through the use of scale_color_nfl in the nflplotR package, which also allows us to select whether we want to display the primary or secondary team color.\nGiven the two examples above, a couple items regarding the use of nflplotR should become apparent.\n\nIf your data already includes team names in team_abbr format (that is: BAL, CIN, DET, DAL, etc.), then using nflplotR is likely a more efficient option as you do not need to merge in team color information. In other words, our play_action_data information could contain just the variables for player, team_abbr, yds, and pa_yds and nflplotR would still work as the package, “behind the scenes”, will automatically correlate the team_abbr with the correct color for each team.\nHowever, if your data does not include teams in team_abbr format and you must merge in information manually, it is likely more efficient to use the $ special operator to bring the team colors in without using the aes() call within geom_point().\n\nFinally, because we have both team_color and team_color2 - the primary and secondary colors for each team - in the data, we can get fancy and create points that are filled with the primary team color and outlined by the secondary team color. Doing so simply requires changing the type of our geom_point. In the below example, we are specifying that we want a specific type of geom_point by using shape = 21 and then providing the fill color and the outline color with color. In each case, we are again using the $ special operator to select the primary and secondary color associated with each team.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2)\n\n\n\n\n\n\n\nWith team colors correctly associated with each point, we can turn back to our “to do list” to see what part of the job is next.\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both the x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n5.5.2 Increasing The Size of Each Point\nDetermining when and how to resize the individual points in a scatterplot is a multifaceted decision. To that end, there is no hard and fast rule for doing so as it depends on both the specific goals and context of the visualization. There are, however, some general guidelines to keep in mind:\n\nData density: if you have a lot of data points within your plot, making the points smaller may help to reduce issues of overlapping/overplotting. Not only is this more aesthetically pleasing, but it can also help in making it easier to see patterns.\nImportant of individual points: if certain points within the scatterplot are important, we may want to increase the size of those specific points to make them standout.\nVisual aesthetics: the size of the points can be adjusted simply for visual appeal.\nContextual factors: can the size of the points be used to highlight even more uniqueness in the data? For example, given the right data structure, we can size individual dots to show the spread in total attempts across the quarterbacks.\n\nGiven the above guidelines, the resizing of the points in our play_action_data dataframe is going to be a strictly aesthetic decision. We cannot, as mentioned above, alter the size of each specific points based on each quarterback’s number of attempts as the data provides attempts for both play action and non-play action passes. Moreover, we could create a new column that add both attempt numbers to get a QB’s cumulative total but that does not have a distinct correlation to the data on either axis.\n\n\n\n\n\n\nDanger\n\n\n\nFor the sake of educational purposes, we can alter the size of each specific point to correlate to the total number of play action attempts for each quarterback (and then divide this by 25 in order to decrease the size of the points to fit them all onto the plot).\nAgain: it is important to point out that this not a good approach to data visualization, as the size of the dots correlates to just one of the variables being explored in the plot.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = play_action_data$pa_att / 25)\n\n\n\n\n\n\n\n\n\nIn order to maintain correct visualization standards, we can resize the points for nothing more than aesthetic purposes (that is: making them bigger so they are easier to see). To do so, we still add the size argument to our geom_point but simply provide a numeric value to apply uniformly across all the points. To process of selecting the numeric value is a case of trial and error - inputting and running, changing and running, and changing and running again until you find the size that provides easier to see points without adding overlap into the visualization.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5)\n\n\n\n\n\n\n\nWith the size of each point adequately adjusted, we can move on to the next part of our data visualization “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both the x-axix and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n5.5.3 Adding Player Names to Each Point\nWhile the our current plot includes team-specific colors for the points, we are still not able to discern - for the most part - which player belongs to which point. To rectify this, we will turn to using the ggrepel package, which is designed to improve the readability of text labels on plots by automatically repelling overlapping labels, if any. To that end, ggrepel operates out of two main functions: geom_text_repel and geom_label_repel. Both provide the same end result, with the core different being geom_label_repel adding a customized label under each player’s name.\nWe can do a bare minimum addition of the player names by adding one additional line of code using geom_text_repel and wrapping it in an aes() call and specifying which variable in the play_action_data is the label we would like to display.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player))\n\n\n\n\n\n\n\nWhile it is a good first attempt at adding the names, many of them are awkwardly close to the respective point. Luckily, the ggrepel package provides plenty of built-in customization options:\n\n\n\nggrepel Option1\n\nDescription of Option\n\n\n\nseed\na random numeric seed number for the purposes of recreating the same layout\n\n\nforce\nforce of repulsion between overlapping text labels\n\n\nforce_pull\nforce of attraction between each text label and its data point\n\n\ndirection\nmove the text labels in either “x” or “y” directions\n\n\nnudge_x\nadjust the starting x-axis starting position of the label\n\n\nnudge_y\nadjust the starting y-axis starting position of the label\n\n\nbox.padding\npadding around the text label\n\n\npoint.padding\npadding around the labeled data point\n\n\narrow\nrenders the line segment as an arrow\n\n\n\nOf the above options, the our current issue with name and point spacing can be resolved by including a numeric value to the box.padding. Moreover, we can control the look and style of the text (such as size, font family, font face, etc.) in much the same way. To make these changes, we can set the box.padding to 0.45, set the size of the text to 3 using size as well as switch the font to ‘Roboto Condensed’ using family, and - finally - make it bold using fontface.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\")\n\n\n\n\n\n\n\nThe plot, as is, is understandable in that we are able to associate each point with a specific quarterback to examine how a quarterback’s total passing yardage is split between play action and non-play action passes. While the graph could hypothetically “standalone” as is - minus a need for a title - we can still do work on it to make it more presentable. Let’s return to our data visualization “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n5.5.4 Editing Axis Ticks and Values\nBecause steps 4 and 5 from the above “to do list” can be accomplished with the same package, we will lump both together and complete them at once.\nLet’s first examine the idea of increasing the number of ticks on each axis. The “axis ticks” refer to the specific spots on each axis wherein a numeric datapoint resides. With our current visualization, we currently have “1000, 2000, 3000” on the x-axis and “400, 800, 1200, 1600” on the y-axis.\nWe may want to increase the number of axis ticks in this visualization as it can provide a more detailed view of the data being presented. Typically, increasing the number of tickets will show more granularity in the data and make it easier to interpret the values represented by each point. In this specific case, we can look at the cluster of points represented by Andy Dalton, Mac Jones, and Matt Ryan. Given the current structure of the axis ticks, we can guess that Matt Ryan has roughly 2,500 yards on non-play action passing attempts. Given we know Matt Ryan’s amount, we can make guesses that Andy Dalton may be around 2,300 and Mac Jones somewhere between the two.\nBy increasing the number of values on each axis, we have the ability to see more specific results. Conversely, we must be careful to not add too many so that the data becomes overwhelming to interpret. Much like the size of geom_point was a case of trial and error, so is selecting an appropriate amount of ticks.\nHowever, before implementing these changes, we need to segue into a discussion on continuous and discrete data.\n\n\n\n\n\n\nImportant\n\n\n\nWhen implementing changes to either the x- or y-axis in ggplot, you will be working with either continuous or discrete data, and using the scale_x_continuous or scale_x_discrete functions (or replacing x with y when working with the opposite axis). In either case, both functions allow you to customize the axis of a plot but are used for different types of data.\nscale_x_continuous is used for continuous (or numeric) data, where the axis is represented by a continuous range of numeric values. The values within a continuous axis can take on any number within the given range.\nscale_x_discrete is used for discrete data (or often character-based data). You will see this function used when working with variables such as player names, teams, college names, etc. In any case, discrete data is limited to a specific set of categories.\nPlease know that ggplot will throw an error if you try to apply a continuous scale to discrete data, or the opposite, that reads: Error: Discrete value supplied to continuous scale.\n\n\nIn the case of our current plot, we now know we will be using the scales_x_continuous and scale_y_continuous functions as both contain continuous (numeric) data. To make the changes, we can turn to the scales package and its pretty_breaks function to change the number of “breaks” (or ticks) on each axis. By placing n = 6 within the pretty_breaks argument, we are requesting a total of six axis ticks on both the x- and y-axis.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6))\n\n\n\n\n\n\n\nDespite our request to build the plot with six ticks on each axis, you will see the generated visualization includes seven on the x-axis and eight on the y-axis. This does not mean your code with pretty_breaks did not work. Instead, the pretty_breaks function is designed to internally determine the best axis tick optimization based on your requested number. To that end, the function - “under the hood” - determined that seven and eight ticks, respectively, was the most optimized way to display the data given our desire to have at least six on each.\nWith the number of axis ticks corrected, we can turn our attention to getting the labels of the axis ticks into correct numeric format. Within the same scale_x_continuous or scale_y_continuous arguments, we will use the labels function, combined with another tool from the scales package to make the adjustments.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma())\n\n\n\n\n\n\n\nBy adding a labels option to both scale_x_continuous and scale_y_continuous, we can use the label_comma() option from within the scales package to easily add a comma into numbers that are in the thousands.\nWith much of the heavy lifting for our visualization now complete, we can move on to the final steps in our “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n5.5.5 Changing Axis Titles and Adding Title, Subtitle, and Caption\nMuch like our last section, we can work on changing the title of each axis and adding a title, subtitle, and caption for the plot within one section, as all this is added and/or changed by using labs().\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\")\n\n\n\n\n\n\n\nWithin the new labs(), we are placing a total of five items: x (allowing us to name the x-axis outside the confines of what it is called in the beginning aes() call, y (allowing us to name the y-axis), title (allowing us to add a title to the top of the plot), subtitle (allowing us to add a subtitle below the title and provide more contextual information), and caption (allowing us to provide information about where the graph come from and who designed it). We will explore ways to change the font, size, color, and more of these items when we move on to the last item of our “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n5.5.6 Adding Mean Lines to Both x-axis and y-axis\nAdding mean (or average) lines to both the x-axis and y-axis allows us to visualize where each quarterback falls within one of four sections (according to the amount of passing yards in both situations). Adding the lines is done by add two additional geoms to the existing plot (in this case geom_hline and geom_vline).\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n   geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\") +\n  geom_hline(yintercept = mean(play_action_data$pa_yds), linewidth = .8, color = \"black\", linetype = \"dashed\") +\n  geom_vline(xintercept = mean(play_action_data$yds), linewidth = .8, color = \"black\", linetype = \"dashed\")\n\n\n\n\n\n\n\nIn the above output, we used geom_hline and geom_vline to draw a dashed line at the average for both pa_yds at the yintercept and yds at the xintercept. Because of this, we can see that - for example - Marcus Mariota is above average for play action yards, but below average for non-play action yards. Additionally, Matthew Stafford, Baker Mayfield, Kyler Murray, and many others are all below average in both metrics while Jared Goff, Justin Herbert, Patrick Mahomes, and others are well above average in both.\nHowever, adding the geom_hline and geom_vline at the very end of the ggplot code creates an issues (and one I’ve intentionally created for the purposes of education). As you look at the plot, you will see that the dashed line runs on top of the names and dots in the plot. This is because ggplot follows a very specific ordering of layering.\n\n\n\n\n\n\nImportant\n\n\n\nIn ggplot, it is important to remember that items in a plot are layered in the order in which they are added to the plot. This process of layering is important because it ultimately determines which items end up on top of others, which can have significant implications on the visual appearance of the plot.\nAs we’ve seen so far in the process, each layer of a plot is added by including a geom_. The first layer added will always be at the very bottom of the plot, with each additional layer building on top of the previous layers.\n\n\nBecause of the important layering issue highlighted above, it is visually necessary for us to move the geom_hline and geom_vline to the beginning of the ggplot code so both are layered underneath everything else in the plot (geom_point and geom_text_repel in this case). As well, we can apply the alpha option to each to slightly decrease each line’s transparency.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_hline(yintercept = mean(play_action_data$pa_yds), \n             linewidth = .8, \n             color = \"black\", \n             linetype = \"dashed\",\n             alpha = 0.5) +\n  geom_vline(xintercept = mean(play_action_data$yds), \n             linewidth = .8, \n             color = \"black\", \n             linetype = \"dashed\",\n             alpha = 0.5) +\n   geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\")\n\n\n\n\n\n\n\nBy moving the average lines to the top of our ggplot code, both are now layered under the other two geom_ and are not visually impacting the final plot.\n\n5.5.6.1 Adding Mean Lines with nflplotR\n\n\n\n\n\n\n\nTip\n\n\n\nEven though we were not able to use nflplotR to handle the colors in this plot because the data lacked a corresponding team_abbr variable, we can still use nflplotR to add our mean lines - and I actually recommend doing so, as it requires less lines of code (and less typing). See below for an example.\n\n\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\")\n\n\n\n\n\n\n\nBy using the geom_mean_lines() function within nflplotR, we can construct both of the lines together rather than needing to provide a geom_hline() and a geom_vline() argument. Because of this, we can also provide the size, width, and type of our line just once (rather than repeating it again like we had to in the former method).\nWe can now move onto the final item on our data visualization “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n5.5.7 Making Changes to Theme Elements\nThere is a laundry list of options to be explored when it comes to editing your plot’s theme elements to make it look exactly as you want. Currently, according to the ggplot2 website, the following is a comprehensive list of elements that you can tinker with.\n\n  line,\n  rect,\n  text,\n  title,\n  aspect.ratio,\n  axis.title,\n  axis.title.x,\n  axis.title.x.top,\n  axis.title.x.bottom,\n  axis.title.y,\n  axis.title.y.left,\n  axis.title.y.right,\n  axis.text,\n  axis.text.x,\n  axis.text.x.top,\n  axis.text.x.bottom,\n  axis.text.y,\n  axis.text.y.left,\n  axis.text.y.right,\n  axis.ticks,\n  axis.ticks.x,\n  axis.ticks.x.top,\n  axis.ticks.x.bottom,\n  axis.ticks.y,\n  axis.ticks.y.left,\n  axis.ticks.y.right,\n  axis.ticks.length,\n  axis.ticks.length.x,\n  axis.ticks.length.x.top,\n  axis.ticks.length.x.bottom,\n  axis.ticks.length.y,\n  axis.ticks.length.y.left,\n  axis.ticks.length.y.right,\n  axis.line,\n  axis.line.x,\n  axis.line.x.top,\n  axis.line.x.bottom,\n  axis.line.y,\n  axis.line.y.left,\n  axis.line.y.right,\n  legend.background,\n  legend.margin,\n  legend.spacing,\n  legend.spacing.x,\n  legend.spacing.y,\n  legend.key,\n  legend.key.size,\n  legend.key.height,\n  legend.key.width,\n  legend.text,\n  legend.text.align,\n  legend.title,\n  legend.title.align,\n  legend.position,\n  legend.direction,\n  legend.justification,\n  legend.box,\n  legend.box.just,\n  legend.box.margin,\n  legend.box.background,\n  legend.box.spacing,\n  panel.background,\n  panel.border,\n  panel.spacing,\n  panel.spacing.x,\n  panel.spacing.y,\n  panel.grid,\n  panel.grid.major,\n  panel.grid.minor,\n  panel.grid.major.x,\n  panel.grid.major.y,\n  panel.grid.minor.x,\n  panel.grid.minor.y,\n  panel.ontop,\n  plot.background,\n  plot.title,\n  plot.title.position,\n  plot.subtitle,\n  plot.caption,\n  plot.caption.position,\n  plot.tag,\n  plot.tag.position,\n  plot.margin,\n  strip.background,\n  strip.background.x,\n  strip.background.y,\n  strip.clip,\n  strip.placement,\n  strip.text,\n  strip.text.x,\n  strip.text.x.bottom,\n  strip.text.x.top,\n  strip.text.y,\n  strip.text.y.left,\n  strip.text.y.right,\n  strip.switch.pad.grid,\n  strip.switch.pad.wrap\n\nIt’s not likely that we will encounter all these theme elements in this book. But, the ones we do use, we will use heavily. For example, I prefer to design all of my data visualizations without the “axis ticks” (those small lines sticking out from the plot just above, or beside, each yardage number).\n\n\n\n\n\n\nTip\n\n\n\nPlease note the keywords in the above paragraph: “I prefer.”\nNearly all the work you conduct within the theme() of your data visualizations are just that - your preference. I very much have a “personal preference” that unites all the data viz work that I do and share for public consumption.\nYou can feel free to follow along with my preferences, including the use of the upcoming nfl_analytics_theme() I will provide, or to make slight (or major!) adjustments to everything we cover in the coming section to make it fit your artistic vision.\nBe creative and do not be afraid to experiment with all the options available to you in theme().\n\n\nLet’s start by removing the ticks on both the x- and y-axis. To do so, we will add the theme() argument at the end of your prior ggplot code and then start building out each and every change we want to make from the above list of options.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\") +\n  theme(\n    axis.ticks = element_blank())\n\n\n\n\n\n\n\nWithin the theme() argument, as mentioned, we simply take the specific element we wish to change (from the above list of possibilities from the ggplot2 website) and then provide the instruction on what to do. In this case, since we wish to completely remove the axis.ticks from the entire plot, we can provide element_blank() which removes them. We can continue making changes to the plot’s elements by adding all of these preferences to out theme().\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.title = element_text(family = \"Roboto Condensed\",\n                              size = 10, \n                              color = \"black\"),\n    axis.text = element_text(family = \"Roboto Condensed\",\n                             face = \"bold\",\n                             size = 10,\n                             color = \"black\"),\n    plot.title.position = \"plot\",\n    plot.title = element_text(family = \"Roboto Condensed\",\n                              size = 16,\n                              face = \"bold\",\n                              color = \"#E31837\",\n                              vjust = .02,\n                              hjust = 0.5),\n    plot.subtitle = element_text(family = \"Roboto Condensed\",\n                                 size = 12,\n                                 color = \"black\",\n                                 hjust = 0.5),\n    plot.caption = element_text(family = \"Roboto Condensed\",\n                                size = 8,\n                                face = \"italic\",\n                                color = \"black\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major =  element_line(color = \"#d0d0d0\"),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    plot.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_blank())\n\n\n\n\n\n\n\nThere is a lot going on now in our theme() argument. Importantly, you may notice that we used element_blank() to remove the axis tick marks, but then switched to using element_text() for the reminder of the edits with our theme(). Both are one of four theme elements that can be modified in the above fashion.\n\n\n\n\n\n\nTip\n\n\n\nThe Four Theme Elements You Can Edit\nWhen working on editing your plot to your liking, you can make changes to one of four theme elements:\n\n\nelement_blank() - this used to entirely remove an element from the plot (like we did with axis.ticks.\n\nelement_rect() - this is used to make changes to the borders and backgrounds of a plot.\n\nelement_lines() - this is used to make changes to any element in the plot that is a line.\n\nelement_text() - this is used to make change to any element in the plot that is text.\n\n\n\nWe first made changes to the text of the title associated with the x- and y-axis and made edits to the text of the numeric values for each yardage distance. This process is started by using axis.title and axis.text in conjunction with element_text(), since that is the specific element type we are wishing to edit. Within our element_text(), we provided the numerous argument on how we wished to edit the text of both by providing the family (or the font), the face, the size, and the color.\nAfter, we got a little fancy in our edits to our plots title and subtitle. I knew that I wanted to center both directly in the middle of the plot. Rather than figuring out the specific horizontal adjustment needed, I used the plot.title.position() argument and set it to \"plot\", which used the entire width of our plot as the reference point for where to center the plot title and subtitle.\nTo take advantage of this, we followed by using the plot.title() argument to set the title’s horizontal adjustment to 0.5 (hjust = 0.5). As you may guess, the inclusion of 0.5 instructs the output to center the title (and the subtitle in the ensuing edit) directly over the middle of the plot (as calculated through our prior use of plot.title.position().\nOur next significant changes occurred by changing the aesthetics of the plot’s grid lines, background, and border. Because we are working with either line or background elements, we switch from element_text() and begin to use either element_line() or element_rect() (as well as again using element_blank() to completely remove the panel’s minor grid lines).\n\n\n\n\n\n\nTip\n\n\n\nIn a plot, which are the minor grid lines and which are the major?\nIn a ggplot2 plot, minor grid lines are those lines that hit either the x- or y-axis between the continuous or discrete values. Conversely, major grid lines are the lines that hit the axis at the same spot as the data values.\nIn the case of the current plot, our major grid lines are those that hit the x-axis at 500, 1,000, 1,500, and so on and hit the y-axis at 400, 600, 800, etc. The minor grid lines met the axis between the major grid lines.\n\n\nAfter removing the panel’s minor grid lines (again, a personal preference of mine), we also change the color of the panel’s major grid lines, then change the color of the plot’s background (both using element_rect()). The end result is a aesthetically pleasing data visualization."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-your-own-ggplot2-theme",
    "href": "04-nfl-analytics-visualization.html#creating-your-own-ggplot2-theme",
    "title": "\n5  Data Visualization with NFL Analytics\n",
    "section": "\n5.6 Creating Your Own ggplot2 Theme",
    "text": "5.6 Creating Your Own ggplot2 Theme\nAs mentioned, I have a distinctive “brand and look” for the data visualizations I create that make use of the same design elements and choices. Rather than copy and paste those into each and every ggplot piece of code I write, I’ve opted to consolidate all the theme() element changes into my own theme() function. In much the same way, I’ve created a “quick and easy” theme for use in this book. To get started, running the following chunk of code will create a function titled nfl_analytics_theme and place it into your RStudio environment.\n\nnfl_analytics_theme <- function(..., base_size = 12) {\n  \n    theme(\n      text = element_text(family = \"Roboto Condensed\", size = base_size),\n      axis.ticks = element_blank(),\n      axis.title = element_text(color = \"black\",\n                                face = \"bold\"),\n      axis.text = element_text(color = \"black\",\n                               face = \"bold\"),\n      plot.title.position = \"plot\",\n      plot.title = element_text(size = 16,\n                                face = \"bold\",\n                                color = \"black\",\n                                vjust = .02,\n                                hjust = 0.5),\n      plot.subtitle = element_text(color = \"black\",\n                                   hjust = 0.5),\n      plot.caption = element_text(size = 8,\n                                  face = \"italic\",\n                                  color = \"black\"),\n      panel.grid.minor = element_blank(),\n      panel.grid.major =  element_line(color = \"#d0d0d0\"),\n      panel.background = element_rect(fill = \"#f7f7f7\"),\n      plot.background = element_rect(fill = \"#f7f7f7\"),\n      panel.border = element_blank())\n}\n\nYou may notice that the elements in the above theme creation are quite similar to the ones we passed into our previous plot. And that is true, but the results will be nearly 99.9% identical. After wrapping our theme() inside a function, indicated by the opening and closing curly brackets { }, we can provide our desired theme element appearances as we would within a regular ggplot2 code block.\nHowever, in the above theme function, we have streamlined the basis a bit by indicating a base_size of all the text, when means all text output will be in size 12 font unless specifically indicated in the element (for example, we have the plot.title set to have a size of 16). As well, the same process was done for font (Roboto Condensed) so there was not a need to type it repeatedly into every element.\nBased on the above example, you are free to create as detailed a theme function as you desire. The beauty of creating your own theme like above is you will no longer need to edit each portion of the every plot element. Rather, you can simply call in your theme after you run it and it resides in under the “Functions” in your RStudio environment.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\") +\n  nfl_analytics_theme()\n\n\n\n\n\n\n\nAs you can see, we just consolidated 28 lines of code into a single line of code by wrapping all our theme elements into an easy to construct function.\nUnfortunately, you will notice that the resulting plot does not output with the title (“Cumulative Passing Yards”) in Kansas City red like in our original. This is because, in our nfl_analytics_theme() function, the color for axis.title() is set to \"black\". Thankfully, we can make this small edit within our ggplot code to switch the title back to Kansas City red, highlighting the idea that - despite the theme being built into a function - we still have the ability to make necessary edits on the fly without including all 28 lines of code. With the nfl_analytics_theme() active, we can still add additional theme elements as needed to make modifications, as seen below.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto Condensed\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\") +\n  nfl_analytics_theme() +\n  theme(plot.title = element_markdown(color = \"#E31837\"))"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#using-team-logos-in-plots",
    "href": "04-nfl-analytics-visualization.html#using-team-logos-in-plots",
    "title": "\n5  Data Visualization with NFL Analytics\n",
    "section": "\n5.7 Using Team Logos in Plots",
    "text": "5.7 Using Team Logos in Plots\nTo explore the process of placing team logos into a plot, let’s stick with our previous example of working with play action passing data but explore it at the team level rather than individual quarterbacks. To gather the data, we can use vroom to read it in from the book’s GitHub.\n\nteam_playaction <- vroom::vroom(\"https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/team_playaction.csv\")\n\nteam_playaction\n\nThe resulting team_playaction data contains nearly identical information to the previous QB-level data. However, there is a slight change in how Sports Info Solutions charts passing yards on the team level. You will notice a column for both net_yds and gross_yds. When charting passing attempts in football, a player’s individual passing yards are aggregated under gross yards with all lost yardage resulting from a sack being included. On the other hand, team passing yards are always presented in net yards and any lost yardage from sacks are not included. Case in point, the gross_yds number in our team_playaction data is greater than the net_yds for all 32 NFL teams. In any case, we will build our data visualization using net_yds.\nIn order to include team logos in the plot, we must first merge in team logo information (again with the understanding that we could use nflplotR if team abbreviations were included in the data). We can collect the team logo information using the load_teams() function within nflreadR. There are three different variations of each team’s logo available in the resulting data: (1.) the logo from ESPN, (2.) the logo from Wikipedia, (3.) a pre-edited version of the logo that is cropped into a square.\n\n\n\n\n\n\nNote\n\n\n\nThere are slight differences in disk space, pixels, and utility in the provided ESPN, Wikipedia, and squared versions of the logos.\nThe Wikipedia versions are, generally, smaller in size. The Arizona Cardinals logo, for example, is just 9.11 KB in size from Wikipedia while the ESPN version of the logo is 20.6 KB. This difference is disk size is the result of each images dimensions and pixels. The ESPN version, with the larger disk size, is also a higher quality image that is scaled in 500x500 dimensions (and 500 pixels). The Wikipedia version is scaled in 179x161 dimensions at 179 pixels and 161 pixels, respectively. The squared version of the logo is a 200x200 image at 200 pixels with a background matching the team’s primary color.\nWhat does this mean? The ESPN version of the logo is better for those applications where the logo will be large and you do not want any loss of quality. The Wikipedia version, conversely, is better suited for applications like our: for use in a small-scale data visualization. We do not plan on “blowing up” the image, thus losing quality and the smaller disk space size of the images allows for a slightly quicker rendering time when we use the ggimage package. While sparingly used, the squared version of the logo can be used in certain data visualization applications that requires the team logo to quickly and easily “merge” into the background of the plot (more on this later in this chapter, though).\n\n\nBecause of the above explanation, we can collect just the team nicknames and the Wikipedia version of the logo to merge into our existing team_playaction data.\n\nteams <- nflreadr::load_teams(current = TRUE) %>%\n  select(team_nick, team_logo_wikipedia)\n\nteam_playaction <- team_playaction %>%\n  left_join(teams, by = c(\"team\" = \"team_nick\"))\n\nteam_playaction\n\nWith the data now containing the correct information, we can build a basic version of our data visualization using the same geom_point as above, and continue to configure the information on both the x- and y-axis, to verify that everything is working correctly before switching out geom_point for geom_image in order to bring the team logos into the plot.\n\nggplot(data = team_playaction, aes(x = net_yds, y = pa_net_yds)) +\n  geom_hline(yintercept = mean(team_playaction$pa_net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_vline(xintercept = mean(team_playaction$net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_point() +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(title = \"Net Passing Yards Without Play Action vs. With Play Action\",\n       subtitle = \"2022 NFL Regular Season\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\",\n       x = \"Net Yards Without Play Action\",\n       y = \"Net Yards With Play Action\") +\n  nfl_analytics_theme()\n\n\n\n\n\n\n\nThe resulting plot, constructed in nearly an identical manner to our above example, looks correct and we are ready to swap out geom_point for team logos.\n\n\n\n\n\n\nImportant\n\n\n\nBefore proceeding with this next step, be sure that you have the ggimage package installed and loaded.\n\n\n\nggplot(data = team_playaction, aes(x = net_yds, y = pa_net_yds)) +\n  geom_hline(yintercept = mean(team_playaction$pa_net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_vline(xintercept = mean(team_playaction$net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_image(aes(image = team_logo_wikipedia)) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(title = \"Net Passing Yards Without Play Action vs. With Play Action\",\n       subtitle = \"2022 NFL Regular Season\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\",\n       x = \"Net Yards Without Play Action\",\n       y = \"Net Yards With Play Action\") +\n  nfl_analytics_theme()\n\n\n\n\n\n\n\nBy using the geom_image function, we are able to wrap the image argument within an aesthetics call (that is, aes()) and then stipulate that the team_logo_wikipedia variable is to serve as the image associated with each data point.\nBut, wait: the image looks horrible, right? Indeed, the logos are pixelated, are skewed in shape, and are just generally unpleasant to look at.\nThat is because we failed to provide an aspect ratio for the team logos. In this case, we need to add asp = 16/9.\n\n\n\n\n\n\nNote\n\n\n\nWhy are we including a specific aspect ratio of 16/9 for each team logo? Good question.\nAn aspect ratio of 16/9 refers to the proportional relationship between the width and height of a rectangular display or image. In this case, the width of the image is 16 units, and the height is 9 units. Importantly, this aspect ratio is commonly used for widescreen displays, including most (if not all) modern televisions, computer monitors, and smartphones.\nAs well, the 16/9 aspect ratio is sometimes referred to as 1.78:1, which means that the width is 1.78 times the height of the image. This aspect ratio is wider than the traditional 4:3 aspect ratio that was common used in older television and CRT-based computer monitors.\n\n\nWe can make the quick adjustment in our prior code to provide the correct aspect ratio for each of our team logos:\n\nggplot(data = team_playaction, aes(x = net_yds, y = pa_net_yds)) +\n  geom_hline(yintercept = mean(team_playaction$pa_net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_vline(xintercept = mean(team_playaction$net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_image(aes(image = team_logo_wikipedia), asp = 16/9) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(title = \"Net Passing Yards Without Play Action vs. With Play Action\",\n       subtitle = \"2022 NFL Regular Season\",\n       caption = \"An Introduction to NFL Analytics with R - Brad J. Congelio\",\n       x = \"Net Yards Without Play Action\",\n       y = \"Net Yards With Play Action\") +\n  nfl_analytics_theme()\n\n\n\n\n\n\n\n\n\n\n\nStikeleather, Jim. 2013. “The Three Elements of Successful Data Visualizations.” https://hbr.org/2013/04/the-three-elements-of-successf."
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#introduction-to-statistics-and-modeling-with-nfl-data",
    "href": "05-nfl-analytics-advanced-methods.html#introduction-to-statistics-and-modeling-with-nfl-data",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.1 Introduction to Statistics and Modeling with NFL Data",
    "text": "6.1 Introduction to Statistics and Modeling with NFL Data"
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#basic-statistical-modeling",
    "href": "05-nfl-analytics-advanced-methods.html#basic-statistical-modeling",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.2 Basic Statistical Modeling",
    "text": "6.2 Basic Statistical Modeling"
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#deciphering-statistical-results",
    "href": "05-nfl-analytics-advanced-methods.html#deciphering-statistical-results",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.3 Deciphering Statistical Results",
    "text": "6.3 Deciphering Statistical Results\ninformation here will be provided to assist in understanding what the results of the models mean and will serve as a reference point for the rest of the chapter.\n\n6.3.1 Understanding Linear Regression Results\nUsing the below example that explores the relationship between an NFL’s teams total yards and total points over the course of a season, the results of a linear regression model calculated using the lm() function includes the following information (explained afterward):\n\n`Residuals:\n    Min      1Q  Median      3Q     Max \n-71.443 -22.334   1.157  19.145  68.080 \n\nCoefficients:\n              Estimate Std. Error t value      Pr(>|t|)    \n(Intercept) -225.03520   65.44927  -3.438       0.00174 ** \ntotal_yards    0.10341    0.01132   9.135 0.00000000036 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 32.04 on 30 degrees of freedom\nMultiple R-squared:  0.7356,    Adjusted R-squared:  0.7268 \nF-statistic: 83.45 on 1 and 30 DF,  p-value: 0.0000000003599`\n\n\n6.3.1.1 Residuals\nA linear model’s residuals are the calculated difference between actual values of the dependent values as found in the data used to build the model and those values predicted by the regression model. In a perfect uniform relationship, all of the values from a dataset would sit perfectly on top of the “line of best fit.” Take the below graph, for example.\n\n\n\n\n\n\n\n\nIn this example model, our regression model was able to successfully capture the entirety of the relationship between Our Predictor Variable on the x-axis and the Our Response Variable on the y-axis. This means that the model leaves no unexplained or undetermined variance between the variables and, because of this, the model can take additional unused data to predict, with 100% accuracy, the resulting value of the dependent variable. It is exceedingly rare to have real-world data be perfectly situated on the line of best fit. In fact, it is more often than not a sign of “overfitting,” which occurs when the model successfully discovers the “random noise” in the data. In the majority of cases, a regression model with a perfect line of fit will perform exceedingly poorly when introduced to unseen data.\nA regression model that is not “overfitted” will have data points that do not fit on the line of best fit, but fall over and under it. The results of the regression model uses a simple formula - residual = observed_value - predicted_value to help us interpret the difference between those actual and estimated values.\nThe information produced in the example lm() summary above includes statistical information about the distribution of the model’s residual errors.\n\n\n\n\n\n\nSummary Distribution\nMeaning\n\n\n\nThe Min Distribution\nThe Min distribution provides the smallest difference between the actual values of the model’s predictor variable (total points) and the predicted. In the example summary, the minimum residual is -71.443 which means that the lm() model predicted that one specific team scored 71.443 more points than it actually did.\n\n\n\nThe 1Q Distribution\nThe 1Q distribution is based on the first quartile of the data (or where the first 25% of the model’s residual fall on the line of best fit). The 1Q residual is -22.334, which means the lm() model predicted that 25% of the teams scored 22.334 more points than the actual values.\n\n\n\nThe Median Distribution\nThe Median distribution, much like the 1Q distribution is data from the first quartile, is the residuals from the 50th percentile of the data. The Median residual in the above summary is 1.157, which means that the lm() model - for 50% of teams - either overestimated or underestimated a teams total points by less than 1.157 points.\n\n\n\nThe 3Q Distribution\nCovering the third quartile of the residuals, the 3Q Distribution is 19.145 which means that 75% of the NFL teams in the data had a total points prediction either overestimated or underestimated by less than 19.145 points.\n\n\n\nThe Max Distribution\nThe opposite of the Min distribution, the Max distribution is the largest difference between the model’s observed and predicted values for a team’s total points. In this case, for one specific team, the model predicted the team scored 68.080 points less than the actual value.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA model’s residuals allow you to quickly get a broad view of how accurately it is predicting the data. Ideally, a well-performing model will return residuals that are small and distributed around zero in a consistent fashion. Residuals that are both small and evenly dispersed around zero are the first sign that the model is making predictions that are close to the actual value in the data and is avoiding both over- and underestimating.\nBut this is not always the case.\nFor example, we can compare our above example’s residuals to the below residual output produced by manually creating a dataset.\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-13.7015  -2.1831  -0.5963   0.0000   4.1594  10.5359 \n\n\nCompared to the residuals from the above NFL data, the residuals from the randomly created data are small in comparison and are more evenly distributed around zero. Given that, it is likely that the linear model is doing a good job at making predictions that are close to the actual value.\nBut that does not mean the residuals from the NFL data indicate a flawed and unreliable model. It needs to be noted that the “goodness” of any specific linear regression model is wholly dependent on both the context and the specific problem that the model is attempting to predict. To that end, it is also a matter of trusting your subject matter expertise on matters regarding the NFL.\nThere could be any number of reasons that can explain why the residuals from the regression model are large and not evenly distributed from zero. For example:\n\n\nRed-zone efficiency: a team that moves the ball downfield with ease, but then struggles to score points once inside the 20-yardline , will accumulate total_yards but failed to produce total_points in the way the model predicted.\n\nTurnovers: Similar to above, a team may rack up total_yards but ultimately continue to turn the ball over prior to being able to score.\n\nDefensive scoring: a score by a team’s defense, in this model, still counts towards total_points but does not count towards total_yards.\n\nStrength of Opponent: At the end of the 2022 season, the Philadelphia Eagles and the New York Jets both allowed just 4.8 yards per play. The model’s predicted values of the other three teams in each respective division (NFC East and AFC East) could be incorrect because information, for example, the opponent’s strength of defense was not included in the model.\n\nAll that to say: residuals are a first glance at the results of the data and provide a broad generalization of how the model performed without taking outside contextual factors into consideration.\n\n\n\n6.3.1.2 Coefficients\nffff\n\n6.3.1.3 Simple Linear Regression\nA simple linear regression is a fundamental statistical technique that is used to explore the relationship between two variables, specifically the dependent variable (also called the “response variable”) and the independent variable (also called the “predictor”). By using a simple linear regression, we can model the relationship between the two variables as a linear equation that best fits the observed data points.\nA simple linear regression aims to fit a straight line through all the observed data points in such a way that the total squared distance between the actual observations and the values predicted by the model are minimal. This line is often referred to as either the “line of best fit” or the “regression line” and it represents the interaction between the dependent and independent variables. Mathematically, the equation for a simple linear regression is as follows:\n\\[\nY = {\\beta}_0 + {\\beta}_1 * X + \\epsilon\n\\]\n\n\n\\(Y\\), in the above equation, is the dependent variable where the \\(X\\) represents the independent variable.\n\n\\({\\beta}_o\\) is the intercept of the regression model.\n\n\\({\\beta}_1\\) is the slope of the model’s “line of best fit.”\n\n\\(\\epsilon\\) represents the error term.\n\nTo better illustrate this, let’s use basic football terms using the above regression equation to compare a team’s offensive points scored in a season based on how many offensive yards it accumulated. The intercept (\\({\\beta}_o\\)) represents the value when a team’s points scored and offensive yards are both zero. The slope (\\({\\beta}_1\\)) represents the rate of change in \\(Y\\) as the unit of \\(X\\) changes. The error term (\\(\\epsilon\\)) is represents the difference between the actual observed values of the regression’s dependent variable and the value as predicted by the model.\nUsing our points scored/total yards example, a team’s total yards gained is the independent variable and total points scored is the dependent variable, as a team’s total yardage is what drives the change in total points (in other words, a team’s total points is dependent on its total yardage). A team will not score points in the NFL if it is not also gaining yardage. We can view this relationship by building a simple linear regression model in R using the lm() function.\n\n\n\n\n\n\nNote\n\n\n\nThe lm() function is a built-in function in RStudio that stands for “linear model” and is used, as described above, to fit a linear regression to the data that you provide. The completed regression estimates the coefficients of the regression, and also includes both the intercept and slope, which are the main factors in explaining the relationship between your data’s response and predictor variables.\nThe lm() function requires just two arguments in order to provide results: a formula and the dataframe to use and is structured like so: model_results <- lm(formula, data).\nThe formula argument require that you specify both the response and predictor variables, as named in your dataframe, in the structure of Y ~ X(wherein Y is the response variables and X is the predictor). In the case that you have more than one predictor variable, the + is used to add to the formula (lm(Y ~ X1 + X2).\nThe lm() function returns the coefficients, residuals, and other statistics of the model in a lm data object. There are There are numerous ways to access this data which are discussed in further detail below.\n\n\nLet’s build a simple linear regression model that explores the relationship between the total yardage earned by a team over the course of a season and the number of points scored. To begin, place the prepared data in the simple_regression_data dataframe by running the code below.\n\nsimple_regression_data <- vroom::vroom(\"https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/simple_regression_data.csv\")\n\nsimple_regression_data\n\nThe data contains the total yardage and points scored for each NFL team between 2012 and 2022. The data does not include any playoff games. Before running a model on all ten years of data, we will begin by selecting just information from 2022 and then build our lm() model. Beforehand, though, we can take the raw values from total_yards and total_points and view the expected line of best fit.\n\nregression_2022 <- simple_regression_data %>%\n  filter(season == 2022)\n\nteams <- nflreadr::load_teams(current = TRUE)\n\nregression_2022 <- regression_2022 %>%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\n  ggplot(regression_2022, aes(x = total_yards, y = total_points)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", linetype = \"dashed\", size = .8) +\n  geom_image(aes(image = team_logo_wikipedia), asp = 16/9) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  labs(title = \"**Line of Best Fit: 2022 Season**\",\n       subtitle = \"*Y = total_yards ~ total_points*\",\n       caption = \"*An Introduction to NFL Analytic with R*<br>Brad J. Congelio\") +\n  xlab(\"Total Yards\") +\n  ylab(\"Total Points\") +\n  nfl_analytics_theme()\n\n\n\n\n\n\n\nThe plot shows that based on a simple regression between total_yards and total_points that several teams - like the Titans, Giants, Packers, Raiders, Jaguars, and the Chiefs - are fitted nearly perfectly with the regression line. Other teams, however, such as the Buccaneers and the Cowboys are well off this line of best fit. To further examine this relationship, we can pass the data into a proper simple linear regression model and start exploring the summary statistics.\n\nresults_2022 <- lm(total_points ~ total_yards, data = regression_2022)\n\nUsing the lm() function, the \\(Y\\) variable (the dependent) is total_yards and the \\(X\\) variable (the predictor) is entered as total_yards with the argument that the data is coming from the regression_2022 dataframe stored in the RStudio environment. We can view the results of the regression model by using the summary() function.\n\nsummary(results_2022)\n\n\nCall:\nlm(formula = total_points ~ total_yards, data = regression_2022)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-71.443 -22.334   1.157  19.145  68.080 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -225.03520   65.44927  -3.438  0.00174 ** \ntotal_yards    0.10341    0.01132   9.135  3.6e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32.04 on 30 degrees of freedom\nMultiple R-squared:  0.7356,    Adjusted R-squared:  0.7268 \nF-statistic: 83.45 on 1 and 30 DF,  p-value: 3.599e-10\n\n\nWhile residuals are not the only summary statistics to examine in the results, they do provide a broad way to understand how the linear model performed. In this case, the residuals have a wide spread and an inconsistent deviation from zero. While the median residual value is the closest to zero at 1.157, it is still a bit too high to safely conclude that the model is making predictions that adequately reflect the actual values. Moreover, both tail ends of the residual values (Min and Max) are a large negative and positive number, respectively, which is a possible indication that both over- and underestimating a team’s total_points by statistically significant amount.\nHowever, as mentioned in this chapter’s explanation of how to interpret residuals from a model’s summary statistics, the widespread and deviation from zero in the results is likely the result of numerous factors outside the model’s purview that occur in any one NFL game. To get a better idea of what the residual values represent, we can plot the data and include NFL team logos.\n\nregression_2022$residuals <- residuals(results_2022)\n\nggplot(regression_2022, aes(x = total_yards, y = residuals)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = .7) +\n  stat_fit_residuals(size = 0.01) +\n  stat_fit_deviations(size = 1.75, color = regression_2022$team_color) +\n  geom_image(aes(image = team_logo_wikipedia), asp = 16/9, size = .0325) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +\n  labs(title = \"**Total Yards & Residual Values**\",\n       subtitle = \"*Y = total_points ~ total_yards*\",\n       caption = \"*An Introduction to NFL Analytic with R*<br>Brad J. Congelio\") +\n  xlab(\"Total Yards\") +\n  ylab(\"Residual of Total Points\") +\n  nfl_analytics_theme() +\n  theme(panel.grid.minor.y = element_line(color = \"#d0d0d0\"))\n\n\n\n\n\n\n\nWith the data visualized, it is clear that the model’s Min distribution of -71.44 is associated with the Tampa Bay Buccaneers, while the Max distribution of 68.08 is the prediction for the total points earned by the Dallas Cowboys. Because a negative residual means that the model’s predicted value is too high, and a positive residual means it was too low, we can conclude that the Buccaneers actually scored 71.4 points less than the the results of the model, while the Cowboys scored 68.08 more than predicted.\n\n`Coefficients:\n              Estimate Std. Error t value      Pr(>|t|)    \n(Intercept) -225.03520   65.44927  -3.438       0.00174 ** \ntotal_yards    0.10341    0.01132   9.135 0.00000000036 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1`˜.â€™ 0.1 â€˜ â€™ 1`\n\nThe (Intercept) of the model, or where the regression line crosses the y-axis, is -225.0350. When working with NFL data, it of course does not make sense that the (Intercept) is negative. Given the model is built on a team’s total yards and total points, it seems intuitive that the regression line would cross the y-axis at the point of (0,0) as an NFL team not gaining any yards is highly unlike to score any points.\nIt is important to remember that the linear model attempts to position the regression line to come as close to all the individual points as possible. Because of this, it is not uncommon for regression line to not cross exactly where the x-axis and y-axis meet. Again, contextual factors of an NFL game are not account for in the model’s data: strength of the opponent’s defense, the quality of special teams play, defensive turnovers and/or touchdowns, field position, etc. can all impact a team’s ability to score points without gaining any yardage. The lack of this information in the data ultimately impact the positioning of the line of best fit.\nThe total_yards coefficient represents the slope of the model’s regression line. It is this slope that represents how a team’s total points are predicted to change with every additional gain of one yard. In this example, the total_yards coefficient is 0.10341 - so for every additional yard gained by a team, it is expected to add 0.10341 points to the team’s cumulative amount.\nThe Std. Error summary statistic provides guidance on the accuracy of the other estimated coefficients. The Std. Error for the model’s (Intercept) is quite large at 65.44927. Given the ability to resample the data from NFL terms numerous times and then allowing the linear model to predict again, this specific Std. Error argues that the regression line will cross the y-axis with 65.44972 of -225.03520 in either direction. Under normal circumstances, a large Std. Error for the (Intercept) would cause concern about the validity of the regression line’s crossing point. However, given the nature of this data - an NFL team cannot score negative points - we should not have any significant concern about the large Std. Error summary statistic for the (Intercept).\nAt 0.01132, the Std. Error for the total_yards coefficient is small and indicates that the Estimate of total_yards - that is, the increase in points per every yard gained - is quite accurate. Given repeated re-estimating of the data, the relationship between total_yards and total_points would vary by just 0.01132, either positively or negatively.\nWith a t-value of 9.135, the total_yards coefficient has a significant relationship with total_points. The A value of -3.438 indicates that the (Intercept) is statistically different from 0 but we should still largely ignore this relationship given the nature of the data.\nThe model’s Pr(>|t|) value of highly significant for total_yards and is still quite strong for the (Intercept). The value of 0.00000000036 indicates an incredibly significant relationship between total_yards and total_points.\nThe linear model’s Residual Standard Error is 32.04, which means that the average predicted values of total_points are 32.04 points different from the actual values in the data. The linear model was able to explain 73.56% of the variance between total_yards and total_points based on the multiple R-squared value of 0.7356. Additionally, the Adjusted R-squared value of 0.7268 is nearly identical to the multiple R2, which is a sign that the linear model is not overfitting (in this case because of the simplicity of the data). The model’s F-Statistic of 83.45 indicates a overall significance to the data, which is backed up by an extremely strong p-value.\nBased on the summary statistics, the linear model did an extremely good job at capturing the relationship between a team’s total_yards and total_points. However, with residuals ranging from -71.443 to 68.080, it is likely that the model can be improved upon by adding additional information and statistics. However, before providing additional metrics, we can try to improve the model’s predictions by including all of the data (rather than just the 2022 season). By including 20-seasons worth of total_yards and total_points, we are increasing the sample size which, in theory, allows for a reduced impact of any outliers and an improve generalizability.\n\n\n\n\n\n\nImportant\n\n\n\nWorking with 10+ years of play-by-play data can be problematic in that them model, using just total_yards and total_points, is not aware of changes in the overall style of play NFL. The balance between rushing and passing has shifted, there’s been a philosophical shift in the coaching ranks in “going for it” on 4th down, etc. A simple linear regression cannot account for how these shifts impact the data on a season-by-season basis.\n\n\nThe results from including the total_points and total_yards for each NFL team from 2012-2022 show an improvement of the model, specifically with the residual values.\n\nregression_all_seasons <- simple_regression_data %>%\n  select(-season)\n\nall_season_results <- lm(total_points ~ total_yards, data = regression_all_seasons)\n\nsummary(all_season_results)\n\nWithout further testing, the residual values after including 20-seasons worth of data are a bit better. The Median is -1.26 which is slightly higher than just one season (M = 1.16). The 1Q and 3Q distributions are both approximately symmetric around the model’s M value compared to just the 2022 season regression that results in a deviation between 1Q and 3Q (-22.33 and 19.15, respectively). The Min and Max values of the new model still indicate longtail cases on both ends of the regression line much like the 2022 model found.\n\n\n\n\n\n\nTip\n\n\n\nTo further examine the residual values, we can use a Shapiro-Wilk Test to test the whether results are normally distributed.\nThe Shapiro-Wilk Test provides two values with the output: the test statistic (provided as a W score) and the model’s p-value. Scores for W can range between 0 and 1, where results closer to 1 meaning the residuals are in a normal distribution. The p-value is used make decision on the null hypothesis (that there is enough evidence to conclude that there is uneven distribution). In most cases, if the p-value is larger than the regression’s level of significance (typically 0.05), than you may reject the null hypothesis.\nWe can run the Shapiro-Wilk Test on our 2012-2022 data using the shapiro.test function that is part of the stats package in R.\n\nresults_2012_2020 <- residuals(all_season_results)\n\nshapiro_test_result <- shapiro.test(results_2012_2020)\n\nshapiro_test_result\n\n\n    Shapiro-Wilk normality test\n\ndata:  results_2012_2020\nW = 0.99704, p-value = 0.7737\n\n\nThe W score for the residual is 1, meaning a very strong indication that the data in our model is part of a normal distribution. The p-value is 0.8, which is much large than the regression’s level of significance (0.05). As a result, we can reject the null hypothesis and again conclude that the data is in a normal distribution.\n\n\nUsing the Shapiro-Wilk Test to confirm the normal distribution of the data is help because, if plotted, the model that covers 20-seasons worth of data is harder to visually interpret than just one season as the plot increases from just 32 data points to 640, with many overlapping.\n\nteams <- nflreadr::load_teams(current = TRUE)\n\nregression_all_seasons <- left_join(regression_all_seasons, teams, by = c(\"team\" = \"team_abbr\"))\n\nregression_all_seasons$residuals <- residuals(all_season_results)\n\nggplot(regression_all_seasons, aes(x = total_yards, y = residuals)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = .7) +\n  stat_fit_residuals(size = 2, color = regression_all_seasons$team_color) +\n  stat_fit_deviations(size = 1, color = regression_all_seasons$team_color, alpha = 0.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +\n  labs(title = \"**Total Yards & Residual Values: 2012-2022**\",\n       subtitle = \"*Y = total_points ~ total_yards*\",\n       caption = \"*An Introduction to NFL Analytic with R*<br>Brad J. Congelio\") +\n  xlab(\"Total Yards\") +\n  ylab(\"Residual of Total Points\") +\n  nfl_analytics_theme() +\n  theme(panel.grid.minor.y = element_line(color = \"#d0d0d0\"))\n\n\n\n\n\n\n\nWe can also compare the multiple R2 and adjusted R2 score between the two regression models.\n2012 - 2022 Data:\nMultiple R-squared:  0.683\nAdjusted R-squared:  0.682\n\n2022 Data\nMultiple R-squared:  0.736\nAdjusted R-squared:  0.727\nThe regression using just the 2022 data results in a slightly better multiple and adjusted R2 score compared to using data from the last twenty seasons of the NFL. While this does indicate that the model based on the single season is better at defining the relationship between a team’s total_yards and total_points it is essential to remember that there is different underlying patterns in the data as a result of the changing culture in the NFL and, ultimately, the epp and flow of team performance as a result of high levels of parity in the league.\nIn order to account for this “epp and flow” in both team performance and the changing culture/rules of the NFL, we need to turn to a multiple linear regression in include these additional factors as it is a model that is capable of better accounting for the nuances of NFL data.\n\n6.3.1.4 Multiple Linear Regression\nA multiple linear regression is extremely similar to a simple linear regression (both in design and how to create one in RStudio). The main difference, as discussed, is that a multiple linear regression allows for us to include additional predictor variables by using the + sign in the model’s formula. The inclusion fo these additional predictive variables, in theory, allow the model to compute the more complex relationships in NFL data and improve its final performance.\nWe will again create our first multiple linear regression with just data from the 2022 season that includes the same predictor (total_yards) and response variable (total_points). For additional predictors, we must consider what circumstances may lead a team to have high total_yardage but an amount of total_points that would fall below the model’s predicted value. We will include as additional predictors:\n\n\nRedzone Efficiency: provided as a percentage, this is a calculation of how many times a team drove into the red zone and scored. A higher percentage is better.\n\nRedzone Touchdown Efficiency: This is the same as redzone efficiency, but includes only the number of red zone trips divided by the total touchdowns scored from the redzone.\n\nRedzone Field Goal Efficiency: The same as redzone touchdown efficiency, but with field goals.\n\nCumulative Turnovers: The total number of turnovers during the regular season.\n\nDefensive Touchdowns: The number of touchdowns scored by each team’s defensive unit.\n\nSpecial Teams Touchdowns: The number of touchdowns scored by special teams (kick/punt returns).\n\nLet’s build the multiple regression model for only the 2022 season. You can read in the data using vroom::vroom.\n\nmultiple_lm_data <- vroom::vroom(\"https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/multiple_lm_data.csv\")\n\nmultiple_lm_data\n\nThe data for the multiple linear regression has the same four columns as the simple linear regression (season, team, total_points, and total_yards). After are the new additional predictors (rz_eff, rz_td_eff, rz_fg_eff, def_td, and spec_tds).\n\n\n\n\n\n\nDanger\n\n\n\nPlease note that, of the predictor and response variables, all of the values are in whole number format except for rz_eff, rz_td_eff, and rz_fg_eff. While it is not a problem to include predictors that are on differing scales (in this case, whole numbers and percentages), it may cause difficulty in interpreting the summary statistics. If this is the case, the issue can be resolved by using the scale() function to standardize all the data’s predictors against one another.\n\n\nThe construction of the multiple linear regression is the same process of the simple linear regression, with the inclusion of additional predictors to the formula using the + sign. We are applying a filter() to our multiple_lm_data to retrieve just the 2022 season to begin.\n\nmultiple_lm_2022 <- multiple_lm_data %>%\n  filter(season == 2022)\n\n\nlm_multiple_2022 <- lm(total_points ~ total_yards + rz_eff + rz_td_eff +\n                         rz_fg_eff + total_to + def_td + spec_tds, data = multiple_lm_2022)\n\nsummary(lm_multiple_2022)\n\n\nCall:\nlm(formula = total_points ~ total_yards + rz_eff + rz_td_eff + \n    rz_fg_eff + total_to + def_td + spec_tds, data = multiple_lm_2022)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.035 -15.584  -0.353  14.603  43.528 \n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -459.52248  128.55567  -3.575  0.00146 ** \ntotal_yards    0.09056    0.01129   8.020 2.25e-08 ***\nrz_eff       228.88531  113.56255   2.016  0.05472 .  \nrz_td_eff    167.23235   82.70971   2.022  0.05401 .  \nrz_fg_eff           NA         NA      NA       NA    \ntotal_to       0.40582    1.49339   0.272  0.78805    \ndef_td         4.45597    4.05728   1.098  0.28255    \nspec_tds       5.49769    7.86741   0.699  0.49113    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.94 on 25 degrees of freedom\nMultiple R-squared:  0.8442,    Adjusted R-squared:  0.8068 \nF-statistic: 22.57 on 6 and 25 DF,  p-value: 5.807e-09\n\n\nThe summary statistic residuals for the multiple linear regression are more evenly distributed towards the mean than our simple linear regression. Based on the residuals, we can conclude that - for 50% of the teams - the model either over or underestimated their total_points by just -0.35 (as listed in the Median residual). The interquartile range (within the 1Q and 3Q quartiles) are both close to the median and the Min and Max residuals both decreased significantly from our simple linear model, indicating a overall better line of fit.\nWe can confirm that the multiple linear regression resulted in an even distribution of the residuals by again using a Shapiro-Wilk’s Test.\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  results_lm_2022\nW = 0.98378, p-value = 0.8989\n\n\nThe results of the Shapiro-Wilk’s test (W = 1 and p-value = 0.9) confirm that residuals are indeed evenly distributed. A visualization showcases the model’s even distribution of the residuals.\n\n### gathering the fitted numbers and residual numbers\nmlm_2022_fitted <- predict(lm_multiple_2022)\nmlm_2022_residuals <- residuals(lm_multiple_2022)\n\n### binding them into a data frame\nplot_data_2022 <- data.frame(Fitted = mlm_2022_fitted, Residuals = mlm_2022_residuals)\n\n### c binding teams\nplot_data_2022 <- plot_data_2022 %>%\n  cbind(teams)\n\n### plotting\nggplot(plot_data_2022, aes(x = Fitted, y = Residuals)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = .7) +\n  stat_fit_deviations(size = 1.75, color = plot_data_2022$team_color) +\n  geom_image(aes(image = team_logo_espn), asp = 16/9, size = .0325) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  labs(title = \"**Multiple Linear Regression Model: 2022**\",\n       caption = \"*An Introduction to NFL Analytic with R*<br>Brad J. Congelio\") +\n  xlab(\"Fitted Values\") +\n  ylab(\"Residual Values\") +\n  nfl_analytics_theme() +\n  theme(panel.grid.minor.y = element_line(color = \"#d0d0d0\"))\n\n\n\n\n\n\n\nJust as the residual values in the summary statistics indicated, plotting the fitted_values against the residual_values shows an acceptable spread in the distribution, especially given the nature of NFL data. Despite positive results in the residual values, the summary statistics of the mulitple linear regression indicates a significant issue with the data. Within the Coefficients, it is explained that one of the items is “not defined because of singularities.”\n\n\n\n\n\n\nImportant\n\n\n\n“Singularities” occur in the data as a result of the dreaded multicollinearity between two or more predictors. The involved predictors were found to have a high amount of correlation between one another, meaning that one of the variables can be predicted in a near linear fashion with one or more of the other predictive variables. As a result, it is difficult for the regression model to correctly estimate the contribution of these dependent variables to the response variable.\nThe model’s Coefficients of our multiple linear regression shows NA values for the rz_fg_eff predictor (the percentage of times a team made a field goal in the red zone rather than a touchdown). This is because rz_fg_eff was one of the predictive variables strongly correlated with another but just that it was the one dropped by the regression model to avoid producing flawed statistics as a result of the multicollinearity.\nIf you are comfortable producing the lienar regression with rz_fg_eff being a dropped predictor, that are no issues with that. However, we can create a correlation plot that allows is to determine which predictors have high correlation values with others. Examining the issue allows us to determine if rz_fg_eff is, indeed, the predictive variable we want the regression to drop or if we’d rather, for example, drop rz_eff and keep just the split between touchdowns and field goals.\n\nregression_corr <- cor(multiple_lm_2022[, c(\"total_yards\", \"rz_eff\", \"rz_td_eff\",\n                                            \"rz_fg_eff\", \"total_to\", \"def_td\", \"spec_tds\")])\n\n### melting the regression_corr to prep for ggplot\nmelted_regression_corr <- melt(regression_corr)\n\n## plotting\nggplot(data = melted_regression_corr, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_distiller(palette = \"PuBu\", direction = -1, limits = c(-1, +1)) +\n  geom_text(aes(x = Var1, y = Var2, label = round(value, 2)), color = \"black\",\n            fontface = \"bold\", family = \"Roboto Condensed\", size = 5) +\n  labs(title = \"Multicollinearity Correlation Matrix\",\n       subtitle = \"Multiple Linear Regression: 2022 Data\",\n       caption = \"**An Introduction to NFL Analytics with R**<br>Brad J. Congelio\") +\n  nfl_analytics_theme() +\n  labs(fill = \"Correlation \\n Measure\", x = \"\", y = \"\") +\n  theme(legend.background = element_rect(fill = \"#F7F7F7\"),\n        legend.key = element_rect(fill = \"#F7F7F7\"))\n\n\n\n\n\n\n\nUsing a correlation plot allows for easy identification of those predictive variables that have high correlation with one another. The general rule is that two predictors become problematic in the regression model in the coefficient between the two is above 0.7 (0.8, given domain knowledge about the context of the data).\nIn our correlation plot, there are two squares in (indicated by the darkest blue color) that have a value greater than 0.7 (or -0.7 in this case, as both strong and negative correlations are capable of producing multicollinearity. The two squares happen to relate to the same relationship between the rz_fg_eff and rz_td_eff predictors.\nRecall that the regression model automatically removed the rz_fg_eff from the measured Coefficients. Given the context of the data, I am not sure that is the best decision. Given we are examining the relationship the predictive variables and total_points, removing the rz_fg_eff variable inherently erases a core source of points in a game of football.\nBecause of this - and since our rz_eff predictor accounts for both touchdowns and field goals - I believe we could move forward on rerunning the regression without either rz_fg_eff and rz_td_eff.\n\n\nTo run the multiple linear regression again, without the predictors relating to red zone touchdown and field efficiency, we will drop both from our multiple_lm_2022 dataframe, rerun the regression model, and then examine the ensuing summary statistics.\n\nmultiple_lm_2022_edit <- multiple_lm_2022 %>%\n  select(-rz_td_eff, -rz_fg_eff)\n\nlm_multiple_2022_edit <- lm(total_points ~ total_yards + rz_eff +\n                         total_to + def_td + spec_tds, data = multiple_lm_2022_edit)\n\nsummary(lm_multiple_2022_edit)\n\n\nCall:\nlm(formula = total_points ~ total_yards + rz_eff + total_to + \n    def_td + spec_tds, data = multiple_lm_2022_edit)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.176 -14.830  -3.786  18.990  55.917 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -472.58766  135.80424  -3.480  0.00178 ** \ntotal_yards    0.09977    0.01093   9.129 1.37e-09 ***\nrz_eff       309.11667  112.54620   2.747  0.01079 *  \ntotal_to      -0.27596    1.53880  -0.179  0.85907    \ndef_td         3.58077    4.26698   0.839  0.40902    \nspec_tds       4.95838    8.31675   0.596  0.55620    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.5 on 26 degrees of freedom\nMultiple R-squared:  0.8187,    Adjusted R-squared:  0.7838 \nF-statistic: 23.48 on 5 and 26 DF,  p-value: 7.028e-09\n\n\nWe have certainly simplified the model by removing both rz_td_eff and rz_fg_eff but the impact of this change is a fair trade off, it seems, to avoid further issues with multicollinearity. Our new adjusted R is still high (0.784), only dropping a bit from the original model that included both predictors (0.807). Both models did well at explaining the amount of variance between the predictors and the response variable. While the F-statistic and the p-value are strong in both models, it is important to note that the Residual standard error dropped from 27 in the original model to 28 in the more simplified version. Given that this value is the average difference between the data’s actual values and the predicted equivalents in the regression, both would ideally be smaller.\nWith multiple linear regression model producing acceptable results over the course of the 2022 season, we can now see if the results remain stable when produced from the course of 2012-2022.\n\nmultiple_lm_data_all <- multiple_lm_data %>%\n  select(-rz_td_eff, -rz_fg_eff, -season)\n\nlm_multiple_all <- lm(total_points ~ total_yards + rz_eff +\n                              total_to + def_td + spec_tds, data = multiple_lm_data_all)\n\nsummary(lm_multiple_all)\n\n\nCall:\nlm(formula = total_points ~ total_yards + rz_eff + total_to + \n    def_td + spec_tds, data = multiple_lm_data_all)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-70.483 -21.187  -1.671  19.644  81.849 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.227e+02  3.430e+01  -9.408  < 2e-16 ***\ntotal_yards  8.686e-02  2.944e-03  29.500  < 2e-16 ***\nrz_eff       2.643e+02  3.558e+01   7.429 8.65e-13 ***\ntotal_to    -2.068e+00  3.004e-01  -6.884 2.75e-11 ***\ndef_td       9.701e+00  9.715e-01   9.986  < 2e-16 ***\nspec_tds    -9.847e-01  1.933e+00  -0.509    0.611    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.51 on 346 degrees of freedom\nMultiple R-squared:  0.821, Adjusted R-squared:  0.8184 \nF-statistic: 317.4 on 5 and 346 DF,  p-value: < 2.2e-16\n\n\nThe results of the multiple linear regression over data from the 2012-2022 indicates a statistically significant relationship between our predictor variables and a team’s total yards. That said, two items are worth further exploration.\n\n\nThe model’s Residual standard error increased to 30, as opposed to the values of 27 and 28 from the models built on a single season of data. This means that the model, on average, is over or underpredicting the actual values by 30 total points. To verify that a residual standard error of 30 is not too high given the nature of our data, we can need to evaluate its value against the scale of our data based on the mean and/or median averages of the total_points variable. As seen below, the model’s RSE as a percentage of the mean is 8.1% and its percentage of the median is 8.2%. Given that both values are below 10%, it is reasonable to conclude that the value of the model’s residual standard error is statistically small compared to the scale of the total_points dependent variable.\n\n\n\n\n\n\nTip\n\n\n\n\ntotal_mean_points <- mean(multiple_lm_data_all$total_points)\ntotal_points_median <- median(multiple_lm_data_all$total_points)\n\nrse_mean_percentage <- (30 / total_mean_points) * 100\nrse_median_percentage <- (30 / total_points_median) * 100\n\n\n\n\nThe spec_tds predictor, which is the total number of special teams touchdowns scored by a team, has a p-value of 0.61. This high of a p-value indicates that the amount of special teams touchdowns earned by a team is not a dependable predictor of the team’s total points. Given the rarity of kickoff and punt returns, it is not surprising that the predictor returned a high p-value. If we run the regression again, without the spec_tds predictive variable, we get results that are nearly identical to the regression model that includes it as a predictor. The only significant difference is a decrease in the F-statistic from 398 to 317. Given the small decrease, we will keep spec_tds in the model.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe final step of our multiple linear regression model is feeding it new data to make predictions on.\nTo begin, we need to create a new dataframe that holds the new predictor variables. For nothing more than fun, let’s grab the highest value from each predictive variable between the 2012-2022 season.\n\nnew_observations <- data.frame(\n  total_yards = max(multiple_lm_data$total_yards),\n  rz_eff = max(multiple_lm_data$rz_eff),\n  total_to = max(multiple_lm_data$total_to),\n  def_td = max(multiple_lm_data$def_td),\n  spec_tds = max(multiple_lm_data$spec_tds))\n\nThis hypothetical team gained a total of 7,317 yards in one season and was incredibly efficient in the red zone, scoring 96% of the time. It also scored nine defensive touchdowns and returned a punt or a touchdown to the house four times. Unfortunately, the offense also turned the ball over a whopping total of 41 times.\nWe can now pass this information into our existing model using the predict function and it will output the predicted total_points earned by this hypothetical team based on the multiple linear regression model we built with 20 years of NFL data.\n\nnew_predictions <- predict(lm_multiple_all, newdata = new_observations)\n\nnew_predictions\n\nThe model determined, based on the new predictor variables provided, that this hypothetical team will score a total of 566 points, which is the second-highest cumulative amount scored by a team dating back to the 2012 season (the 2013 Denver Broncos scored 606 total points). In this situation, the hypothetical team has nearly double the turnovers as the 2013 Bronco (41 turnovers to 21). It is reasonable that providing this hypothetical team a lower number of turnovers would result in it becoming the highest scoring team since 2012.\n\n\n\n6.3.2 Logistic Regressions\nintroduction to logistic regressions here.\n\n6.3.2.1 Binary Classification\nexamples of binary classification here using nfl data.\n\n6.3.2.2 Multiclass Classification\nexamples of multiclass classification here using nfl data."
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#advanced-regression-techniques",
    "href": "05-nfl-analytics-advanced-methods.html#advanced-regression-techniques",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.4 Advanced Regression Techniques",
    "text": "6.4 Advanced Regression Techniques\n\n6.4.1 Regularization\nintro to regularization techniques here.\n\n6.4.1.1 Ridge Regression\nexamples of ridge regression using nfl data.\n\n6.4.1.2 Lasso Regression\nexamples of lasso regression using nfl data.\n\n6.4.1.3 Elastic Ridge Regression\nexamples of elastic ridge regression using nfl data.\n\n6.4.2 Generalized Linear and Additive Models (GLM/GAM)\nintro to both GLM and GAM here.\n\n6.4.2.1 Poisson Regression (GLM)\nexample of poisson regression using nfl data.\n\n6.4.2.2 Polynominal Regression(GAM)\nexample of polynominal regression using nfl data."
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#advanced-modeling-techniques",
    "href": "05-nfl-analytics-advanced-methods.html#advanced-modeling-techniques",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.5 Advanced Modeling Techniques",
    "text": "6.5 Advanced Modeling Techniques\nintroduction to advanced modeling techniques in the nfl.\n\n6.5.1 Clustering\nintroduction to clustering.\n\n6.5.1.1 K-means Clustering\nexamples of k-means clustering here.\n\n6.5.1.2 Hierarchical Clustering\nexamples of hierarchical clustering here.\n\n6.5.2 Decision Trees/Random Forests\nintroduction to decision trees/random forests in nfl data.\n\n6.5.2.1 Classification Trees\nexample of classification tree using nfl data.\n\n6.5.2.2 Regression Trees\nexample of regression tree using nfl data."
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#creating-our-own-rushing-yards-over-expected-model",
    "href": "05-nfl-analytics-advanced-methods.html#creating-our-own-rushing-yards-over-expected-model",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.6 Creating Our Own Rushing Yards Over Expected Model",
    "text": "6.6 Creating Our Own Rushing Yards Over Expected Model\nBack in January of 2021, Tej Seth posted an article to the Michigan Football Analytics Society blog that outlined his vision for creating a “public expected rushing yards model.” The structure of his model, as explained by Tej, was inspired by the prior work of Michael Egle, an honorable mention in both the 2021 and 2023 NFL Big Data Bowl, who previously used the college football equivalent of open-source data (cfbfastR) to create an RYOE model for the collegiate game.1 In Tej’s case, his approach to created an NFL-centric RYOE model culminated with the creation of his RYOE Shiny App that allows anybody to explore his RYOE metric by season or team and even through three-way rusher comparisons.\nDespite a slightly intimidating title, rushing yards over expected is a fantastic entrypoint into exploring model creation and analysis in NFL analytics - in fact, the growing number of “over expected” metrics in the NFL are all great ways to begin learning about and understanding advanced modeling. Robby Greerre, the owner of nfeloapp.com - a website that provides “data-driven analytics, picks, and predictions for the NFL” - explains that over expected metrics are a increasingly popular avenue in which analysts can “paint a more accurate picture of performance by adjusting familiar statistics like ‘completion percentage’ or ‘yards per rush’ for conflating factors like degree of difficulty or game text” (Greerre 2022).\nSome of these metrics, like completion percentage over expected (CPOE), are widely accepted. Specifically, CPOE calculates how likely any quarterback’s pass is going to be complete or incomplete compared to other passing attempts. It is considered “widely accepted” because the metric itself is considered “stable” in that the r-squared value retains a strong correlation for individual quarterbacks across several seasons. In fact, as Greerre points out, the r-squared value for CPOE for just one season is 0.226 which is extremely strong based on NFL analytics standards.\nOn the other hand, RYOE - based on Greerre’s analysis - maintains an r-squared value below 0.15 until a running back’s fourth season, wherein the average improves to 0.263 (an otherwise stable value). But that does not mean that RYOE is not a metric worth further exploration. The effectiveness of any one metric to account for factors such as degree of difficulty or game text largely relies on our ability to provide adequate feature engineering - specifically, how much relevant data the machine learning model can ingest to begin making predictions.\nBecause of that, significant machine learning models have been built with information provided by the NFL’s Big Data Bowl as it is the one chance that the public receives to feature engineer with the NFL’s tracking data (wherein a player’s position, speed, direction, etc. is tracked and recorded every 1/10th of a second). Unfortunately, only small windows of data exist from the Big Data Bowl releases and, as a result, we are often required to find creative ways to provide further context to each play/player for the machine learning model.\nTo showcase this idea, we are going to begin exploring ways to add additional feature engineering to Tej Seth’s already fantastic Rushing Yard Over Expected model. While not the most stable metric, as mentioned, the idea of RYOE is generally easy to understand for even the most analyst. Broadly, given what we know about every other rushing play during a specific span of seasons, what is the most likely amount of yards a running back is going to gain on a specific rushing play as predicted by the model on other similar situations?\nThat difference is rushing yards over expected.\nUsing Tej’s Shiny app, we can explore all seasons between 2015 and 2022 for those running backs that had a minimum of 755 rushing attempts.\n\n\n\n\nCredit: Tej Seth\n\n\n\n\nAccording to Tej’s model, since 2015, Nick Chubb of the Cleveland Browns earned - on average - 0.66 over expected. Aaron Jones is closely behind with 0.61 over expected and then a significant drop occurs for the third and fourth players.\nTo understand how Tej engineered his model and to begin exploring other possible features to feed into the model, we can dive into his publicly available code.\n\n6.6.1 Tej Seth’s RYOE Model: Under The Hood\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to immediately point out that Tej built his RYOE model using the xgboost package whereas we will begin constructing ours using tidymodels.\nWhile the underlying eXtreme Gradient Boosting process is largely the same with both approaches, the necessary framework we will construct with tidymodels differs greatly from the coding used with the xgboost package.\nThe xgboost package is a standalone package in R that provide an implementation of the eXtreme Gradient Boosting algorithm. To that end, it offers a highly efficient and flexible way to train gradient boosting models for various machine learning tasks, such as classification, regression, and ranking. The package provides its own set of functions for training, cross-validation, prediction, and feature importance evaluation.\nThe tidymodels package, on the other hand, is a collection of R packages that provide a unified framework for modeling and machine learning tasks. It includes several package for data preprocessing, modeling, validation, and evaluation. Because of this, the core goal of the tidymodels teams is to offer a consistent syntax and workflow for a wide range of machine learning models. It is a fair comparison to say that tidymodels is the tidyverse of the machine learning world.\n\n\nJust like the model we will be building in this chapter, Tej constructed his model via eXtreme Gradient Boosting.\nWhich may lead to a very obvious question if you are new to machine learning: what exactly is eXtreme Gradient Boosting?\n\n6.6.2 eXtreme Gradient Boosting Explained\neXtreme Gradient Boosting (XGBoost) is a powerful machine learning technique that is particularly good at solving supervised machine learning problems, such as classification (categorizing data into classes, for example) and regression (predicting numerical values).\nXGBoost can be thought of as an “expert team” that combines the knowledge and skills of multiple “individual experts” to make better decisions or predictions. Each of these “experts” in this context is what we call a decision tree, which is a flowchart structure used for making decisions based on a series of question about the data.\nOnce provided data, XGBoost seeks to iteratively build a collection of “bad” decision trees and then build an ensemble of these poor ones into a more accurate and robust model. The term “gradient” comes from the fact that the algorithm uses the gradient (or the slope) of the loss function (a measure of how well the model fits the data) to guide the learning process."
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#building-a-ryoe-model-with-tidymodels",
    "href": "05-nfl-analytics-advanced-methods.html#building-a-ryoe-model-with-tidymodels",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.7 Building a RYOE Model with tidymodels\n",
    "text": "6.7 Building a RYOE Model with tidymodels\n\nPlace holding for the tidymodels code below.\n\nlibrary(tidyverse)\nlibrary(nflverse)\nlibrary(tidymodels)\nlibrary(tune)\nlibrary(bon)\n\noptions(scipen = 999)\noptions(digits = 3)\n\n### reading in NFL play-by-play data from 2016 to 2022\npbp <- nflreadr::load_pbp(2016:2022)\n\n### filtering to just rushing attempts that are not missing any yards_gained\nrush_attempts <- pbp %>%\n  filter(season_type == \"REG\") %>%\n  filter(rush_attempt == 1, qb_scramble == 0,\n         qb_dropback == 0, !is.na(yards_gained)) \n\n### quickly calculating each defteam's avg. rushing yards allowed per season\ndef_ypc <- rush_attempts %>%\n  filter(!is.na(defteam)) %>%\n  group_by(defteam, season) %>%\n  summarize(def_ypc = mean(yards_gained))\n\n### joining the defteam avg yards gained into the rushing data\nrush_attempts <- rush_attempts %>%\n  left_join(def_ypc, by = c(\"defteam\", \"season\"))\n\n### gathering offensive formation, offensive personnel, defensive personnel, and defenders in box\nparticipation <- nflreadr::load_participation(seasons = 2016:2022) %>%\n  select(nflverse_game_id, play_id, possession_team, offense_formation,\n         offense_personnel, defense_personnel, defenders_in_box)\n\n### merging participation data into our rushing attempts data\n### note: the team match is likely not necessary but I was being careful\nrush_attempts <- rush_attempts %>%\n  left_join(participation, by = c(\"game_id\" = \"nflverse_game_id\",\n                                  \"play_id\" = \"play_id\",\n                                  \"posteam\" = \"possession_team\"))\n\n### creating a secondary dataframe for joining back in player names\nrushing_data_join <- rush_attempts %>%\n  group_by(game_id, rusher, fixed_drive) %>%\n  mutate(drive_rush_count = cumsum(rush_attempt)) %>%\n  ungroup() %>%\n  group_by(game_id, rusher) %>%\n  mutate(game_rush_count = cumsum(rush_attempt)) %>%\n  mutate(rush_prob = (1 - xpass) * 100,\n         strat_score = rush_prob / defenders_in_box,\n         wp = wp * 100) %>%\n  ungroup() %>%\n  mutate(red_zone = if_else(yardline_100 <= 20, 1, 0),\n         fg_range = if_else(yardline_100 <= 35, 1, 0),\n         two_min_drill = if_else(half_seconds_remaining <= 120, 1, 0)) %>%\n  select(label = yards_gained, season, week, yardline_100, quarter_seconds_remaining,\n         half_seconds_remaining, qtr, down, ydstogo, shotgun, no_huddle,\n         ep, wp, drive_rush_count, game_rush_count, red_zone, fg_range, two_min_drill,\n         offense_formation, offense_personnel, defense_personnel, defenders_in_box,\n         rusher, rush_prob, def_ypc, strat_score, rusher_player_id, posteam, defteam) %>%\n  na.omit()\n\n### bringing in pre-aggregated next gen stats\nnext_gen_stats <- nflreadr::load_nextgen_stats(seasons = 2016:2022, stat_type = \"rushing\") %>%\n  filter(week > 0 & season_type == \"REG\") %>%\n  select(season, week, player_gsis_id,\n         against_eight = percent_attempts_gte_eight_defenders, avg_time_to_los)\n\n### merging in next gen stats\nrushing_data_join <- rushing_data_join %>%\n  left_join(next_gen_stats, by = c(\"season\", \"week\", \"rusher_player_id\" = \"player_gsis_id\")) %>%\n  na.omit()\n\n### placing offense personnel positions in individual columns\n### new column: extra_ol\nrushing_data_join <- rushing_data_join %>%\n  mutate(\n    ol = str_extract(offense_personnel, \"(?<=\\\\s|^)\\\\d+(?=\\\\sOL)\") %>% as.numeric(),\n    rb = str_extract(offense_personnel, \"(?<=\\\\s|^)\\\\d+(?=\\\\sRB)\") %>% as.numeric(),\n    te = str_extract(offense_personnel, \"(?<=\\\\s|^)\\\\d+(?=\\\\sTE)\") %>% as.numeric(),\n    wr = str_extract(offense_personnel, \"(?<=\\\\s|^)\\\\d+(?=\\\\sWR)\") %>% as.numeric()) %>%\n  replace_na(list(ol = 5)) %>%\n  mutate(extra_ol = if_else(ol > 5, 1, 0)) %>%\n  mutate(across(ol:wr, as.factor)) %>%\n  select(-ol, -offense_personnel)\n\n### doing some as above but for defense personnel\nrushing_data_join <- rushing_data_join %>%\n  mutate(dl = str_extract(defense_personnel, \"(?<=\\\\s|^)\\\\d+(?=\\\\sDL)\") %>% as.numeric(),\n         lb = str_extract(defense_personnel, \"(?<=\\\\s|^)\\\\d+(?=\\\\sLB)\") %>% as.numeric(),\n         db = str_extract(defense_personnel, \"(?<=\\\\s|^)\\\\d+(?=\\\\sLB)\") %>% as.numeric()) %>%\n  mutate(across(dl:db, as.factor)) %>%\n  select(-defense_personnel)\n\nrushing_data_join <- rushing_data_join %>%\n  filter(qtr < 5) %>% ### let's remove rushes that took place in OT\n  mutate(qtr = as.factor(qtr),\n         down = as.factor(down),\n         shotgun = as.factor(shotgun),\n         no_huddle = as.factor(no_huddle),\n         red_zone = as.factor(red_zone),\n         fg_range = as.factor(fg_range),\n         two_min_drill = as.factor(two_min_drill),\n         extra_ol = as.factor(extra_ol))\n\n### going to build model from rushes so will remove identifying information\nrushes <- rushing_data_join %>%\n  select(-season, -week, -rusher, -rusher_player_id, -posteam, -defteam) %>%\n  mutate(across(where(is.character), as.factor))\n\nstr(rushes)\n\n#################################\n## tidymodels work now\n################################\nset.seed(1984)\n\nstr(rushes)\n\nrushing_split <- initial_split(rushes)\nrushing_train <- training(rushing_split)\nrushing_test <- testing(rushing_split)\nrushing_folds <- vfold_cv(rushing_train)\n\n### creating our xgboost recipe\nrushing_recipe <-\n  recipe(formula = label ~ ., data = rushing_train) %>%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\n### creating the model boosting tree specifications\nrushing_specs <- boost_tree(\n  trees = tune(),\n  tree_depth = tune(), \n  min_n = tune(),\n  mtry = tune(),\n  loss_reduction = tune(),\n  sample_size = tune(),\n  learn_rate = tune(),\n  stop_iter = tune()) %>%\n  set_engine(\"lightgbm\", num_leaves = tune()) %>%\n  set_mode(\"regression\")\n\n### creating the tuning grid\nrushing_grid <- grid_latin_hypercube(\n  trees(),\n  tree_depth(),\n  finalize(mtry(), rushes),\n  min_n(),\n  num_leaves(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  learn_rate(),\n  stop_iter(),\n  size = 5)\n\n### adding everything into the workflow\nrushing_workflow <-\n  workflow() %>%\n  add_recipe(rushing_recipe) %>%\n  add_model(rushing_specs)\n\nregisterDoSEQ() ### to leave parallel processing\nregisterDoParallel(cores = 11)\ndoParallel::stopImplicitCluster()\n\n### tuning the grid with tictoc() running to see how long it takes\ntic()\nrushing_tune <-\n  tune_grid(rushing_workflow, resamples = rushing_folds, grid = rushing_grid, control = control_grid(save_pred = TRUE,\n                                                                                                     verbose = TRUE))\ntoc() ## 344.16 seconds/5 minutes (100 trees, 30 grid)\n      ## 5783.15 seconds/ 96 minutes (1000 trees, 60 grid)\n      ## 15643.44 seconds / 4.5 hours (tune() trees, 100 grid)\n\n### extracting the best performing hyperparameters from the tuning results\nbest_params <- rushing_tune %>%\n  select_best(metric = \"rmse\")\n\n### creating a final workflow with this updated model specification\nrushing_final_workflow <- rushing_workflow %>%\n  finalize_workflow(best_params)\n\n### fitting the workflow on the testing data\nfinal_model <- rushing_final_workflow %>%\n  fit(data = rushing_test)\n\n### using final mod to add the predictins to our prior rushing_data_join information\nrushing_predictions <- predict(final_model, rushing_data_join)\n\n### creating our projections\nryoe_projs <- cbind(rushing_data_join, rushing_predictions) %>%\n  rename(actual_yards = label,\n         exp_yards = .pred)\n\n### doing some math to find the league-wide average of mean_ryoe per season\nmean_ryoe <- ryoe_projs %>%\n  dplyr::group_by(season) %>%\n  summarize(nfl_mean_ryoe = mean(actual_yards) - mean(exp_yards))\n\n### merging in the mean_ryoe into the data per season\nryoe_projs <- ryoe_projs %>%\n  left_join(mean_ryoe, by = c(\"season\" = \"season\"))\n\n### taking a player's actual yards minus his expected yards and then weighing it against NFL average per season\nryoe_projs <- ryoe_projs %>%\n  mutate(ryoe = actual_yards - exp_yards + nfl_mean_ryoe)\n\n### outputting the results\nryoe_projs %>%\n  group_by(rusher) %>%\n  summarize(\n    rushes = n(),\n    yards = sum(actual_yards),\n    exp_yards = sum(exp_yards),\n    ypc = yards / rushes,\n    exp_ypc = exp_yards / rushes,\n    avg_ryoe = mean(ryoe)) %>%\n  filter(rushes > 1000) %>%\n  arrange(-avg_ryoe)\n\nfffff\n\nggplot(data = for_plot, aes(x = yards, y = exp_yards)) +\n  stat_poly_line(formula = y ~ x + 0, \n                 se = FALSE, color = \"black\", \n                 linewidth = .8, linetype = \"dashed\") +\n  stat_poly_eq(formula = y ~ x + 0,\n               aes(label = after_stat(rr.label), \n                   family = \"Roboto Condensed\", \n                   size = 9, fontface = \"bold\"),\n               label.y = 1, label.x = 0.4) +\n  geom_point(shape = 21, fill = for_plot$team_color, \n             color = for_plot$team_color2, size = for_plot$rushes / 200) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     label = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     label = scales::comma_format()) +\n  nfl_analytics_theme() +\n  labs(x = \"Actual Rushing Yards\",\n       y = \"Expected Rushing Yards\",\n       title = \"Actual Rushing Yards vs. Expected Rushing Yards\",\n       subtitle = \"2016 - 2022   |   Model: LightGBM\",\n       caption = \"An Introduction to NFL Analytics with R\\nBrad J. Congelio\") +\n  geom_text_repel(data = filter(for_plot, yards >= 4500),\n                  aes(label = rusher), box.padding = .8, \n                  point.padding = 1, family = \"Roboto Condensed\")"
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#navigating-the-nfls-big-data-bowl",
    "href": "05-nfl-analytics-advanced-methods.html#navigating-the-nfls-big-data-bowl",
    "title": "\n6  Advanced Methods: Modeling and Big Data Bowl\n",
    "section": "\n6.8 Navigating the NFL’s Big Data Bowl",
    "text": "6.8 Navigating the NFL’s Big Data Bowl\nlots of place holding for Big Data Bowl material below\nThomas Bliss, a Data Scientist with the NFL, provided an incredibly helpful list of potential topics that can be explored using 2023 Big Data Bowl information:\n\nanalyze blocker positioning after the QB leaves the pocket and/or is pressured\nanalyze blocker ability to hold a defender in place without moving towards the QB\nlink the rate of false starts to an offensive line’s time off the snap\nlink between QB release time, receiver separation, and offensive line performance\n\n\n### reading all weeks and writing to a parquet file\nall_bdb_weeks <- function(dir = file.path('core-data')) {\n  paths <- fs::dir_ls(dir, regexp = 'week\\\\d+')\n  all_weeks <-\n    paths %>%\n    purrr::map_df(vroom::vroom) %>%\n    janitor::clean_names() %>%\n    arrow::write_parquet(file.path('core-data', 'data.parquet'))\n}\n\nall_bdb_weeks()\n\n### reading in all play information provided and writing to a parquet file\nread_bdb_plays <- memoise::memoise({function() {\n  plays <- file.path('core-data', 'plays.csv') %>%\n    readr::read_csv() %>%\n    janitor::clean_names() %>%\n    arrow::write_parquet(file.path('core-data', 'plays.parquet'))\n}})\n\nread_bdb_plays()\n\n### reading in individual game information and writing to a parquet file\nread_game_info <- memoise::memoise({function() {\n  file.path('core-data', 'games.csv') %>%\n    readr::read_csv() %>%\n    janitor::clean_names() %>%\n    dplyr::mutate(dplyr::across(game_date = lubridate::mdy)) %>%\n    arrow::write_parquet(file.path('core-data', 'games.parquet'))\n}})\n\n### creating an individual .cvs file for each game in bdb\nall.weeks <- read_parquet(\"./core-data/large-lfs-files/all-weeks-parquet\")\n\nall.weeks %>%\n  group_by(game_id) %>%\n  group_walk(~ write_csv(.x, paste0('tracking_gameId_', .y$game_id, \".csv\")))\n\n### writing in team colors, logos\nteam.colors <- nflfastR::teams_colors_logos %>%\n  select(team_abbr, team_color, team_color2, team_logo_espn)\n\nfffff\n\nrotate_the_dots <- function(df) {\n  \n  if(!\"play_direction\" %in% names(df)) {\n    message(\"Cannot find play directions. Inferring from offense and defense locations at snap.\")\n    \n    df <- df %>%\n      filter(event == \"ball_snap\", team != \"football\") %>%\n      group_by(game_id, play_id, defensive_team) %>%\n      summarize(mean_x = mean(x, na.rm = T)) %>%\n      pivot_wider(names_from = defensive_team, values_from = mean_x, names_prefix = \"x_\") %>%\n      ungroup() %>%\n      mutate(\n        play_direction = \n          ifelse(\n            x_1 > x_0,\n            \"right\",\n            \"left\") %>%\n          select(game_id, play_id, play_direction) %>%\n          inner_join(df, by = c(\"game_id\", \"play_id\")))\n        \n  }\n  \n  df <- df %>%\n    mutate(\n      to_left = ifelse(play_direction == \"left\", 1, 0),\n      x = ifelse(to_left == 1, 120 - x, x),\n      y = ifelse(to_left == 1, 160 / 3 - y, y),\n      los_x = 100 - absolute_yardline_number,\n      dist_from_los = x - los_x)\n  \n  if (\"o\" %in% names(df)) {\n    df <- df %>%\n      mutate(\n        o = ifelse(to_left == 1, o + 180, o),\n        o = ifelse(o > 360, 0 - 360, o),\n        o_rad = pi * (o / 180),\n        o_x = ifelse(is.na(o), NA_real_, sin(o_rad)),\n        o_y = ifelse(is.na(o), NA_real_, cos(o_rad)))\n  }\n  \n  if (\"dir\" %in% names(df)) {\n    df <- df %>%\n      mutate(\n        dir = ifelse(to_left == 1, dir + 180, dir),\n        dir = ifelse(dir > 360, dir - 360, dir),\n        dir_rad = pi * (dir / 180),\n        dir_x = ifelse(is.na(dir), NA_real_, sin(dir_rad)),\n        dir_y = ifelse(is.na(dir), NA_real_, cos(dir_rad)),\n        s_x = dir_x * s,\n        s_y = dir_y * s,\n        a_x = dir_x * a,\n        a_y = dir_y * a)\n  }\n  \n  return(df)\n}\n\nffff\n\nfind_o_diff <- function(df, prefix = \"qb\") {\n  \n  name_x <- sym(paste0(prefix, \"_x\"))\n  name_y <- sym(paste0(prefix, \"_y\"))\n  \n  new_column <- paste0(\"o_to_\", prefix)\n  \n  df <- df %>%\n    mutate(\n      dis_x = {{name_x}} - x,\n      dis_y = {{name_y}} - y,\n      \n      tmp = atan2(dis_y, dis_x) * (180 / pi),\n      tmp = (360 - tmp) + 90,\n      tmp = case_when(tmp < 0 ~ tmp + 360,\n                      tmp > 360 ~ tmp - 360,\n                      TRUE ~ tmp),\n      \n      diff = abs(o - tmp),\n      \n      diff = abs(o - tmp),\n      \n      !!new_column := pmin(360 - diff, diff)) %>%\n        select(-diff, -tmp)\n      \n    return(df)\n}\n\n\n########\n## READING IN PITTSBURGH VS. BUFFALO - WEEK 1\n########\n\npitt.buff <- arrow::read_parquet(\"core-data/large-lfs-files/all-weeks-parquet\") %>%\n  filter(game_id == \"2021091201\")\n\n########\n## READING IN PLAYS FROM PITTSBURGH VS. BUFFALO - WEEK 1\n########\n\npitt.buff.plays <- readr::read_csv(\"core-data/plays.csv\") %>%\n  janitor::clean_names() %>%\n  filter(game_id == \"2021091201\")\n\n########\n## READING IN GAME INFO FROM PITTSBURGH VS. BUFFALO - WEEK 1\n########\n\npitt.buff.info <- readr::read_csv(\"core-data/games.csv\") %>%\n  janitor::clean_names() %>%\n  filter(game_id == \"2021091201\")\n\n########\n## COMBING CORE GAME FILE WITH PLAYS, AND THEN BY INFO (TO AVOID MULTIPLE GAME_IDs IN DF)\n########\n\ncomplete.data <- inner_join(pitt.buff, pitt.buff.plays, by = c(\"game_id\" = \"game_id\", \"play_id\" = \"play_id\"))\n\ncomplete.data <- complete.data %>%\n  inner_join(pitt.buff.info, by = c(\"game_id\" = \"game_id\"))\n\n### ROTATING THE DOTS\ncomplete.data <- rotate_the_dots(complete.data)\n\n########\n## MUTATING TO CHARACTER VARIABLE DEFINING WHETHER TEAM IN FRAMES IS ON OFFENSE OR DEFENSE\n########\n\ncomplete.data <- complete.data %>%\n  mutate(off_or_def = case_when(\n    team == possession_team ~ \"offense\",\n    team != possession_team ~ \"defense\",\n    TRUE ~ \"football\"))\n\n            ########\n            ## CORE CLEANING AND PREP IS COMPLETE\n            ########\n\n########\n## ADDING IN INFORMATION FROM PLAYERS.CSV TO BUILD CHULLs FOR JUST O-LINE\n########\n\nplayer.info <- readr::read_csv(\"core-data/players.csv\") %>%\n  janitor::clean_names() %>%\n  select(nfl_id, official_position, display_name)\n\ncomplete.data <- complete.data %>%\n  inner_join(player.info, by = c(\"nfl_id\" = \"nfl_id\"))\n\n########\n## LET'S PICK OUT A FUN PLAY TO WORK WITH\n########\n\none.play <- complete.data %>%  ### BEN PASS TO EBRON FOR 19 YARDS\n  filter(play_id == 2209)\n\n########\n## NOW LET'S BUILD A CONVEX HULL FOR JUST THE OFFENSIVE LINE\n########\n\nol_chull_order <- one.play %>%\n  filter(off_or_def == \"offense\") %>%\n  filter(official_position %in% c(\"T\", \"C\", \"G\")) %>%  #### IMPORTANT TO KNOW PERSONNEL PACKAGE HERE: 0 RB, 0 TE, 5 WR\n  select(frame_id, x, y) %>%\n  chull\n\nol_chull_order <- c(ol_chull_order, ol_chull_order[1])\n\nol_chull_coords <- one.play %>%\n  filter(off_or_def == \"offense\") %>%\n  select(frame_id, x, y) %>%\n  slice(ol_chull_order)\n\nol_chull_poly <- sp::Polygon(ol_chull_coords, hole = F)\nol_chull_area <- ol_chull_poly@area\n\n########\n## NOW LET'S PLOT IT\n########\n\none.play %>%\n  the_dots(\n    animated = TRUE,\n    orientation = FALSE,\n    convex = TRUE,\n    segment_length = 6,\n    segment_size = 3,\n    dot_size = 4,\n    animated_h = 4,\n    animated_w = 8,\n    animated_res = 150\n  )\n\n\n\n\n\nGreerre, Robby. 2022. “Over Expected Metrics Explained – What Are CPOE, RYOE, and YACOE.” https://www.nfeloapp.com/analysis/over-expected-explained-what-are-cpoe-ryoe-and-yacoe/."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Awbrey, Jake. 2020. “The Future of NFL Analytics.” https://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics.\n\n\nBechtold, Taylor. 2021. “How the Analytics Movement Has Changed\nthe NFL and Where It Has Fallen Short.” https://theanalyst.com/na/2021/04/evolution-of-the-analytics-movement-in-the-nfl/.\n\n\n“Big Data Bowl: The Annual Analytics Contest Explores Statistical\nInnovations in Football.” n.d. https://operations.nfl.com/gameday/analytics/big-data-bowl/.\n\n\nBushnell, Henry. 2021. “NFL Teams Are Taking 4th-down Risks More\nThan Ever - but Still Not Often Enough.” https://sports.yahoo.com/nfl-teams-are-taking-4th-down-risks-more-than-ever-but-still-not-often-enough-163650973.html.\n\n\nCarl, Sebastian. 2022. “nflplotR.” https://nflplotr.nflverse.com/.\n\n\nFortier, Sam. 2020. “The NFL’s Analytics Movement Has Finally\nReached the Sport’s Mainstream.” https://www.washingtonpost.com/sports/2020/01/16/nfls-analytics-movement-has-finally-reached-sports-mainstream/.\n\n\nGreerre, Robby. 2022. “Over Expected Metrics Explained – What Are\nCPOE, RYOE, and YACOE.” https://www.nfeloapp.com/analysis/over-expected-explained-what-are-cpoe-ryoe-and-yacoe/.\n\n\nHeifetz, Danney. 2019. “We Salute You, Founding Father of the\nNFL’s Analytics Movement.” https://www.theringer.com/nfl-preview/2019/8/15/20806241/nfl-analytics-pro-football-focus.\n\n\nKirschner, Alex. 2022. “The Rams’ Super Bowl Afterparty Turned\ninto a Historic Hangover.” https://fivethirtyeight.com/features/the-rams-super-bowl-afterparty-turned-into-a-historic-hangover/.\n\n\nKozora, Alex. 2015. “Tomlin Prefers \"Feel over Analytics\".”\nhttp://steelersdepot.com/2015/09/tomlin-prefers-feel-over-analytics/.\n\n\nRosenthal, Gregg. 2018. “Super Bowl LII: How the 2017 Philadelphia\nEagles Were Built.” https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753.\n\n\nSilge, Julia. n.d. “Tidymodels.” https://tidymodels.org.\n\n\nStikeleather, Jim. 2013. “The Three Elements of Successful Data\nVisualizations.” https://hbr.org/2013/04/the-three-elements-of-successf.\n\n\nWickham, Hadley. 2022. “Tidyverse Packages.” https://www.tidyverse.org/packages/."
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#air-yards",
    "href": "a1-nfl-analytics-dictionary.html#air-yards",
    "title": "Appendix A — NFL Analytics Reference Guide",
    "section": "\nA.1 Air Yards",
    "text": "A.1 Air Yards\nAir yards is the measure that the ball travels through the air, from the line of scrimmage, to the exact point where the wide receivers catches, or does not catch, the football. It does not take into consideration the amount of yardage gained after the catch by the wide receiver (which would be yards after catch).\nFor an example, please see the below illustration. In it, the line of scrimmag is at the 20-yardline. The QB completes a pass that is caught at midfield (the 50-yardline). After catching the football, the wide receiver is able to advance the ball down to the opposing 30-yardline before getting tackled. First and foremost, the quarterback is credited with a total of 50 passing yards on the play, while the wide receiver is credited with the same.\nHowever, because air yards is a better metric to explore a QB’s true impact on a play, he is credited with 30 air yards while the wide receiver is credited with 20 yards after catch.\nIn the end, quarterbacks with higher air yards per attempt are generally assumed to be throwing the ball deeper downfield than QBs with lower air yards per attempt.\n\n\n\n\n\n\n\nThere are multiple ways to collect data pertaining to air yards. However, the most straightforward way is to use load_player_stats:\n\ndata <- nflreadr::load_player_stats(2021)\n\nair.yards <- data %>%\n  filter(season_type == \"REG\") %>%\n  group_by(player_id) %>%\n  summarize(\n    attempts = sum(attempts),\n    name = first(player_name),\n    air.yards = sum(passing_air_yards),\n    avg.ay = mean(passing_air_yards)) %>%\n  filter(attempts >= 100) %>%\n  select(name, air.yards, avg.ay) %>%\n  arrange(-air.yards)\n\ntibble(air.yards)\n\n# A tibble: 42 x 3\n   name       air.yards avg.ay\n   <chr>          <dbl>  <dbl>\n 1 T.Brady         5821   342.\n 2 J.Allen         5295   311.\n 3 M.Stafford      5094   300.\n 4 D.Carr          5084   299.\n 5 J.Herbert       5069   298.\n 6 P.Mahomes       4825   284.\n 7 T.Lawrence      4732   278.\n 8 D.Prescott      4612   288.\n 9 K.Cousins       4575   286.\n10 J.Burrow        4225   264.\n# ... with 32 more rows\n\n\nIn the above example, we can see that Tom Brady led the NFL during the 2021 regular season with a comined total of 5,821 air yards which works out to an average of 342 air yards per game."
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#average-depth-of-target",
    "href": "a1-nfl-analytics-dictionary.html#average-depth-of-target",
    "title": "Appendix A — NFL Analytics Reference Guide",
    "section": "\nA.2 Average Depth of Target",
    "text": "A.2 Average Depth of Target\nAs mentioned above, a QB’s air yards per attempt can highlight whether or not he is attempting to push the ball deeper down field than his counterparts. The official name of this is Average Depth of Target (or ADOT). We can easily generate this statistic using the load_player_stats function within nflreader:\n\ndata <- nflreadr::load_player_stats(2021)\n\nadot <- data %>%\n  filter(season_type == \"REG\") %>%\n  group_by(player_id) %>%\n  summarize(\n    name = first(player_name),\n    attempts = sum(attempts),\n    air.yards = sum(passing_air_yards),\n    adot = air.yards / attempts) %>%\n  filter(attempts >= 100) %>%\n  arrange(-adot)\n\ntibble(adot)\n\n# A tibble: 42 x 5\n   player_id  name       attempts air.yards  adot\n   <chr>      <chr>         <int>     <dbl> <dbl>\n 1 00-0035704 D.Lock          111      1117 10.1 \n 2 00-0029263 R.Wilson        400      3955  9.89\n 3 00-0036945 J.Fields        270      2636  9.76\n 4 00-0034796 L.Jackson       382      3531  9.24\n 5 00-0036389 J.Hurts         432      3882  8.99\n 6 00-0034855 B.Mayfield      418      3651  8.73\n 7 00-0026498 M.Stafford      601      5094  8.48\n 8 00-0031503 J.Winston       161      1340  8.32\n 9 00-0034857 J.Allen         646      5295  8.20\n10 00-0029604 K.Cousins       561      4575  8.16\n# ... with 32 more rows\n\n\nAs seen in the results, if we ignore Drew Lock’s 10.1 ADOT on just 111 attempts during the 2021 regular season, Russell Wilson attempted to push the ball, on average, furtherst downfield among QBs with atleast 100 attempts."
  },
  {
    "objectID": "a2-nfl-further-reading.html#introduction-to-r-programming-books",
    "href": "a2-nfl-further-reading.html#introduction-to-r-programming-books",
    "title": "Appendix B — Further Reading Suggestions",
    "section": "B.1 Introduction to R Programming Books",
    "text": "B.1 Introduction to R Programming Books\n\nR for Data Science: Import, Tidy, Transform, Visualize, and Model Data\nHands-On Programming with R: Write Your Own Functions and Simulations\nThe Book of R: A First Course in Programming and Statistics\nLearning R: A Step-by-Step Function Guide to Data Analysis\nThe Art of R Programming: A Tour of Statistical Software Design\nAdvanced R (Second Edition)"
  },
  {
    "objectID": "a2-nfl-further-reading.html#data-visualization-in-r-and-visualization-guides",
    "href": "a2-nfl-further-reading.html#data-visualization-in-r-and-visualization-guides",
    "title": "Appendix B — Further Reading Suggestions",
    "section": "B.2 Data Visualization in R and Visualization Guides",
    "text": "B.2 Data Visualization in R and Visualization Guides\n\nR Graphics Cookbook: Practicl Recipes for Visualizing Data\nStorytelling with Data: A Data Visualization Guides for Business Professionals\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks"
  },
  {
    "objectID": "a2-nfl-further-reading.html#sport-analytics-guidesbooks",
    "href": "a2-nfl-further-reading.html#sport-analytics-guidesbooks",
    "title": "Appendix B — Further Reading Suggestions",
    "section": "B.3 Sport Analytics Guides/Books",
    "text": "B.3 Sport Analytics Guides/Books\n\nThe Midrange Theory: Basketball’s Evolution in the Age of Analytics\nAnalyzing Baseball Data with R (2nd edition)\nA Fan’s Guide to Baseball Analytics: Why WAR, WHIP, wOBA, and other Advanced Sabermetrics Are Essential to Understanding Modern Baseball\nThe Book: Playing the Percentages in Baseball\nThe Hidden Game of Baseball: A Revolutionary Approach to Baseball and Its Statistics\nThe Hidden Game of Football: A Revealing and Lively Look at the Pro Game, With New Stats, Revolutionary Strategies, and Keys to Picking the Winners\nMathletics: How Gamblers, Managers, and Fans Use Mathematics in Sports\nBasketball Data Science: With Applications in R\nData Analytics in Football (Soccer): Positional Data Collection, Modelling, and Analysis"
  }
]