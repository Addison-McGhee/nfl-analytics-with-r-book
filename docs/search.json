[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to NFL Analytics with R",
    "section": "",
    "text": "Preface\nOn April 27, 2020, Ben Baldwin hit send on a Tweet that announced the birth of nflfastR, an R package designed to scrape NFL play-by-play data, allowing the end-user to access it at speeds quicker than similar predecessors (hence the name).\nThanks to the work of multiple people (@mrcaseB, @benbbaldwin, @TanHo, @LeeSharpeNFL, and @thomas_mock … to name just a few), the process of getting started with analytics using NFL data is now easier than ever.\nThat said, and without getting too far into the weeds of the history, the above-mentioned people are responsible in some shape or form for the current status of the nflverse, which is a superb collection of data and R-based packages that allows anybody the ability to access deeply robust NFL data as far back as the 1999 season.\nThe nflverse as we know it today was initially birthed from the nflscrapR project, which was started by the Carnegie Mellon University student and professor duo of Maksim Horowitz and Sam Ventura. After Horowitz graduated - and got hired by the Atlanta Hawks - the nflscrapR package was taken over by fellow CMU student Ron Yurko (who would go on to receive his Ph.D. from the Statistics and Data Science program and, at the time of this book’s writing, is an Assistant Teaching Professor in the Department of Statistics and Data Science at CMU). The trio’s work on nflscrapR led to a peer-reviewed paper titled “nflWAR: A Reproducible Method for Offensive Player Evaluation in Football.” Ultimately, the nflscrapR project came to an end when the specific .json feed used to gather NFL data changed. At this point, Ben Baldwin and Sebastian Carl had already built upon the nflscrapR project’s foundations to create nflfastR. Yurko officially marked the end of the nflscrapR era and the beginning of the nflfastR era with a tweet on September 14, 2020:1\nAs a reply to his first tweet about the nflfastR project, Baldwin explained that he created the original function to scrape NFL data for the creation of his NFL analytics website. Thankfully, he and Carl did not keep the creation to themselves and released nflfastR to the public. Because of the “open source” nature of R and R packages, a laundry list of companion packages quickly developed alongside nflfastR. The original nflfastR package is now part of the larger nflverse of packages that drive the NFL analytics community on Twitter and beyond.\nThe creation of the nflverse allowed for anybody interested in NFL analytics to easily access data, manipulate it to their liking, and release their visualizations and/or metrics to the wider public. In fact, it is now a regular occurrence for somebody to advance their R programming ability because of the nflverse and then go on to win the Big Data Bowl. As of the 2022 version of the Big Data Bowl, over “30 participants have been hired to work in data and analytics roles in sports, including 22 that were hired in football” (Big Data Bowl, n.d.). Most recently, the Chargers hired 2020 participate Alex Stern and the Chiefs hired Marc Richards, a member of the winning 2021 team, as a Football Research Analyst.\nKevin Clark, in a 2018 article for The Ringer, explained that despite not being as obvious as the sabermetrics movement in baseball, the analytics movement in the NFL is “happening in front of you all the time.” The use of analytics in the NFL did, however, predate Clark’s article. In 2014, Eagles head coach Doug Pederson explained that all decisions made by the organization - from game planning to draft strategy - are informed by hard data and analytics. Leading this early adoption of analytics, and reporting directly to team Vice President Howie Roseman, were Joe Douglas and Alec Halaby, “a 31-year-old Harvard grad with a job description” that had an emphasis on “integrating traditional and analytical methods in football decision-making.” The result? A “blending of old-school scouting and newer approaches” that were often only seen in other leagues, such as the NBA and MLB (Rosenthal, 2018). Pederson believed in and trusted the team’s approach to analytics so much that a direct line of communication was created between the two during games, with the analytics department providing the head coach with math-based recommendations for any scenario Pederson requested (Awbrey, 2020).2\nIn just under five years time since the publishing of that article, it has become hard to ignore the analytic movement within the NFL. Yet, there is still so much growth to happen in the marriage between the NFL and advanced metrics. For example, there is no denying that the sabermetrics movement drastically “altered baseball’s DNA” Heifetz (2019)]. Moreover, as explained in Seth Partnow’s outstanding The Midrange Theory: Basketball’s Evolution in the Age of Analytics, the analytics movement in the NBA essentially killed the midrange shot (briefly: it is more beneficial to try to work the ball in directly under the basket (for a high-percentage shot) or to take the 3-pointer, as the possible additional point is statistically worth more despite the lower success probability as opposed a 2-point midrange shot).\nCompared to both the NBA and MLB, the NFL is playing catch up in analytics driving changes equivalent to the death of the midrange shot or the plethora of additional tactics and changes to baseball because of sabermetrics. Joe Banner, who served as the President of the Eagles from 2001-2012 and then the Chief Executive Officer of the Browns from 2012-2013, explained that some of the hesitation to incorporate analytics into NFL game planning was a result of the game being “very much driven by conventional wisdom to an extreme degree” (Fortier, 2020). Perhaps nobody encapsulates this better than Pittsburgh SteelersHead Coach Mike Tomlin. When asked about his position on analytics during the 2015 season, Tomlin explained:\nGiven that Tomlin’s quote is from 2015, perhaps the Steelers pivoted since and are now more analytically inclined. That does not seem to be the case. In a poll of NFL analytics staffers conducted by ESPN, the Steelers were voted as one of the least analytically advanced teams in the league.\nThere is large gap between the least analytically inclined teams (Washington, Tennessee, Cincinnati, New York Giants, and Pittsburgh) and those voted as the most analytically inclined (Baltimore, Cleveland, Philadelphia, and Houston). In the ESPN poll, the Browns were voted as the analytics department producing the highest level of work. One of those polled spoke to the fact that much of this outstanding work is a result of General Manager Andrew Berry being a “true believer,” explaining that he is one of the “rare guys you’ll come across in life where you think to yourself, ‘Man, this guy thinks at a different level. Just pure genius.’ He’s one of them.”\nIn his article for the Washington Post, Sam Fortier argues that many teams became inspired to more intimately introduce analytics into game planning and on-field decisions after the 2017 season. On their run to becoming Super Bowl Champions, the Philadelphia Eagles were aggressive on 4th down, going for it 26 times during the season and converting on 17 of those for a conversion percentage of 65.4%. An examination and visualization of the data highlights the absolutely staggering increase in 4th down aggressiveness among NFL head coaches from 2000-2021:\nNumber of 4th Down Attempts: 2000-2021\nThere has been a 96.3% increase in the number of 4th down attempts from just 2000 to 2021. In fact, the numbers may actually be higher as I was quite conservative in building the above plot by only considering those 4th down attempts that took place when the offensive team had between a 5-to-95% winning probability and those prior to the two-minute warning of either half. Even with those conservative limitations, the increase is staggering. The numbers, however, support this aggression. During week one of both the 2020 and 2021 seasons, not going for it on 4th down “cost teams a cumulative 170 percentage points of win probability” (Bushnell, 2021).\nBen Baldwin, using the nfl4th package that is part of the nflverse, tracked this shift in NFL coaching mentality regarding 4th down decisions by comparing 2014’s “go for it percentage” against the same for 2020. When compared to the 2014 season, NFL coaches are now much more in agreement with analytics on when to “go for it” on 4th down in relation to the expected gain in win probability.\nCredit: Ben Baldwin\nIt should not be surprising then, considering Mike Tomlin’s quote from above and other NFL analytics staffers voting the Steelers as one of the least analytically driven teams in the league, that Pittsburgh lost the most win probability by either kicking or punting in “go for it” situations during the 2020 NFL season. On the other end, the Ravens and Browns - two teams voted as the most analytically inclined - are the two best organizations at knowing when to “go for it” on 4th down based on win probability added. There seems to be a defined relationship between teams buying into analytics and those who do not:\nCredit: Ben Baldwin\nThe NFL’s turn towards more aggressive 4th-down decisions is just one of the many analytics-driven changes occurring in the league. Another significant example is Defense-Adjusted Value over Average (or DVOA), a formula created by Aaron Schatz, now the editor in chief of Football Outsiders, that sought to challenge the notion that teams should, first, establish the running game in order to open up the passing game. Some of these changes are apparent on televisions screens on Sunday afternoons in the Fall, while others are occurring behind the scenes (analytics departments working on scouting and draft preparation, for example). Indeed, the use of analytics in the NFL is not as tightly ingrained as we see in other prominent leagues. And we must remember that there are certainly continued hold outs among some NFL coaches (like Mike Tomlin).\nDespite some coaching hold outs on fully embracing analytics, the “thirst for knowledge in football is as excessive as any other sport and the desire to get the most wins per dollar is just as high.” As the pipeline of data continues to grow, both internally in the league and data that becomes publicly accessible, “smart teams will continue to leave no rock unturned as they push the limits on how far data can take them.” Joe Banner explained that while the NFL has long been a league of coaches saying “well, that is the way we’ve always done it,” the league is ripe for a major shift (Bechtold, 2021).\nBanner’s belief that those teams searching for every competitive advantage will “leave no rock unturned” is the driving force behind this book. For all intents and purposes, the age of analytics in the NFL is still in its infancy. Turning back, again, to the 2017 season, the Eagles’ management praised and credited the team’s analytics department as part of the reason they were able to win Super Bowl LII. Doing so, Danny Heifetz argues, “changed the language of football.” The NFL, he explains, is a “copycat league” and, as witnessed with the increase in 4th down aggressiveness since 2017, teams immediately began to carbon copy Philadelphia’s approach to folding traditional football strategy with a new age analytics approach. Because of the modernity of this relationship between long-held football dogmas and analytics, nobody can be quite sure what other impacts it will create on the gamesmanship of football.\nHowever, as Heifetz opines, both the NBA and MLB can serve as a roadmap to where analytics will take the NFL. Importantly, the NFL’s relationship with analytics is still in its “first frontier of what will likely be a sweeping change over the next two decades.” Because of this, we cannot be sure what the next major impact analytics will make, nor when it may occur. But, with the ever-growing amount of publicly accessible data, it is only a matter of time until it is discovered. For example, in an interview with Heifetz, Brian Burke - one of the forefather’s of NFL analytics and now a writer for ESPN - expressed his belief that the next major impact will be “quantifying how often quarterbacks make the correct decision on the field.”\nIt seems that every new NFL season results in an amateur analyst bringing a groundbreaking model and/or approach to the table. Unlike, for example, MLB where there is little left to discover in terms of sabermetrics and new approaches to understanding the game and its associated strategy, the NFL is - for lack of a better phrase - an open playing field. With more and more data becoming available to the public, it is now easier than ever investigate your own ideas and suspicions and to create your own models to confirm your intuition.\nFor example, I am originally from the greater Pittsburgh area and am a big Steelers fan (which certainly explains some of the Steelers-centric examples I use in the writing of this book). I was adamant in my belief that Pittsburgh’s TJ Watt should win the 2021 Defensive Player of the Year award, despite many others calling for Los Angeles’ Aaron Donald to claim the title. In effort to prove my point, I sought out to design what I coined Adjusted Defensive Impact. To begin, I wanted to explore the idea that not all defensive sacks are created equal, as a player’s true impact is not always perfectly represented by top-level statistics.\nTo account for that, I opted to adjust and add statistical weight to sack statistics. This was done over multiple areas. For instance, not all players competed in all 17 regular-season games in 2021. To adjust for this, I took the total of game played in the data (2,936) and divided by 17 (a full season) to achieve a weighted adjustment of 0.0058. TJ Watt played in just 15 games in 2021. His adjusted equation, therefore, is (17-‘games’) * 0.0058. The result? He gets a bit more credit for this adjustment than, say, Myles Garrett who played all 17 regular-season games.\nGoing further with the model, I created a weighted adjustment for solo sacks (0.90), a negative weighted adjustment (-0.14) for any sack charted as “unblocked,” and a weighted adjustment to account for how many times a player actually rushed the QB compared to how many defensive snaps they played. Using data from the SIS Data Hub, the full code is below:\noptions(digits = 2)\n\npass.data &lt;- pass_rush_data %&gt;%\n  select(Player, Team, Games, `Pass Snaps`, `Pass Rushes`,\n         `Solo Sacks`, `Ast. Sacks`, `Comb. Sacks`, \n         `Unblocked Sacks`, Hurries, Hits) %&gt;%\n  rename(total.snaps = `Pass Snaps`,\n         total.rushes = `Pass Rushes`,\n         solo.sacks = `Solo Sacks`,\n         asst.sacks = `Ast. Sacks`,\n         comb.sacks = `Comb. Sacks`,\n         unblocked.sacks = `Unblocked Sacks`,\n         player = Player,\n         team = Team,\n         games = Games,\n         hurries = Hurries,\n         hits = Hits)\n\npass.data$rush.percent &lt;- pass.data$total.rushes / pass.data$total.snaps\n\ncalculated.impact &lt;- pass.data %&gt;%\n  group_by(player) %&gt;%\n  summarize(\n    adjusted.games = (17 - games) * 0.0058,\n    adjusted.solo = solo.sacks * 0.9,\n    adjusted.unblocked = unblocked.sacks / -0.14,\n    adjusted.rush.percent = 0.81 - rush.percent,\n    combined.impact = sum(adjusted.games + \n                            (solo.sacks * 0.9) + \n                            (unblocked.sacks * -0.14) + \n                            adjusted.rush.percent))\nThe end result? Taking into account the above adjusted defensive impact, TJ Watt was absolutely dominant during the 2021 season:\nAdjusted Defensive Impact Results\nAll of these examples - from Ben Baldwin’s 4th-down model, to Football Outsiders’ DVOA, to my attempt to further quantify defensive player impact - are just the leading edge of the burgeoning analytics movement in the NFL. Moreover, the beauty of analytics is that you do not have to be a mathematician or statistics buff in order to enter the fray. All it takes is a genuine curiosity to explore what Bob Carroll, Pete Palmer, and John Thorn coined as the “Hidden Game of Football” and the desire to learn, if you have not already, the R programming language."
  },
  {
    "objectID": "index.html#overview-of-chapters",
    "href": "index.html#overview-of-chapters",
    "title": "Introduction to NFL Analytics with R",
    "section": "Overview of Chapters",
    "text": "Overview of Chapters\n\nChapter 1 introduces the process of investigate inquiry, and how it can be used to formulate a question to be answered through the use of data and R. Specifically, the chapter provides a broad overview of what is possible with analytics and NFL data by exploring the impact of having a high number of unique offensive line combinations had on the 2022 Los Angeles Rams. The process of data collection, manipulation, preparation, and visualization are introduced to seek the answer regarding how changes in an offensive line can impact various attributes of an offense. The chapter concludes with another working example, allowing readers to explore which teams from the 2022 season most required an explosive play per drive to score a touchdown.\nChapter 2 covers the process of downloading both R and RStudio, as well as the necessary packages to do NFL analytics. As one of the most important chapters in the book (especially for those new to the R programming language), readers take a deep dive into wrangling NFL data with the tidyverse package. To begin, readers will learn about the dplyr pipe (%&gt;%) and use, in exercises, the six most important verbs in the dplyr language: filter(), select(), arrange(), summarize(), mutate(), and group_by(). At the conclusion of the chapter, multiple exercises are provided to allow readers to practice using the dplyr verbs, relational operators within the filter() function and creating “new stats” by using the summarize() function. Moreover, readers will determine the relationship between the dplyr language and important variables within the nflverse data such as player_name and player_id, which is important for correct manipulation and cleaning of data.\nChapter 3 examines the numerous and, often, bewildering amount of functions “underneath the hood” of the packages that makes up the nflverse. For example, load_pbp() and load_player_stats() are included in both nflfastR and nflreadr. However, load_nextgen_stats(), load_pfr_stats(), and load_contracts() are all part of just nflreadr. Because of this complexity, readers will learn how to efficiently operate within the nflverse. Moreover, chapter 3 provides a plethora of examples and exercises related to all of the various functions included.\nChapter 4 moves readers from data cleaning and manipulation to an introduction to data visualization using ggplot2. As well, chapter 4 provides further instruction on nflverse functions such as clean_player_names(), clean_team_abbrs(), and clean_homeaway(). As well, to prep for data visualization, readers will be introduced to the teams_colors_logos and load_rosters functions as well as the nflplotR package, which provides “functions and geoms to help visualization of NFL related analysis” (Carl, 2022). Readers will produce multiple types of visualizations, including geom_bar, geom_point, geom_density, and more. As well, readers will learn to use facet_wrap and facet_grid to display data over multiple seasons. For visualizations that include team logos or player headshots, instruction will cover both how to do the coding manually using teams_colors_logos or load_rosters and to use the nflplotr package to avoid the need to use left_join to merge teams_colors_logos to your data frame. At the conclusion of the chapter, readers will be introduced to the gt and gtExtras packages for creating sleek tables as well as be walked through the creation of their first Shiny App.\nChapter 5 introduces advanced methods in R using nflverse data, with a specific focus on modeling and machine learning. To streamline the process of learning, readers will be introduced to tidymodels, a “collection of packages for modeling and machine learning using tidyverse principles” (Silge, n.d.). Readers will first be introduced to the modeling process by creating and running a simple linear regression model. After, regressions are built upon with multiple linear regressions, binary regressions, and binomial regression. Readers will then begin working with more advanced methods of machine learning, such as k-means clustering and building an XGBoost model."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "Introduction to NFL Analytics with R",
    "section": "About The Author",
    "text": "About The Author\nI (Bradley Congelio) am currently an Assistant Professor in the College of Business at Kutztown University of Pennsylvania. Aside from my core area of instruction, I also teach the very popular Sport Analytics (SPT 313) course.\nI earned my Ph.D. from the University of Western Ontario and received a specialized certificate in R for Data Analytics from the University of California, San Diego in 2021. I am a proud undergraduate alumni of West Liberty University and am a strong advocate of a broad-based liberal arts education.\nMy research focuses on using big data, the R programming language, and analytics to explore the impact of professional stadiums on neighboring communities. I use the proprietary Zillow ZTRAX database as well as U.S. Census and other forms of data to create robust, applied, and useful insight into how best to protect those living in areas where stadiums are proposed for construction.\nAs well, my work in sport analytics, specifically the NFL, has been featured on numerous media outlets, including the USA Today and Sports Illustrated.\nFinally, my most recent academic, peer-reviewed publications include:\n\nCongelio, B. (2022). ’Examining the Impact of New Stadium Construction on Local Property Prices Using Data Analytics and the Zillow ZTRAX Database.” Journal of Business, Economics, and Technology Spring 2022, 39-55.\nCongelio, B. (2021). “Monitoring the Policing of Olympic Host Cities: A Novel Approach Using Data Analytics and the LA2028 Olympic Summer Games.” Journal of Olympic Studies 2(2), 129-145.\nCongelio, B. “Predicting the Impact of a New Stadium on Surrounding Neighborhoods Through the Use of a k-means Unsupervised Clustering Algorithm.” Currently under peer review.\nCongelio, B. “Examining Megaevent’s Impact on Foot Traffic to Local Businesses Using Mobility and Demographic Aggregation Data.” Currently writing and funded by a $15,000 grant.\n\nWhy A Book Instead of Working in Analytics?\nI am sometimes asked why I spend time in the classroom teaching this material rather than taking my domain knowledge to the “industry side” and working in the NFL or an otherwise NFL-connected outlet.\nThe honest and, likely boring, answer is this: I love teaching. My favorite experience in the classroom yet is always in my Sport Analytics course. The frustration and sense of helplessness is palpable in the first weeks of the semester as students attempt to wrap their head around, what a former student called, “this [censored] foreign language.” I insist that they keep pushing through the exercises and assignments. Often, there is line out my door and extending down the hallway during office hours comprised of just students from the Sport Analytics class.\nAnd then something amazing happens.\nTypically about halfway through the semester, I start seeing the light bulbs go off. Instead of cursing in anger at the “foreign language,” students begin randomly cursing in excitement as the flow of the tidyverse language “clicks.” Once that happens, it is off to the races because, once they understand speaking in tidyverse, learning more difficult packages (like tidymodels) seems doable.\nAnd that is why I teach. That moment where I realize my lecturing, assisting, explaining, and gentle nudging are all finally paying dividends - not for me, though. For the students.\nThis book serves as an extension of that classroom experience. As a reader of this book, you are now a “student” and I hope you do not hesitate to reach out to me if you ever have any questions or, more importantly, when (not if) you have that “light bulb moment” and everything begins to click for you."
  },
  {
    "objectID": "index.html#technical-details",
    "href": "index.html#technical-details",
    "title": "Introduction to NFL Analytics with R",
    "section": "Technical Details",
    "text": "Technical Details\nThis book was written using RStudio’s Visual Editor for R Markdown. It was published using the Quarto publishing system built on Pandoc. As well, the following packages were used in this book:\n\n\n\n\nPackages Used In This Book\n\npackage\nversion\nsource\n\n\n\narrow\n10.0.1\nCRAN (R 4.1.3)\n\n\nbonsai\n0.2.1\nCRAN (R 4.1.3)\n\n\ncaret\n6.0-94\nCRAN (R 4.1.3)\n\n\ncowplot\n1.1.1\nCRAN (R 4.1.1)\n\n\ncropcircles\n0.2.1\nCRAN (R 4.1.3)\n\n\ndoParallel\n1.0.17\nCRAN (R 4.1.3)\n\n\ndplyr\n1.1.1\nCRAN (R 4.1.3)\n\n\nextrafont\n0.19\nCRAN (R 4.1.3)\n\n\nfactoextra\n1.0.7\nCRAN (R 4.1.1)\n\n\ngeomtextpath\n0.1.1\nCRAN (R 4.1.3)\n\n\nggcorrplot\n0.1.4\nCRAN (R 4.1.3)\n\n\nggfx\n1.0.1\nCRAN (R 4.1.3)\n\n\nggimage\n0.3.1\nCRAN (R 4.1.3)\n\n\nggplot2\n3.4.2\nCRAN (R 4.1.3)\n\n\nggpmisc\n0.5.2\nCRAN (R 4.1.3)\n\n\nggrepel\n0.9.2\nCRAN (R 4.1.3)\n\n\nggridges\n0.5.4\nCRAN (R 4.1.3)\n\n\nggtext\n0.1.2\nCRAN (R 4.1.3)\n\n\nglue\n1.6.2\nCRAN (R 4.1.3)\n\n\ngt\n0.8.0\nCRAN (R 4.1.3)\n\n\ngtExtras\n0.4.5\nCRAN (R 4.1.3)\n\n\nlightgbm\n3.3.5\nCRAN (R 4.1.3)\n\n\nmagick\n2.7.3\nCRAN (R 4.1.1)\n\n\nnflfastR\n4.5.1\nCRAN (R 4.1.3)\n\n\nnflreadr\n1.3.2\nCRAN (R 4.1.3)\n\n\nnflverse\n1.0.2\nhttps://nflverse.r-universe.dev (R 4.1.3)\n\n\nnnet\n7.3-16\nCRAN (R 4.1.1)\n\n\nRColorBrewer\n1.1-3\nCRAN (R 4.1.3)\n\n\nreshape2\n1.4.4\nCRAN (R 4.1.1)\n\n\nscales\n1.2.1\nCRAN (R 4.1.3)\n\n\ntidymodels\n1.0.0\nCRAN (R 4.1.3)\n\n\ntidyverse\n2.0.0\nCRAN (R 4.1.3)\n\n\nvroom\n1.6.1\nCRAN (R 4.1.3)\n\n\nwebshot\n0.5.4\nCRAN (R 4.1.3)\n\n\n\n\n\n\n\n\nFinally, please note that this book uses the dplyr pipe operator (%&gt;%) as opposed to the new, built-in pipe operator released with version 4.1 of R (|&gt;). It is likely that you can work through the exercises and examples in this book by using either operator. I maintain my use of the dplyr pipe operator for no other reason than a personal (and problematic) dislike of change."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to NFL Analytics with R",
    "section": "License",
    "text": "License\nThe online version of this book is published with the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license.\n\n\n\n\nAwbrey, J. (2020). The future of NFL analytics. Retrieved from https://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics\n\n\nBechtold, T. (2021). How the analytics movement has changed the NFL and where it has fallen short. Retrieved from https://theanalyst.com/na/2021/04/evolution-of-the-analytics-movement-in-the-nfl/\n\n\nBig data bowl: The annual analytics contest explores statistical innovations in football. (n.d.). Retrieved from https://operations.nfl.com/gameday/analytics/big-data-bowl/\n\n\nBushnell, H. (2021). NFL teams are taking 4th-down risks more than ever - but still not often enough. Retrieved from https://sports.yahoo.com/nfl-teams-are-taking-4th-down-risks-more-than-ever-but-still-not-often-enough-163650973.html\n\n\nCarl, S. (2022). nflplotR. Retrieved from https://nflplotr.nflverse.com/\n\n\nFortier, S. (2020). The NFL’s analytics movement has finally reached the sport’s mainstream. Retrieved from https://www.washingtonpost.com/sports/2020/01/16/nfls-analytics-movement-has-finally-reached-sports-mainstream/\n\n\nHeifetz, D. (2019). We salute you, founding father of the NFL’s analytics movement. Retrieved from https://www.theringer.com/nfl-preview/2019/8/15/20806241/nfl-analytics-pro-football-focus\n\n\nKozora, A. (2015). Tomlin prefers \"feel over analytics\". Retrieved from http://steelersdepot.com/2015/09/tomlin-prefers-feel-over-analytics/\n\n\nRosenthal, G. (2018). Super bowl LII: How the 2017 philadelphia eagles were built. Retrieved from https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753\n\n\nSilge, J. (n.d.). Tidymodels. Retrieved from https://tidymodels.org"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to NFL Analytics with R",
    "section": "",
    "text": "Thanks to Ben Baldwin for chatting with me on Discord and providing this brief understanding of the backstory.↩︎\nThanks, again, to Ben Baldwin for providing his personal knowledge about the “early days” of the Eagles’ analytics department.↩︎"
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#introduction",
    "href": "01-nfl-analytics-and-r.html#introduction",
    "title": "\n1  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nIt might seem odd to begin an introductory book with coding and visualization in Chapter 1, while placing information about learning the basics of the tidyverse in a later chapter. But there is good reason for this pedagogical approach being utilized in this book. As explained by Hadley Wickham and Garrett Grolemund in their outstanding book R for Data Science, the process of reading in and then cleaning data is not exactly the most exciting part of doing analytics. As evidence suggest, early excitement about and integration into a topic increases the likelihood of following up and learning the “boring” material.\nBecause of this, I follow the approach of Wickham and Grolemund and provide data that is already, for the most part, “tidied” and ready to be used. We will however, in later chapters, pull raw data directly from its source (such as nflreadr, Pro Football Reference, and Sports Info Solutions) that requires manipulation and cleaning before any significant analysis can begin.\n\n\n\n\n\n\nImportant\n\n\n\nI am assuming, while you may not have a full grasp of the tidyverse yet, that you do currently have base R and RStudio installed. If you do not, more detailed instructions are provided in Chapter 2. If you would rather jump right into this material, you can download base R and RStudio at the following links. Once both are installed, you can return to this point in the chapter to follow along.\nTo download/install base R: cran.rstudio.com\nTo download/install RStudio: RStudio Desktop (scroll to bottom of page for Mac options)\n\n\nMoreover, as briefly outlined in the Preface, we move through the process of learning NFL analytics via a close relationship with investigative inquiry. In this instance, we will define the process of investigative inquiry as one that seeks both knowledge and information about a problem/question through data-based research. To that end, we will routinely use the process throughout this book to uncover insights, patterns, and trends relating to both players and teams that serve to help us answer the problem/question we are examining.\nWhile it can - and should - be entertaining to develop visualization and models around arbitrarily picked statistics and metrics, it is important to remember that the end goal of the process is to glean useful insights that, ultimately, can be shared with the public. Much like the work done by a data analyst for a Fortune 500 company, the work you produce as a result of this book should do two things: (1.) provide deeper insight and knowledge about NFL teams and players and (2.) effectively communicate a story.\nThis is why the process of investigative inquiry is ingrained, as much as possible, into every example provided in the coming chapters. In doing so, the standard outline for completing an investigate inquiry is modified to fit the needs of this book - specifically, the addition of communicating your findings to the public at the end."
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#the-investigate-inquiry-outline",
    "href": "01-nfl-analytics-and-r.html#the-investigate-inquiry-outline",
    "title": "\n1  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n1.2 The Investigate Inquiry Outline",
    "text": "1.2 The Investigate Inquiry Outline\n\nIdentify the problem or question. The first step in any investigative inquiry is to clearly define the problem or question that you are trying to answer. Many times, fans have questions about their individuals favorite team and/or players. For example, the 2022 Los Angeles Rams - the defending Super Bowl Champions - were eliminated from playoff contention with three weeks remaining in the season. With the early exit, the Rams tied the 1999 Denver Broncos for the earliest elimination from playoff contention for any prior Super Bowl Champion. The Rams’ early elimination can be explained by the high number of injuries during the season, including Matthew Stafford, Cooper Kupp, and Aaron Donald. However, another factor that was routinely discussed during the season, was the team’s inability to keep offensive linemen healthy. In this specific example, in terms of identifying the problem or question, a potential problem or question to explore is: how many unique combinations of offensive linemen did the 2022 LA Rams use and what sort of impact did this have on the team’s playmaking ability? Have other teams in recent history faced the same amount of offensive line turnover yet still make the playoffs? As you can see, there are a number of different avenues in which the problem or question surrounding the Rams’ offensive line injury issues can be explored.\nGather data. With a question or problem determined, we now turn to the process of finding and gathering the necessary data to find answers. Unfortunately, data is not always going to be available to answer your investigate inquiry. For example, the NFL’s tracking data is only released in partial form during the annual Big Data Bowl. In the event that your question or problem requires data that is not available, you must loop back to Step 1 and reconfigure your question to match available data. In the case of the 2022 LA Rams’ offensive line, access to data that can answer the question is available through two cores avenues: the load_participation() and load_snap_counts() functions within the nflverse family of packages.\nClean and prepare the data. It is not often that the data you obtain to answer your question will be perfectly prepared for immediate analysis. As will be explored below, the data collected to explore the Rams’ offensive line combinations required both (1.) a critical thought process on how to best solve oddities in the data while still producing reliable information and (2.) cleaning and preparation to make the changes as a result of that critical thinking process. As you progress through the many examples and exercises in this book, you will often be presented with prepared data sets that require you to determine the best approach to data manipulation through this critical thinking and cleaning/preparation process.\nAnalyze the data. After problem solving to ensure the data is as reliable and consistent as possible, we can turn to analyzing the data. In this case, since we are concerned with unique combinations of offensive linemen, we can quickly get results by using the n_distinct() function within dplyr.\n\nVisualize the data. There are generally two options for visualizing data: plotting with ggplot() or creating a table with gt and the outstanding companion package gtExtras. Considering the following can help determine whether to present your findings in chart or table format.\n\nThe type of data you are working with. If you have a large amount of numerical data that needs to be compared or analyzed, a table may be the most effective way to present this information. On the other hand, if you want to highlight trends or patterns in your data, a chart can help illustrate the information in a more clear manner.\nThe purpose of your visualization. You must consider what you ultimately want to communicate with your visualization. If you want to provide a detailed breakdown of your data, a table is usually more appropriate. However, if you want to show the overall trend or pattern in your data, a chart is going to be more effective.\nThe audience for your visualization. As you determine whether to use a chart or a table, think about who will be viewing your visualization and what level of detail they need. If your audience is familiar with the data and needs to see precise values, a table may be a better choice. If your audience is not as familiar with the data and you want to highlight the main trends or patterns, a chart my be more effective.\n\n\nInterpret and communicate the results. Lastly, it is time to communicate your results to the public. Whether this be through Twitter, a team blog, or a message board, there are numerous items to consider when preparing to build your story/narrative for sharing. This will be covered further in Chapter 4 as well.\n\nWith a clear direction via the investigative inquiry process, we can turn to taking a deeper dive into the LA Rams’ 2022 offensive linemen issue."
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#investigating-the-rams-2022-offensive-line",
    "href": "01-nfl-analytics-and-r.html#investigating-the-rams-2022-offensive-line",
    "title": "\n1  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n1.3 Investigating the Rams’ 2022 Offensive Line",
    "text": "1.3 Investigating the Rams’ 2022 Offensive Line\nThe “Super Bowl hangover” is real.\nAt least for the loser of the big game.\nSince the AFL and NFL merged in 1970, a total of 15 of the 51 losers of the Super Bowl went on to miss the playoffs the following season, while 13 failed to even achieve a winning record. Teams coming off a Super Bowl victory have generally fared better, with the winners putting together a .500 record or better 45 out of 51 times.\nOf those six teams to not achieve a .500 record after winning the Super Bowl, only a few have been as downright terrible as the 2022 Los Angeles Rams.\nAs explained by Mike Ehrmann, the Rams’ poor Super Bowl defense is “what happens when a laundry list of things go wildly wrong at the same time” (Kirschner, 2022). As outlined above in our investigative inquiry outline, one of the core items on the Rams’ laundry list of bad luck was the absurd amount of offensive linemen ending up on the injured list. This, combined with losing Andrew Whitworth to retirement after the Super Bowl, led to quarterback Matthew Stafford being sacked on 8.6-percent of his dropback attempts (a rate that nearly doubled from the previous season).\nGiven that context, just how historically bad was the Rams’ 2022 offensive line turnover? We can being diving into the data to find our results and build our story.\n\n1.3.1 Unique Offensive Line Combinations: How to Collect The Data?\nTo begin obtaining and preparing the data to determine the number of unique offensive line combinations the Rams had in the 2022 season, we turn to two possible options: the load_participation() and load_snap_counts() functions within the nflreadr package. The load_participation() function will return, if include_pbp = TRUE, a list of every player ID number that was on the field for each play, whereas load_snap_counts() returns - on a per game basis - the percentage of snaps each player was on the field for.\nIn the end, using load_snap_counts() creates the most accurate, reliable, and straightforward way in each to collect unique offensive line combinations. The load_participation() function results in several oddities in the data (not with the collection of it by the nflverse maintainers, but with individual NFL team strategies and formations). To highlight this, the following code selects the first offensive play for each team, in each game, of the 2022 season.\n\nparticipation &lt;- nflreadr::load_participation(2022, include_pbp = TRUE)\nrosters &lt;- nflreadr::load_rosters(2022) %&gt;%\n  select(full_name, gsis_id, depth_chart_position)\n\noline_participation &lt;- participation %&gt;%\n  filter(play_type %in% c(\"pass\", \"run\")) %&gt;%\n  group_by(nflverse_game_id, possession_team, fixed_drive) %&gt;%\n  filter(fixed_drive == 1 | fixed_drive == 2) %&gt;%\n  filter(row_number() == 1) %&gt;%\n  select(nflverse_game_id, play_id, possession_team, \n         offense_personnel, offense_players) %&gt;%\n  dplyr::mutate(gsis_id = stringr::str_split(offense_players, \";\")) %&gt;%\n  tidyr::unnest(c(gsis_id)) %&gt;%\n  left_join(rosters, by = c(\"gsis_id\" = \"gsis_id\"))\n\noline_participation &lt;- oline_participation %&gt;%\n  filter(depth_chart_position %in% c(\"T\", \"G\", \"C\")) %&gt;%\n  group_by(nflverse_game_id, possession_team) %&gt;%\n  mutate(starting_line = toString(full_name)) %&gt;%\n  select(nflverse_game_id, possession_team, \n         offense_personnel, starting_line) %&gt;%\n  distinct()\n\nWhile the output using the load_participation() function is correct, a quick examination of the offense_personnel column causes concern about the viability of this approach to calculate the total number of unique offensive line combinations. A grouping and summing of the offense_personnel column highlights the issue.\n\noline_participation %&gt;%\n  group_by(offense_personnel) %&gt;%\n  summarize(total = n())\n\n# A tibble: 14 x 2\n   offense_personnel                        total\n   &lt;chr&gt;                                    &lt;int&gt;\n 1 1 RB, 0 TE, 4 WR                             4\n 2 1 RB, 1 TE, 3 WR                           240\n 3 1 RB, 2 TE, 2 WR                           171\n 4 1 RB, 3 TE, 1 WR                            19\n 5 2 QB, 1 RB, 1 TE, 2 WR                       4\n 6 2 RB, 0 TE, 3 WR                             1\n 7 2 RB, 1 TE, 2 WR                            89\n 8 2 RB, 2 TE, 1 WR                            14\n 9 3 RB, 1 TE, 1 WR                             1\n10 6 OL, 1 RB, 0 TE, 3 WR                       2\n11 6 OL, 1 RB, 1 TE, 2 WR                      12\n12 6 OL, 1 RB, 2 TE, 1 WR                       1\n13 6 OL, 2 RB, 0 TE, 2 WR                       1\n14 7 OL, 0 RB, 0 TE, 0 WR,1 P,1 LS,1 DL,1 K     1\n\noline_participation\n\n# A tibble: 560 x 4\n# Groups:   nflverse_game_id, possession_team [557]\n   nflverse_game_id possession_team offense_personnel starting_line   \n   &lt;chr&gt;            &lt;chr&gt;           &lt;chr&gt;             &lt;chr&gt;           \n 1 2022_01_BAL_NYJ  NYJ             1 RB, 2 TE, 2 WR  George Fant, Al~\n 2 2022_01_BAL_NYJ  BAL             2 RB, 1 TE, 2 WR  Ben Powers, Mor~\n 3 2022_01_BUF_LA   BUF             1 RB, 1 TE, 3 WR  Ryan Bates, Dio~\n 4 2022_01_BUF_LA   LA              1 RB, 1 TE, 3 WR  Rob Havenstein,~\n 5 2022_01_CLE_CAR  CAR             1 RB, 1 TE, 3 WR  Ikem Ekwonu, Ta~\n 6 2022_01_CLE_CAR  CLE             1 RB, 1 TE, 3 WR  Jedrick Wills, ~\n 7 2022_01_DEN_SEA  SEA             1 RB, 2 TE, 2 WR  Phil Haynes, Ab~\n 8 2022_01_DEN_SEA  DEN             1 RB, 2 TE, 2 WR  Garett Bolles, ~\n 9 2022_01_GB_MIN   MIN             1 RB, 1 TE, 3 WR  Christian Darri~\n10 2022_01_GB_MIN   GB              1 RB, 2 TE, 2 WR  Royce Newman, J~\n# i 550 more rows\n\n\nOf concern are rows 10 through 14. In 15 different cases, a team ran its first play of the game with six offensive linemen. And, in one case, the resulting data indicates that the Dallas Cowboys ran their first play in week 5 against the LA Rams with seven offensive linemen, one punter, one long snapper, and a kicker.\nIn the first case, the data is correct that the teams ran their first offensive play with six offensive linemen. For example, in its week 3 game against the Steelers, the data list the Cleveland Browns as having started Jack Conklin (tackle), Jedrick Wills Jr. (tackle), Joel Bitonio (guard), Michael Dunn (guard), Wyatt Teller (guard), and Ethan Pocic (center). Viewing the NFL’s All-22 film of this specific play confirms that, indeed, all six offensive linemen were on the field for the Browns’ first snap of the game.\n\n\n\n\nSteelers vs. Browns - 6 Offensive Linemen\n\n\n\nIn the second case, Dallas’ offense personnel on its “first play” from scrimmage is the result of the Cowboys returning a fumble for a touchdown on the Rams’ first offensive possession with a botched snap on the ensuing extra point attempt. Because of that, the extra point attempt is no longer scored as an extra_point in the play_type variable within the play-by-play data, but a rushing attempt. As a result of this oddity, the data is correct in listing Dallas’ first offensive play as coming out of an extra point personnel grouping.\nBoth of these examples are problematic as a team’s “starting offensive line” is considered to be just five players: the left tackle, the left guard, the center, the right guard, and the right tackle. In order to correctly determine the number of combinations used, we need to first determine the five most-commonly used offensive linemen for each team. Because of the off-the-wall situations that can occur in football, building offensive line combinations through personnel groupings in the play-by–play data is tricky, at best.\nTo avoid these situations, we can turn to the load_snap_counts() function with the nflreadr package to determine the number of unique offensive line combinations. The process to do so occurs over several steps and involves decision-making on our end on how best to accurately represent the five core offensive linemen for each team.\n\noline_snap_counts &lt;- nflreadr::load_snap_counts(seasons = 2022)\n\noline_snap_counts &lt;- oline_snap_counts %&gt;%\n  select(game_id, week, player, position, team, offense_pct) %&gt;%\n  filter(position %in% c(\"T\", \"G\", \"C\")) %&gt;%\n  group_by(game_id, team) %&gt;%\n  arrange(-offense_pct) %&gt;%\n  dplyr::slice(1:5) %&gt;%\n  ungroup()\n\noline_snap_counts\n\n# A tibble: 2,840 x 6\n   game_id          week player             position team  offense_pct\n   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;\n 1 2022_01_BAL_NYJ     1 Ben Powers         G        BAL          1   \n 2 2022_01_BAL_NYJ     1 Morgan Moses       T        BAL          1   \n 3 2022_01_BAL_NYJ     1 Kevin Zeitler      G        BAL          1   \n 4 2022_01_BAL_NYJ     1 Tyler Linderbaum   C        BAL          1   \n 5 2022_01_BAL_NYJ     1 Patrick Mekari     G        BAL          0.57\n 6 2022_01_BAL_NYJ     1 Max Mitchell       T        NYJ          1   \n 7 2022_01_BAL_NYJ     1 Laken Tomlinson    G        NYJ          1   \n 8 2022_01_BAL_NYJ     1 Alijah Vera-Tucker G        NYJ          1   \n 9 2022_01_BAL_NYJ     1 George Fant        T        NYJ          1   \n10 2022_01_BAL_NYJ     1 Connor McGovern    C        NYJ          1   \n# i 2,830 more rows\n\n\nFirst, we obtain snap count data from the 2022 season and write it into a data frame titled oline_snap_counts. After, we select just the necessary columns and then filter out the position information to include only tackles, guards, and centers. After grouping each individual offensive line by game_id and his respective team, we arrange each player’s snap count in descending order using offense_pct.\nAnd this is where a decision needs to be made on how to best construct the five starting offensive linemen for each team. By including slice(1:5), we are essentially selecting just the five offensive linemen with the most snap counts in that singular game.\nAre these five players necessarily the same five that started the game as the two tackles, two guards, and one center? Perhaps not. But, hypothetically, a team’s starting center could have been injured a series or two into the game and the second-string center played the bulk of the snaps in that game.\nBecause of such situations, we can make the argument that the five offensive line players with the highest percentage of snap counts for each unique game_id are to be considered the combination of players used most often in that game.\nNext, let’s make the decision to arrange each team’s offensive line, by game, in alphabetical order. Since we do not have a reliable way to include specific offensive line positions (that is, we have just tackle instead of left tackle or right tackle), we can build our combination numbers strictly based on the five downed linemen, regardless of specific position on the line of scrimmage.\nAfter, we use the toString() function to place all five names into a single column (starting_line) and then filter out the data to include just one game_id for the linemen.\n\noline_snap_counts &lt;- oline_snap_counts %&gt;%\n  group_by(game_id, team) %&gt;%\n  arrange(player, .by_group = TRUE)\n\noline_final_data &lt;- oline_snap_counts %&gt;%\n  group_by(game_id, week, team) %&gt;%\n  mutate(starting_line = toString(player)) %&gt;%\n  select(game_id, week, team, starting_line) %&gt;%\n  distinct(game_id, .keep_all = TRUE)\n\nThe end result includes the game_id, the week, the team abbreviation, and the starting_line column that includes the names of the five offensive line players with the highest snap count percentage for that specific game.\n\n\n# A tibble: 568 x 4\n# Groups:   game_id, week, team [568]\n   game_id          week team  starting_line                          \n   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt; &lt;chr&gt;                                  \n 1 2022_01_BAL_NYJ     1 BAL   Ben Powers, Kevin Zeitler, Morgan Mose~\n 2 2022_01_BAL_NYJ     1 NYJ   Alijah Vera-Tucker, Connor McGovern, G~\n 3 2022_01_BUF_LA      1 BUF   Dion Dawkins, Mitch Morse, Rodger Saff~\n 4 2022_01_BUF_LA      1 LA    Brian Allen, Coleman Shelton, David Ed~\n 5 2022_01_CLE_CAR     1 CAR   Austin Corbett, Brady Christensen, Ike~\n 6 2022_01_CLE_CAR     1 CLE   Ethan Pocic, James Hudson, Jedrick Wil~\n 7 2022_01_DEN_SEA     1 DEN   Cameron Fleming, Dalton Risner, Garett~\n 8 2022_01_DEN_SEA     1 SEA   Abraham Lucas, Austin Blythe, Charles ~\n 9 2022_01_GB_MIN      1 GB    Jake Hanson, Jon Runyan, Josh Myers, R~\n10 2022_01_GB_MIN      1 MIN   Brian O'Neill, Christian Darrisaw, Ed ~\n# i 558 more rows\n\n\nWith the data cleaned and prepared, we are now able to take our first look at the results. In the code below, we are grouping by all 32 NFL team and then summarizing the total number of unique offensive line combinations used during the 2022 regular season.\n\ntotal_combos &lt;- oline_final_data %&gt;%\n  group_by(team) %&gt;%\n  summarize(combos = n_distinct(starting_line)) %&gt;%\n  arrange(-combos)\n\nDespite much of the media focus being on the Rams’ poor performance, given their title of defending Super Bowl Champions, the Arizona Cardinals had nearly as many unique offensive line combinations at the conclusion of the season.\nWith the data cleaned and prepared, let’s use it to create a ggplot graph and compare the relationship between a team’s number of unique offensive lines against its winning percentage. To complete this, we first need to join the winning percentages to our existing total_combos data frame.\n\n1.3.2 Unique Offensive Line Combinations vs. Win Percentage\nTo bring in the individual winning percentages, we will use the get_nfl_standings() function from espnscrapeR and then combine the two sets of data on team abbreviations via a left_join(). Unfortunately, the team abbreviation returned from espnscrapeR does not match up with those used in the nflverse for both the Los Angeles Rams and the Washington Commanders (LAR vs. LA and WSH vs. WAS). As evidenced in the below code, correcting this issue is simple with the clean_team_abbrs() function in the nflreadr package.\n\nrecords &lt;- espnscrapeR::get_nfl_standings(season = 2022) %&gt;%\n  select(team_abb, win_pct)\n\nrecords$team_abb &lt;- nflreadr::clean_team_abbrs(records$team_abb)\n\ntotal_combos &lt;- total_combos %&gt;%\n  left_join(records, by = c(\"team\" = \"team_abb\"))\n\ntotal_combos\n\n# A tibble: 32 x 3\n   team  combos win_pct\n   &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt;\n 1 LA        13   0.294\n 2 ARI       12   0.235\n 3 CHI       10   0.176\n 4 WAS       10   0.5  \n 5 DEN        9   0.294\n 6 NO         9   0.412\n 7 NYJ        9   0.412\n 8 GB         8   0.471\n 9 TB         8   0.471\n10 BUF        7   0.812\n# i 22 more rows\n\n\nAfter collecting team records and merging them into the offensive line combination data, we can use ggplot to visualize the data. Individual team logos are used in place of the typical geom_point by using the nflplotR package.\n\n\n\n\n\n\nImportant\n\n\n\nPlease note in the below ggplot() coding that we are using a custom theme, nfl_analytics_theme(). If you wish to replicate the below visualization using the theme, run the below code to place the theme into your RStudio environment allowing you to call it within ggplot. Be sure to install and run the ggtext package as the theme uses the package’s element_markdown() function.\n\n\n\nnfl_analytics_theme &lt;- function(..., base_size = 12) {\n  \n  theme(\n    text = element_text(family = \"Roboto\",\n                        size = base_size,\n                        color = \"black\"),\n    axis.ticks = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.title = element_markdown(size = 16,\n                                  vjust = .02,\n                                  hjust = 0.5),\n    plot.subtitle = element_markdown(hjust = 0.5),\n    plot.caption = element_markdown(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major =  element_line(color = \"#d0d0d0\"),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    plot.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_blank(),\n    legend.background = element_rect(color = \"#F7F7F7\"),\n    legend.key = element_rect(color = \"#F7F7F7\"),\n    legend.title = element_text(face = \"bold\"),\n    legend.title.align = 0.5,\n    strip.text = element_text(face = \"bold\"))\n}\n\n\nggplot(data = total_combos, aes(x = combos, y = win_pct)) +\n  geom_line(stat = \"smooth\", method = \"lm\",\n            linewidth = .7, color = \"blue\",\n            alpha = 0.25) +\n  nflplotR::geom_mean_lines(aes(x0 = combos, y0 = win_pct),\n                            color = \"black\", size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  nfl_analytics_theme() +\n  scale_x_reverse(breaks = scales::pretty_breaks(n = 12)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_number(accuracy = 0.001)) +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Win Percentage\") +\n  labs(title = \"**Unique Offensive Line Combinations vs. Win Percentage**\",\n       subtitle = \"*Through Week #15*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nNumber of Unique Combos vs. Win Percentage\n\n\n\nAs can be seen in the resulting graph - which shows little, if any, statistical correlation - there are still teams with worse records than the Rams and Cardinals that have fewer unique offensive line combinations (the Houston Texans and the Chicago Bears, for example). Perhaps there is a metric that correlates more strongly with a team’s number of offensive line combinations?\nTo begin exploring that, we can hypothesize that more offensive line combinations leads to more quarterback pressures as the various member of the offensive line never have time to properly “gel.”\n\n1.3.3 Unique Offensive Line Combinations vs. Pressure Rate\nRather than calculate the data ourselves (which is the total number dropbacks divided by the total number of pressures), we can turn away from nflverse data and retrieve the information from the SIS Data Hub. After downloading the spreadsheet, we can read it into the RStudio environment using vroom and then merge the information into the existing total_combos data frame by matching on the individual team abbreviations.\n\npressure_rate &lt;- vroom(\"http://nfl-book.bradcongelio.com/pressure-rate\")\n\nteams &lt;- nflreadr::load_teams() %&gt;%\n  select(team_abbr, team_nick)\n\npressure_rate &lt;- pressure_rate %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_nick\"))\n\ntotal_combos &lt;- total_combos %&gt;%\n  left_join(pressure_rate, by = c(\"team\" = \"team_abbr\"))\n\n\n\n# A tibble: 32 x 6\n   team  combos win_pct season team.y     pressure_percent\n   &lt;chr&gt;  &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1 LA        13   0.294   2022 Rams                   34.7\n 2 ARI       12   0.235   2022 Cardinals              26.5\n 3 CHI       10   0.176   2022 Bears                  42.3\n 4 WAS       10   0.5     2022 Commanders             38.4\n 5 DEN        9   0.294   2022 Broncos                33.8\n 6 NO         9   0.412   2022 Saints                 28.2\n 7 NYJ        9   0.412   2022 Jets                   33.9\n 8 GB         8   0.471   2022 Packers                25.4\n 9 TB         8   0.471   2022 Buccaneers             19.6\n10 BUF        7   0.812   2022 Bills                  31.8\n# i 22 more rows\n\n\nWith the pressure rate now merged with our unique offensive line combination data, we can make slight adjustments to our prior ggplot code to examine any potential relationship.\n\nggplot(data = total_combos, aes(x = combos, y = pressure_percent)) +\n  geom_line(stat = \"smooth\", method = \"lm\",\n            size = .7, color = \"blue\",\n            alpha = 0.25) +\n  nflplotR::geom_mean_lines(aes(x0 = combos, y0 = pressure_percent),\n                            color = \"black\", size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks(n = 12)) +\n  scale_y_reverse(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::percent_format(scale = 1,\n                                                     accuracy = 0.1)) +\n  nfl_analytics_theme() +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Pressure Rate (per Dropback)\") +\n  labs(title = \"**Unique Offensive Line Combinations vs. Pressure Rate**\",\n       subtitle = \"*2022 Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nNumber of Unique Combos vs. Pressure Percentage\n\n\n\nAgain, there does not seem to be any statistical correlation between a team’s number of unique offensive line combinations and the pressure rate per dropback allowed. Rather than examining the pressure per dropback rate, perhaps there is a correlation between the number of unique offensive line combinations and the total number of quarterback hits allowed through the season? To determine the validity of that hypothesis, we can use data from nflreadr to collect total QB hits.\n\n1.3.4 Unique Offensive Line Combinations vs. QB Hits Allowed\nTo begin, we can collect the QB hit data from nflreadr being sure to group_by() the posteam variable in order to calculate the number of QB hits allowed by each team’s offensive line. After, we can merge the data into the total_combos data frame and produce the plot.\n\npbp &lt;- nflreadr::load_pbp(2022)\n\nqb_hits &lt;- pbp %&gt;%\n  filter(!is.na(posteam)) %&gt;%\n  group_by(posteam) %&gt;%\n  summarize(total_qb_hits = sum(qb_hit == 1, na.rm = TRUE))\n\nqb_hits_combined &lt;- left_join(\n  total_combos, qb_hits, by = c(\"team\" = \"posteam\"))\n\nAfter first filtering out any data that does not include posteam information, we can group_by() each individual offensive unit and then calculate the total sum of QB hits for each.\n\nggplot(data = qb_hits_combined, aes(x = combos, y = total_qb_hits)) +\n    geom_line(stat = \"smooth\", method = \"lm\",\n              size = .7, color = \"blue\",\n              alpha = 0.25) +\n  nflplotR::geom_mean_lines(aes(x0 = combos, y0 = total_qb_hits),\n                            color = \"black\", size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks()) +\n  scale_y_reverse(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Total QB Hits Allowed\") +\n  labs(title = \"Unique Offensive Line Combinations vs. QB Hits Allowed\",\n       subtitle = \"2022 Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nNumber of Unique Combos vs. QB Hits\n\n\n\nThere is again very little, if any, correlation between a team’s unique number of offensive line combinations and the number of times its quarterback is hit on passing attempts.\n\n1.3.5 Unique Offensive Line Combinations vs. Adjusted Sack Rate\nIn the previous examples, both pressure rate and QB hits are unadjusted metrics, meaning the results are not manipulated to account for context within the data. To add an example of a metric that is adjusted to our list of examples, we will use Adjusted Sack Rate from Football Outsiders. Aaron Schatz, in an article introducing the measurements in December of 2003, explained his findings that the league-wide sack rate varied based on both the down and yards to go.\n\n2003 NFL Sack Rate - Aaron Schatz (FootballOutsiders.com)\n\n\n\n\n\n\n\n\n\n\nYARDS TO GO\n1-4\n5-8\n9-12\n13-16\n&gt;17\n\n\n1st Down\n1.5%\n5.4%\n4.8%\n3.3%\n5.6%\n\n\n2nd Down\n5.6%\n4.7%\n5.0%\n6.1%\n4.5%\n\n\n3rd/4th Down\n5.9%\n8.2%\n10.5%\n7.6%\n11.1%\n\n\n\nAs a result of this, Schatz designed Adjusted Sack Rate which accounts for the number of pass attempts, down, distance, and - importantly - the team’s opponent. We can plot the Adjusted Sack Rate for each team from the 2022 season against the unique number of offensive line combinations.\n\nadjusted_sack_rate &lt;- vroom(\"http://nfl-book.bradcongelio.com/adj-sack-rate\")\n\n\nadjusted_sack_rate %&gt;%\n  filter(season == 2022) %&gt;%\n  ggplot(aes(x = combos, y = adj_sack_rate)) +\n  geom_line(stat = \"smooth\", method = \"lm\",\n            size = .7,\n            color = \"blue\",\n            alpha = 0.25) +\n  nflplotR::geom_mean_lines(aes(x0 = combos, y0 = adj_sack_rate),\n                            color = \"black\", size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks()) +\n  scale_y_reverse(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format(scale = 1)) +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Adjusted Sack Rate\") +\n  labs(title = \"**Unique Offensive Line Combinations vs.\n       Adjusted Sack Rate**\",\n       subtitle = \"*2022 Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  nfl_analytics_theme()\n\n\n\nNumber of Unique Combos vs. Adjusted Sack Rate\n\n\n\nThe graph shows that, in general, those teams with higher numbers of unique offensive line combinations trend towards the lower-left quadrant and some - the Rams and Bears - are significantly below the league average, indicating a worse adjusted sack rate (with the Arizona Cardinals being an outlier). Compared to the prior metrics (pressure rate and QB hits), it seems that adjusted sack rate may be impacted by the consistency of a team’s offensive linemen.\nUsing the adjusted_sack_rate data, we can easily visualize the same results over the last three NFL seasons to see if there is any consistency between the offensive line combinations and a team’s adjusted sack rate.\n\nadjusted_sack_rate %&gt;%\n  ggplot(aes(x = combos, y = adj_sack_rate)) +\n  geom_line(stat = \"smooth\", method = \"lm\",\n            size = .7,\n            color = \"blue\",\n            alpha = 0.25) +\n  nflplotR::geom_mean_lines(aes(x0 = combos, y0 = adj_sack_rate),\n                            color = \"black\", size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks()) +\n  scale_y_reverse(breaks = scales::pretty_breaks(),\n                  labels = scales::percent_format(scale = 1)) +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Adjusted Sack Rate\") +\n  labs(title = \"**Unique Offensive Line Combinations vs.\n       Adjusted Sack Rate**\",\n       subtitle = \"*2020 - 2022 Seasons*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  nfl_analytics_theme() +\n  facet_wrap(~season, nrow = 2)\n\n\n\nNumber of Unique Combos vs. Adj. Sack Rate per Season\n\n\n\nGiven the result of just the 2022 season, the output for the 2021 season remains consistent, perhaps even more so with less spread from the line of best fit. In 2021, the Carolina Panthers had the most unique offensive line combinations as well as one of the worst adjusted sack rates in the league, while those teams with fewer combinations generally trend towards the upper-right quadrant of the plot.\nHowever, the consistency between the 2022 and 2021 seasons is not found in the 2020 season. The Philadelphia Eagles had the most unique combinations in 2020 (13 in total), but were all but tied with the Houston Texans for worst adjusted sack rate (9.4% to 9.5%). Moreover, the Pittsburgh Steelers, Buffalo Bills, and Indianapolis Colts - despite all having more than 10 unique offensive line combinations - are among the best in adjusted sack rate.\nWhy is there this sudden departure from the consistency in 2022 and 2021?\nThe answer is within the context of data, in that Ben Roethlisberger - the long-time quarterback for the Steelers - had one of the fastest Time to Throw scores in modern NFL history, getting rid of the ball, on average, in just 2.19 seconds after the snap. Philip Rivers, of the Colts, was nearly as quick to release the ball with an average of 2.39 seconds. It proved to be quite difficult for opposing defenses to get to either Roethlisberger or Rivers given these incredibly quick snap-to-release times.\nIt should be noted that the same contextual explanation does not hold true for Buffalo. In the 2020 season, Josh Allen released the ball, on average, 3.04 seconds after the snap which is plenty of time for the opposing defense to generate pressure and record sacks. Because of this, if we were to add statistical weight using average time to throw, the Buffalo Bills would remain an outlier in this specific season while both the Steelers and Colts would likely regress closer to the mean for those teams with higher numbers of offensive line combinations.\nWhile pressure rate and QB hits did not prove to impacted by a team’s number of line combinations, there is a broad relationship (though not strong enough to begin using the term “correlation”) between the number of combinations and adjusted sack rate.\nTo continue this exploration, let’s pivot away from exploring the impact of offensive line combinations on the quarterback and examine any potential correlation with the running game and run blocking, while using metrics that are adjusted, as the additional context and weighting seemed to be helpful in finding relationships. We can gather statistics provided by Football Outsiders concerning the performance of offensive lines.\n\n1.3.6 Unique Offensive Line Combinations vs. Adjusted Line Yards\n\n\n\nAdjusted Line Yards - derived by Football Outsiders with a regression analysis, this metric takes the total of a team’s total rushing attempts and assigns a quantitative value to the offensive line’s impact. Any run that ends with a loss of yards results in 120% of the value being placed on the offensive line, while a 0-4 yard rush is 100%, a 5-10 yard rush is 50%, and anything over 11 yards is a 0% value. As explained by Football Outsiders, if a running back breaks free for a 50-yard gain, just how much of that is the offensive line responsible for? Adjusted Line Yards produces a numeric value as the answer to that question.\n\nPower Rank - a ranking between 1 (best) and 32 (worst), power rank is the result of the power success metric, which is the percentage of rushing attempts on 3rd or 4th down, with two or less yards to go, that resulted in either a 1st down or a touchdown.\n\nStuffed Rank - again provided in the data frame as a ranking between 1 and 32, stuffed rank is the percentage of rushing attempts where the running back was tackled at, or behind, the line of scrimmage.\n\nTo begin, we can gather the necessary data into a data frame called fb_outsiders_oline.\n\nfb_outsiders_oline &lt;- vroom(\"http://nfl-book.bradcongelio.com/fbo_oline\")\n\nfb_outsiders_oline\n\n# A tibble: 32 x 5\n   team  combos adj_line_yards power_rank stuffed_rank\n   &lt;chr&gt;  &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n 1 LV         7           4.93         23            6\n 2 GB         8           4.85         18            2\n 3 KC         5           4.82         31            9\n 4 SF         6           4.7          26           22\n 5 ATL        4           4.68         17            5\n 6 PHI        5           4.66          7            7\n 7 DET        7           4.66         20           18\n 8 MIA        7           4.61         30           14\n 9 CAR        3           4.56         14           11\n10 PIT        2           4.54          1            4\n# i 22 more rows\n\n\nYou can see in the output of the data that power_rank and stuffed_rank are both in a “ranked format,” meaning the team with the best power success score (the Pittsburgh Steelers) are ranked 1st while the team with the worst (the Minnesota Vikings) are ranked 32nd. The adj_line_yards variable is provided in its unranked, raw format. Because of this, let’s first use both power_rank and stuffed_rank and use the cowplot package to view them together, and then construct the plot for adj_line_yards separately since it operates on a differing y-axis scale.\n\npower_rank_plot &lt;- ggplot(fb_outsiders_oline,\n                          aes(x = combos, y = power_rank)) +\n  nflplotR::geom_mean_lines(aes(x0 = combos,\n                                y0 = power_rank),\n                            color = \"black\",\n                            size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks()) +\n  scale_y_reverse(breaks = seq(1, 32, 2)) +\n  labs(title = \"**Unique Offensive Line Combinations vs.\n       Power Ranking**\",\n       subtitle = \"*2022 Season  |  FootballOutsiders.com*\") +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Power Ranking (1 = best, 32 = worst)\") +\n  nfl_analytics_theme()\n\nstuffed_rank_plot &lt;- ggplot(fb_outsiders_oline,\n                            aes(x = combos, y = stuffed_rank)) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.8, alpha = .08) +\n  nflplotR::geom_mean_lines(aes(x0 = combos,\n                                y0 = stuffed_rank),\n                            color = \"black\",\n                            size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks()) +\n  scale_y_reverse(breaks = seq(1, 32, 2)) +\n  labs(title = \"**Unique Offensive Line Combinations vs.\n       Stuffed Rank**\",\n       subtitle = \"*2022 Season  |  FootballOutsiders.com*\")+\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Stuffed Ranking (1 = best, 32 = worst)\") +\n  nfl_analytics_theme()\n\nplot_grid(power_rank_plot, stuffed_rank_plot, ncol = 1)\n\n\n\nNumber of Unique Combos vs. Adj. Sack Rate per Season\n\n\n\nBased on the results, it does not seem that the number of unique offensive line combinations has impact on either power rank or stuffed rank. The Los Angeles Rams and Arizona Cardinals, despite have the two highest unique line combinations, are among the best in the league in the metric. In fact, the opposite seems to be true in that the Minnesota Vikings, with just six unique line combinations during the 2022 season, are last in the league in power ranking. The Rams performed in bit worse when comparing line combinations to stuffed rank, but the Cardinals are still among the best in the league. The Vikings again are at the bottom, only under performed by the Jacksonville Jaguars (who had just three unique combinations throughout the season).\n\nggplot(fb_outsiders_oline, aes(x = combos, y = adj_line_yards)) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.8, alpha = .08) +\n  nflplotR::geom_mean_lines(aes(x0 = combos, y0 = adj_line_yards),\n                            color = \"black\", size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  labs(title = \"**Unique Offensive Line Combinations vs.\n       Adjusted Line Yards**\",\n       subtitle = \"*2022 Season  |  FootballOutsiders.com*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Adjusted Line Yards\") +\n  nfl_analytics_theme()\n\n\n\nNumber of Unique Combos vs. Adjusted Line Yards\n\n\n\nComparing a team’s unique offensive line combinations to adjusted line yards is more promising than power and stuffed rank, as the teams with the highest amount of combinations begin to fall under the league-average line for ALY. There are still several teams that, despite having a low number of combinations through the season, performed poorly such as the Houston Texans, Seattle Seahawks, and the Jaguars.\nSo what do we make of this?\nGiven that adjusted line yards is constructed to measure the responsibility of the offensive line, it is logical to also want to consider the “performance” of the running back. A running back that is able to avoid early hits, or break tackles and earn yards after contact, can overcome an under performing offensive line (whether that be from lack of talent or lack of cohesiveness from a high number of combinations). In the description of the adjusted line yards metrics, Football Outsiders confirms this, writing that a “team with a very good running back will appear higher no matter how bad their line, and a team with a great line will appear lower if the running back is terrible.”\nFor the last exploration in this topic, we can bring in Elusive Rating, which is a signature statistic from Pro Football Focus that quantifies the “success and impact of a runner with the ball independently of the blocking. Let’s bring in the data that contains all of the information from fb_outsiders_oline, but with the addition of an averaged elusive rating for each team from PFF.\n\noline_combo_elusive &lt;- vroom(\"http://nfl-book.bradcongelio.com/ol-elusive\")\n\nBecause we want to determine if the elusiveness of a team’s stable of running backs directly impacts the offensive line’s Adjusted Line Yards, we can create a new variable that use the team’s elusive_rating to, in an elementary fashion, add weight to the adj_line_yards. While there are numerous approaches to doing so (such as ranking, categorical weighting, logarithmic weighting, using coefficients from a regression model, etc.), we will conduct a more simple type of feature engineering by first using the scale() package to standardize the values, resulting in each team receiving what is called a ‘z-score’ (or the total standard deviations away from the mean). After, we will create the weighted_aly metric by subtracting each team’s z-score from the existing adj_line_yards, and then plot the results.\n\noline_combo_elusive &lt;- oline_combo_elusive %&gt;%\n  mutate(adjusted_elu = scale(avg_elu),\n         weighted_aly = adj_line_yards - adjusted_elu)\n\nonline_combo_test &lt;- oline_combo_elusive %&gt;%\n  mutate(weighted_aly = adj_line_yards - adjusted_elu)\n\nggplot(oline_combo_elusive, aes(x = combos, y = weighted_aly)) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.8, alpha = .08) +\n  nflplotR::geom_mean_lines(aes(x0 = combos, y0 = weighted_aly),\n                            color = \"black\", size = .8) +\n  nflplotR::geom_nfl_logos(aes(team_abbr = team), width = 0.065) +\n  scale_x_reverse(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  labs(title = \"**Unique Offensive Line Combinations vs.\n       Weighted Adjusted Line Yards**\",\n       subtitle = \"*2022 Season  |  Adjusted with PFF's 'Elusive'*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")+\n  xlab(\"# of Unique Offensive Line Combinations\") +\n  ylab(\"Weighted Adjusted Line Yards\") +\n  nfl_analytics_theme()\n\n\n\nNumber of Unique Combos vs. Weighted Elusiveness\n\n\n\nDespite having two of the bottom four scores in elusive_rating (21.4 and 21.8), the Rams and the Cardinals are in the league’s top five for Weighted Adjusted Line Yard. This is an indication that each team’s offensive line is proficient at run blocking and that any perceived shortcoming in the ground game was the result of a lack of elusive running backs. Conversely, the Washington Commanders unweighted Adjusted Line Yards was 4.29 but, when account for the high elusiveness score among the team’s running backs, the metric dipped to a league-worst 2.14 weighted adjusted line yards.\nWhile there is certainly more examination that could be conducted, these preliminary results seem to indicate that a higher number of unique offensive line combinations has little impact on both the passing and rushing games, with the exception of a general relationship with the number of combinations and a team’s adjusted sack rate. Even then, there are caveats within the data that force us to consider further ways to contextualize the data such as a quarterback’s average time to release."
  },
  {
    "objectID": "01-nfl-analytics-and-r.html#exploring-explosive-plays-per-touchdown-drive",
    "href": "01-nfl-analytics-and-r.html#exploring-explosive-plays-per-touchdown-drive",
    "title": "\n1  An Introduction to NFL Analytics and the R Programming Language\n",
    "section": "\n1.4 Exploring Explosive Plays per Touchdown Drive",
    "text": "1.4 Exploring Explosive Plays per Touchdown Drive\nIn November of 2022, Warren Sharp - of Sharp Football Analysis - tweeted perhaps the most ridiculous statistic of the 2022 NFL season.\n\n\nHeading into their week 9 bye week, the Steelers’ longest play resulting in a touchdown was just eight yards, or less than what is required for a 1st down. Perhaps making it worse, the touchdown came early in the season, on a pass from Mitch Trubisky to Pat Freiermuth in a week 2 contest against the New England Patriots.\nHowever, the statistic as presented by Sharp does not tell the complete story as it isolates the entirety of a touchdown drive to a single play. While the team’s longest touchdown scoring play was just eight yards, this does not mean that there was a lack of explosive, high-yardage plays earlier in the drive that helped the Steelers get closer to the endzone.\nTo determine if this is the case, we can construct a metric that explore explosive plays per touchdown drive during the 2022 season.\n\npbp &lt;- nflreadr::load_pbp(2022)\n\nexplosive &lt;- pbp %&gt;%\n  filter(!is.na(posteam) &\n           !is.na(yards_gained)\n         & fixed_drive_result == \"Touchdown\") %&gt;%\n  filter(special == 0 & fumble == 0 & interception == 0) %&gt;%\n  group_by(posteam, game_id, drive) %&gt;%\n  summarize(max_yards = max(yards_gained)) %&gt;%\n  mutate(explosive_play = if_else(max_yards &gt;= 20, 1, 0)) %&gt;%\n  ungroup() %&gt;%\n  group_by(posteam) %&gt;%\n  summarize(tds_no_explosive = sum(explosive_play == 0),\n            tds_explosive = sum(explosive_play == 1),\n            total_drives = sum(tds_no_explosive + tds_explosive),\n            percent_no_exp = tds_no_explosive / total_drives,\n            percent_w_exp = tds_explosive / total_drives) %&gt;%\n  select(posteam, percent_w_exp, percent_no_exp)\n\nAfter collecting the play-by-play information for the 2022 NFL season, we use filter() to gather only those fixed_drive_result rows that include Touchdown and then remove any play that is a special teams play and includes a fumble or interception. This is done as we only want those touchdowns scored by the offense, not a fumble or interception return for a score. Because of this, the total touchdowns result is lower than the “official” number from Pro Football Reference, as the site’s “Touchdown Log” includes this data.\nWe then use group_by() on posteam, game_id, and drive in order to create the column max_yards with summarize(). The resulting numeric value in max_yards represents the largest amount of yards gained on a single play, per drive. The important explosive_play metric is returned, in a binary 1 or 0 format, based on whether or not the max_yards in each drive was greater than or equal to 20 yards. With this calculation complete, we ungroup() the data and then group it only by posteam to determine the results. After finding the percent of drives with an explosive play and those without, we can plot the results.\n\nggplot(explosive, aes(y = reorder(posteam, percent_w_exp),\n                      x = percent_w_exp)) +\n  geom_col(aes(color = posteam, fill = posteam), width = 0.5) +\n  nflplotR::scale_color_nfl(type = \"secondary\") +\n  nflplotR::scale_fill_nfl(alpha = 0.5) +\n  nfl_analytics_theme() +\n  theme(axis.text.y = element_nfl_logo(size = .65)) +\n  scale_x_continuous(expand = c(0,0),\n                     breaks = scales::pretty_breaks(n = 5),\n                     label = scales::percent_format()) +\n  labs(title = \"**Which Team Has The Highest % of\n       Explosive Plays per TD Drive?**\",\n       subtitle = \"*2022 Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"Percent of TD Drives with an Explosive Play (20+ Yards)\") +\n  ylab(\"\")\n\n\n\nExplosive Plays per Touchdown Drive\n\n\n\nThe Carolina Panthers had a play that gained 20 or more yards on nearly 82% of their touchdown drives, while the Indianapolis Colts had a league-worst 52% of touchdown drives including an explosive plays. Notably, the Steelers are the near bottom as well with result of just under 61%. Given the statistic in Sharp’s tweet, we can make an assumption that Pittsburgh (as well as other the other teams with lower percentages) methodically “marched down” the field on the way to scoring touchdowns while teams such as the Panthers and Cleveland Browns heavily relied on at least one big “homerun” play per touchdown drive in order to move the ball\n\n\n\n\nKirschner, A. (2022). The rams’ super bowl afterparty turned into a historic hangover. Retrieved from https://fivethirtyeight.com/features/the-rams-super-bowl-afterparty-turned-into-a-historic-hangover/"
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#downloading-r-and-rstudio",
    "href": "02-nfl-analytics-tidyverse.html#downloading-r-and-rstudio",
    "title": "\n2  Wrangling NFL Data in the tidyverse\n",
    "section": "\n2.1 Downloading R and RStudio",
    "text": "2.1 Downloading R and RStudio\nPrior to downloading R and RStudio, it is important to explain the difference between the two, as they are separate pieces of our analytics puzzle that are used for differing purposes. R is the core programming language used for statistical computing and graphics. R provides a wide range of statistical and graphical techniques and has a large community of users who develop and maintain a multitude of packages (essentially libraries of pre-written functions) that extend the capabilities and ease of coding. While R can be run from your computer’s command line, it also has an integrated development environment (IDE) in RStudio that provides a graphical user interface for working with R scripts, data files, and packages.\nRStudio is free to download and use and provides a user-friendly interface for writing R code, organizing projects and files, and working with both big and small data. Regularly updated by the Posit team, RStudio includes many features that are specifically designed to making working with R easier, including syntax highlighting, code suggestions, robust debugging tools, and a built-in package manager.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important that you download and successfully install R before proceeding to install RStudio on your computer.\n\n\n\n2.1.1 Downloading R\n\nTo download R, visit the official R website at https://www.r-project.org/\n\nClick on the ‘CRAN’ link at the top of the page, directly underneath the word ‘Download.’ This will bring you to the Comprehensive R Archive Network.\nEach mirror that hosts a copy of R is sorted by country. Select a mirror that is geographically close to you.\nClick on the version of R that is appropriate for your operating system (Linux, macOS, or Windows).\nSelect the ‘base’ option to download an updated version of R to your computer.\nOpen the downloaded file and follow the provided installation instructions.\n\n2.1.2 Downloading RStudio\n\nTo download RStudio, visit the official RStudio website at https://posit.co\n\nClick on the ‘Download RStudio’ button in the top-right of the page.\nScroll down and select ‘Download’ within the ‘RStudio Desktop - Free’ box.\nOpen the downloaded file and follow the provided installation instructions.\n\n2.1.3 The Layout of RStudio\nWhen you open RStudio for the first time, you will see the interface laid out as in the picture below (sourced from the official RStudio documentation from posit).\n\n\n\n\nRStudio Layout\n\n\n\nAs you can see, RStudio provides a graphical interface for working with R and is sorted into four main panes, each of which serves a specific purpose.\n\n\nThe source pane: In the upper-left, the source pane is where you write and run your R code. It serves as the main working pane within RStudio.\n\nThe console pane: the console pane serves multiple functions, including allowing you to interact directly with R by typing commands and receiving the output. Additionally, any errors outputted by code ran in the source pane will be detailed in the console, allowing you to troubleshoot and debug.\n\nEnvironment and history pane: this pane, in the upper-right, displays information about the current R environment and command history. Perhaps more important, it displays the information of all the R-created objects currently stored in your computer’s memory including data sets, vectors, lists, etc.\n\nFiles, plots, packages, and help pane: in the bottom-right, this pane provides access to numerous RStudio tools and resources including the ability to browse and navigate through the files and folders on your computer and view the output of plots and graphics. As well, the ‘Packages’ tab gives you the ability to manage any R packages that you have installed on your system and to view each packages’ help documentation.\n\n2.1.4 Running Code in RStudio\nTo begin writing code in RStudio you first need to open a new R script. To do so, select ‘File -&gt; New File -&gt; R Script.’ A new script tab, titled Untitled 1 will open in your RStudio’s source pane. To run a line of code - or multiple lines of code - you can do one of two options:\n\nPlace your cursor directly at the end of the last line of code, or highlight all the code you wish to run, and press ‘Ctrl + Enter’ (Windows) or ‘Command + Enter’ (Mac).\nPlace your cursor directly at the end of the last line of code, or highlight all the code you wish to run, and then use your mouse to click the ‘Run’ button in the source pane’s toolbar.\n\nAs a working example, let’s do a simple addition problem within the the source pane:\n\n2 + 2\n\nAfter following one of the two above options for running the addition problem, the output in your console should appear like below:\n\n\n\n\nExample RStudio Console Output\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt is important to notice the &gt; after the output in the console. That indicates that the coding process has completely run and RStudio is ready to run the next task that you submit. If you have an error in your code, you will notice that the &gt; is replaced by what my students refer to as the ‘hanging plus sign’, +.\nYou receive the + sign when RStudio is expecting a continuation of the code. Several issues can cause this to happen, including forgetting to provide an equal number of opening and closing parenthesis or mistakenly including a pipe, %&gt;%, after the last line of code.\nIn any case, when you see the + in the console, simply use your mouse to click within the console and hit your keyboard’s escape key. Doing so exits the prompt and resets the console to include the &gt; symbol."
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#installing-and-loading-necessary-packages",
    "href": "02-nfl-analytics-tidyverse.html#installing-and-loading-necessary-packages",
    "title": "\n2  Wrangling NFL Data in the tidyverse\n",
    "section": "\n2.2 Installing and Loading Necessary Packages",
    "text": "2.2 Installing and Loading Necessary Packages\nInstalling and loading packages is an important part of working with RStudio, as they provide additional functionality that allow you to more efficiently conduct data analysis and/or modeling. Several packages are widely used through this book, including the ever-important tidyverse, the nflverse family of packages to retrieve NFL statistics and data, and many others such as tidymodels when we tackle building and testing advanced models in Chapter 5.\nTo begin, let’s install both tidyverse and nflverse. In your source pane, you can install a package by using the install.packages() function. To complete the code, you simply supply the package name within quotation marks.\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"nflverse\")\n\nAfter running the code, your console pane will output what is going on “behind the scenes.” When complete, you will again see the &gt; symbol within the console. At this point, you are able to load the packages you just installed.\n\nlibrary(tidyverse)\nlibrary(nflverse)"
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#the-tidyverse-and-its-verbs",
    "href": "02-nfl-analytics-tidyverse.html#the-tidyverse-and-its-verbs",
    "title": "\n2  Wrangling NFL Data in the tidyverse\n",
    "section": "\n2.3 The tidyverse and Its Verbs",
    "text": "2.3 The tidyverse and Its Verbs\nThe tidyverse, now installed and loaded within RStudio, is a collection of R packages designed for data manipulation, visualization, and analysis. It was developed by Hadley Wickham, the Chief Scientist at Posit, and a varied team of contributors. The goal of the tidyverse is to provide a consistent, easy-to-understand set of functions and syntax for working with data in R.\nThe core principle of the tidyverse is “tidy data,” which is the development team’s belief in creating a standard way of organizing data sets. To that end, a “tidy” data set is one that is comprised of observations (rows) and variables (columns) with each variable being a distinct piece of information and each observation being a unit of analysis.\nInstalling and loading the tidyverse results eight of the core packages automatically being loaded and ready to use:\n\n\ndplyr: “dplyr provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges.”\n\ntidyr: “tidyr provides a set of functions that help you get to tidy data. Tidy data is data with a consistent form: in brief, every variable goes in a column, and every column is a variable.”\n\nreadr: “readr provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is deigned to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes.”\n\npurrr: “purrr enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. Once you master the basic concepts, purrr allows you to replace many for loops with code that is easier to write and more expressive.”\n\ntibble: “tibble is a modern re-imagining of the data frame, keeping what time has proven to be effective, and throwing out what it has not. Tibbles are data.frames that are lazy and surly; they do less and complain more forcing you to confront problems earlier, typically leading to cleaner, more expressive code.”\n\nstringr: “stringr provides a cohesive set of functions designed to make working with strings as easy as possible. It is built on top of stringi, which uses the ICU C library to provide fast, correct implementations of common string manipulations.”\n\nforcats: “forcats provides a suite of useful tools that solve common problems with factors. R uses factors to handle categorical variables, variables that have a fixed and known set of possible values.”\n\nggplot2: “ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map the variables to aesthetics, what graphical primitives to use, and it takes care of the details” (Wickham, 2022).\n\nAside from the core eight packages, the tidyverse will also install a multiple of other packages such as rvest (for web scraping), readxl (for reading Excel sheets in the RStudio environment), lubridate (a very powerful tool for working with times and dates), and magrittr (the package that provides the pipe %&gt;%). As well, prior versions of the tidyverse utilized the modelr package. Modeling is now handled in the tidyverse by the tidymodels package."
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#the-flow-of-the-tidyverse",
    "href": "02-nfl-analytics-tidyverse.html#the-flow-of-the-tidyverse",
    "title": "\n2  Wrangling NFL Data in the tidyverse\n",
    "section": "\n2.4 The Flow of the tidyverse\n",
    "text": "2.4 The Flow of the tidyverse\n\nThe underlying design of coding in the tidyverse, aside from the dplyr verbs, are both the assignment statement (&lt;-) and the pipe (%&gt;%). Please note, as mentioned in the book’s Preface, that I still use the pipe (%&gt;%) that is part of the magrittr package and not the native pipe operator (|&gt;) included in the 4.1 release of R. The choice of pipe operator you use is your decision to make, as either will work seamlessly within the examples and activities provided in this book.\nAs I explain to my Sports Analytics students, the language and flow of the tidyverse can seem like a foreign language at first. But, it is important that you stick with it because, sooner rather than later, the light bulb above your head will go off. Before detailing the in’s and out’s of the tidyverse in the below section, let’s first dissect an example of the tidyverse workflow.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(posteam == \"PHI\" & rush == 1) %&gt;%\n  group_by(rusher) %&gt;%\n  summarize(success_rate = mean(success))\n\nThe given example involves multiple iterations of the tidyverse paradigm. At the outset of my Sport Analytics course, when introducing the concepts of the tidyverse, I emphasize that it is possible to “talk your way through” the process from the beginning to your end goal (especially once you have you have a more comprehensive understanding of the dplyr verbs, which are expounded upon in the subsequent section). The following stepwise method illustrates this using the above example of code:\n\nWe first create a data set, denoted by pbp, by utilizing the load_pbp function from the nflreadr package. To talk through this, you can say “pbpis an copy of nflreadr::load_pbp(2022).” While R purists may laugh at teaching the tidyverse in such language, it does indeed work. Going forward, the assignment operator (&lt;-) simply implies that “something is.” In this case, our pbp data frame is the container for the play-by-play data we are collecting from nflreadr.\nWe then move into our first pipe operator (%&gt;%). Again, R language purist will likely develop a eye twitch upon reading this, but I explain to my students that the pipe operator serves as a “… and then” command. In terms of the “talk it out” method above, the flow would be: “pbp is a copy of nflreadr::load_pbp(2022) and then …”\nAfter the pipe operator (or the first “and then …” command), we move into our first dplyr verb. In this case, we are using the filter() verb to select just the Philadelphia Eagles as the offensive team and just offensive plays that are rush attempts. With another pipe operator, we are including a second “… and then” command.\nTo finish the example, we are grouping by each individual rusher on the Eagles “and then” summarize the average success rate for each rusher.\n\nTo put it together, “talking it out” from beginning to end results in:\n“First, create a data set called pbp that is a copy of nflreadr::load_pbp(2022) and then filter() for all instances where the posteam is PHI and rush == 1 and then group_by() each individual rusher, and then summarize the average success rate for each rusher into a new column titled success_rate.”\nTo showcase this visually, the “talking through” method is inputted into the example code below:\n\npbp &lt;- \"is\" nflreadr::load_pbp(2022) %&gt;% \"... and then\"\n  filter(posteam == \"PHI\" & rush == 1) %&gt;% \"... and then\"\n  group_by(rusher) %&gt;% \"... and then\"\n  summarize(success_rate = mean(success))"
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#working-with-nfl-data-and-the-dplyr-verbs",
    "href": "02-nfl-analytics-tidyverse.html#working-with-nfl-data-and-the-dplyr-verbs",
    "title": "\n2  Wrangling NFL Data in the tidyverse\n",
    "section": "\n2.5 Working With NFL Data and the dplyr Verbs",
    "text": "2.5 Working With NFL Data and the dplyr Verbs\nOf the packages nestled within the tidyverse, dplyr is perhaps the most important in terms of wrangling and cleaning data. As mentioned above, dplyr is a powerful tool for data manipulation in R as it provides a key set of functions, known as verbs, that are designed to be easy to use and understand. The verbs can be used to filter, group, summarize, rearrange, and transform all types of data sets. For those just starting their NFL analytics endeavors in the R programming language, the following four dplyr verbs are perhaps the most important. Specific examples of working with these verbs, as well as others, follow below.\n\n\nfilter(): the filter() verb allows you to subset data based on certain criteria. For example, you can use filter() to keep only those rows in a data set where a certain variable meets a certain conditions (ie., more than 100 completed passes). Moreover, the filter() verb can be used in conjunction with logical operators such as & and | to create more complex criteria.\n\ngroup_by(): the group_by() verb allows you to group a data set by one or more variables. It is a useful tool when you want to perform an operation on each group, such as calculating a summary statistic (ie, intended air yards per quarterback) or when creating a plot.\n\nsummarize(): the summarize() verb allows you to reduce a data set to a single summary value. The summarize() verb is often used in conjunction with the group_by() function, allowing you to group the data by one or more variables. The summarize() verb allows for a wide range of summary statistics, including means, medians, standard deviations, and more. You can also use it to calculate custom summary statistics.\n\nmutate(): the mutate() verbs allows you to create new variables within your data while also preserving existing ones.\n\n\n2.5.1 NFL Data and the filter() verb\nThe filter() verb allows you to extract specific rows from your dataset based on one, or multiple, supplied conditions. The conditions are supplied to the filter() verb by using logical operators, listed in the below table, that ultimately evaluate to either TRUE or FALSE in each row of the dataset. The filter() process returns a data set that includes only those rows that meet the specified conditions.\n\n\n\n\nLogical Operator\nMeaning\n\n\n\n==\nequal to\n\n\n!=\nnot equal to\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n!\nnot\n\n\n&\nand\n\n\n|\nor\n\n\n%in%\nincludes\n\n\nc()\nused to combine arguments into a vector\n\n\nis.na\nchecks for missing values\n\n\n!is.na\nis not missing specific values\n\n\n\n\nIn order to work through specific examples of the above logical operators, we will use 2022 play-by-play data from nflreadr. To begin, let’s read in the data:\n\n\n\n\n\n\nNote\n\n\n\nPlease note that a more detailed overview of reading in nflreadr data is provided in Chapter 3. For the purposes of learning about the filter() verb, please make sure you have both the tidyverse and nflreadr loaded by running the following:\nlibrary(tidyverse)library(nflreadr)\nIf you have difficult with the above step, please see the Installing and Loading Necessary Packages section above.\n\n\n\npbp &lt;- nflreadr::load_pbp(2022)\n\nAfter running the above code, you will have a data set titled pbp placed into your RStudio environment consisting of 50,147 observations over 372 variables. With the data set loaded, let’s create a new data set titled ne_offense that contains only those plays where the New England Patriots are the offensive team.\n\nne_offense &lt;- pbp %&gt;%\n  filter(posteam == \"NE\")\n\nne_offense\n\n# A tibble: 1,323 x 372\n   play_id game_id   old_game_id home_team away_team season_type  week\n     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1      44 2022_01_~ 2022091106  MIA       NE        REG             1\n 2      59 2022_01_~ 2022091106  MIA       NE        REG             1\n 3      83 2022_01_~ 2022091106  MIA       NE        REG             1\n 4     109 2022_01_~ 2022091106  MIA       NE        REG             1\n 5     130 2022_01_~ 2022091106  MIA       NE        REG             1\n 6     154 2022_01_~ 2022091106  MIA       NE        REG             1\n 7     175 2022_01_~ 2022091106  MIA       NE        REG             1\n 8     196 2022_01_~ 2022091106  MIA       NE        REG             1\n 9     236 2022_01_~ 2022091106  MIA       NE        REG             1\n10     571 2022_01_~ 2022091106  MIA       NE        REG             1\n# i 1,313 more rows\n# i 365 more variables: posteam &lt;chr&gt;, posteam_type &lt;chr&gt;, ...\n\n\nThe output shows that the posteam variable contains only NE, which is the abbreviation for the New England Patriots in the nflreadr play-by-play data. In the code that produced ne_offense, it is important to notice that:\n\nthe “equal to” logical operator consist of TWO equal signs, not one.\nthe team abbreviation (NE) is in quotation marks.\n\nIn the first, a single equal sign (=) can be used as a synonym for assignment (&lt;-) but is most often used when passing values into functions. To avoid confusion with this possibility, the test for equality when using filter() is always ==.\nIn the second, you must use quotation marks around the character-based value you are placing into the filter() verb because, if not, R will interpret posteam incorrectly and ultimately generate an error. On the other hand, you do not need to include quotation marks if you are filtering out numeric-based variables. Below are incorrect and correct examples of both:\n\nne_offense &lt;- pbp %&gt;%\n  filter(posteam == NE) #this is incorrect.\n                        ##Character values must be in quotation marks.\n\nne_offense &lt;- pbp %&gt;%\n  filter(posteam == \"NE\") #this is correct for character values.\n\nne_offense &lt;- pbp %&gt;%\n  filter(air_yards &gt;= \"5\") #this is incorrect.\n                           #Numeric values do not need quotation marks.\n\nne_offense &lt;- pbp %&gt;%\n  filter(air_yards &gt;= 5) #this is correct for numeric values.\n\nHow do we approach the logical operators if we want to retrieve every offensive team in the pbp data except for New England? In that case, we can use the “not equal to” (!=) operator:\n\nnot_ne_offense &lt;- pbp %&gt;%\n  filter(posteam != \"NE\")\n\nThe resulting data set titled not_ne_offense will still include all 372 variables housed within the nflreadr play-by-play data, but will not include any row in which New England is the offensive (posteam) team.\nContinuing with examples, how do we use the filter() verb on multiple teams at once? For instance, let’s use the above filtering process for offensive teams but only retrieve information from the play-by-play data for the four teams that comprise the AFC East (Buffalo Bills, Miami Dolphins, New England Patriots, and the New York Jets). There are, in fact, two logical operators that can produce the results we are looking for: the “or” logical operator (|) or by using the “includes” logical operator (%in%) combined with the “concatenate” operator (c()). Let’s start with using the “or” operator.\n\nafc_east &lt;- pbp %&gt;%\n  filter(posteam == \"NE\" | posteam == \"MIA\"\n         | posteam == \"NYJ\" | posteam == \"BUF\")\n\nBy using the | logical operator, which translates to the word “or”, we can string together for separate filters for posteam within the play-by-platy data. That said, it probably seems odd to have to include the posteam argument four different times, rather than being able to do this:\n\nafc_east &lt;- pbp %&gt;%\n  filter(posteam == \"NE\" | \"MIA\" | \"NYJ\" | \"BUF\")\n\nWhile the above example logically makes sense (verbally saying “posteam equals NE or MIA or NYJ or BUF”), it unfortunately results in an error. To that end, if you’d rather avoid the need to type posteam four different times, as in the above example, you can switch to using the %in% operator combined with c(). It is possible to combine just filter() and the %in% operator to retrieve one specific team. But, as in the above example where, we will receive an error if we try to do it for multiple teams without including the c() operator, as such:\n\nafc_east &lt;- pbp %&gt;%\n  filter(posteam %in% c(\"NE\", \"MIA\", \"NYJ\", \"BUF\"))\n\nafc_east\n\n# A tibble: 5,625 x 372\n   play_id game_id   old_game_id home_team away_team season_type  week\n     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;       &lt;int&gt;\n 1      43 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 2      68 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 3      89 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 4     115 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 5     136 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 6     172 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 7     391 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 8     412 2022_01_~ 2022091107  NYJ       BAL       REG             1\n 9     436 2022_01_~ 2022091107  NYJ       BAL       REG             1\n10     469 2022_01_~ 2022091107  NYJ       BAL       REG             1\n# i 5,615 more rows\n# i 365 more variables: posteam &lt;chr&gt;, posteam_type &lt;chr&gt;, ...\n\n\nThe above approach is a simplified version of first creating a vector of the team abbreviations and then passing that into the filter() argument. For example, we can create the following vector that appears in the “Values” area of your RStudio environment, and then use that to retrieve the same results in did in the above code:\n\nafc_east_teams_vector &lt;- c(\"NE\", \"MIA\", \"NYJ\", \"BUF\")\n\nafc_east &lt;- pbp %&gt;%\n  filter(posteam %in% afc_east_teams_vector)\n\nafc_east_teams_vector\n\n[1] \"NE\"  \"MIA\" \"NYJ\" \"BUF\"\n\n\nIn other cases, we may need to do the opposite of above - that is, select all teams except for specific ones. For example, using data from Pro Football Focus regarding a quarterback’s average time to throw, let’s determine how to remove all AFC North teams (Pittsburgh, Cleveland, Baltimore, and Cincinnati) from the data set while leaving all other teams intact. Such a procedure is necessary, for example, if we wanted to explore the average time to throw for the combined AFC North teams against the rest of the NFL.\nWe cannot leave the data as is and find the averages outright. We must remove the AFC North teams in order to avoid “baking them into” the league-wide average, thus skewing the results of our analysis. To complete this process, we turn to the ! (“is not”) logical operator.\nTo begin, we will use the vroom package to read in the data. Let’s load the ‘vroom’ package before using it.\n\nlibrary(vroom)\n\n\ntime_in_pocket &lt;- vroom(\"http://nfl-book.bradcongelio.com/qb-tip\")\n\n\n\n# A tibble: 30 x 5\n   player       position team_name player_game_count avg_time_to_throw\n   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n 1 Patrick Mah~ QB       KC                       20              2.85\n 2 Tom Brady    QB       TB                       18              2.31\n 3 Justin Herb~ QB       LAC                      18              2.75\n 4 Joe Burrow   QB       CIN                      19              2.51\n 5 Josh Allen   QB       BUF                      18              2.92\n 6 Kirk Cousins QB       MIN                      18              2.7 \n 7 Trevor Lawr~ QB       JAX                      19              2.51\n 8 Geno Smith   QB       SEA                      18              2.79\n 9 Daniel Jones QB       NYG                      18              3.03\n10 Jalen Hurts  QB       PHI                      18              2.86\n# i 20 more rows\n\n\nThe structure and organization of the data is quite similar to the data sets we worked with in our above examples, despite coming from a different source (PFF vs. nflreadr). Our time_in_pocket data includes a quarterback’s name on each row with his corresponding team, player_game_count, and avg_time_to_throw. Again, we are seeking to compare the average time to release - combined - for the AFC North against the rest of the NFL. In order to avoid skewing the results by “baking in data” (that is, not removing AFC North teams prior to finding the NFL average), we can use the ! logical operator to remove all four teams at once (as oppose to structuring the filter() to include four separate team_name != for all four different AFC North teams.\n\ntime_in_pocket_no_afcn &lt;- time_in_pocket %&gt;%\n  filter(!team_name %in% c(\"PIT\", \"BAL\", \"CIN\", \"CLE\"))\n\ntime_in_pocket_no_afcn\n\n# A tibble: 26 x 5\n   player       position team_name player_game_count avg_time_to_throw\n   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;                 &lt;dbl&gt;             &lt;dbl&gt;\n 1 Patrick Mah~ QB       KC                       20              2.85\n 2 Tom Brady    QB       TB                       18              2.31\n 3 Justin Herb~ QB       LAC                      18              2.75\n 4 Josh Allen   QB       BUF                      18              2.92\n 5 Kirk Cousins QB       MIN                      18              2.7 \n 6 Trevor Lawr~ QB       JAX                      19              2.51\n 7 Geno Smith   QB       SEA                      18              2.79\n 8 Daniel Jones QB       NYG                      18              3.03\n 9 Jalen Hurts  QB       PHI                      18              2.86\n10 Jared Goff   QB       DET                      17              2.7 \n# i 16 more rows\n\n\nWith Pittsburgh, Baltimore, Cincinnati, and Cleveland removed from the data set, we can - for the sake of completing the analysis - take our new time_in_pocket_afcn data set and find the league-wide average for avg_time_to_throw.\n\ntime_in_pocket_no_afcn %&gt;%\n  summarize(combined_average = mean(avg_time_to_throw))\n\n# A tibble: 1 x 1\n  combined_average\n             &lt;dbl&gt;\n1             2.76\n\ntime_in_pocket %&gt;%\n  filter(team_name %in% c(\"PIT\", \"BAL\", \"CIN\", \"CLE\")) %&gt;%\n  summarize(combined_average = mean(avg_time_to_throw))\n\n# A tibble: 1 x 1\n  combined_average\n             &lt;dbl&gt;\n1             2.89\n\n\nThe AFC North quarterbacks had an average time to throw of 2.89 seconds while the rest of the NFL averaged 2.76 seconds, meaning the AFC North QBs released the ball, on average, 0.13 seconds slower than the rest of the quarterbacks in the league.\nSo far, all our filter() operations have include just one logical operator, or multiple values built into one operator using the %in% option. However, because of the structure of the logical operators, we are able to construct a single filter() combined with various operator requests. For example, let’s gather only those rows that fit the following specifications:\n\nthat play occurs on 2nd down.\nthe play must be a pass play.\nthe play must include a charted QB hit.\nthe play must result in a complete pass.\nthe pass must have an air yards amount that is greater than or equal to the yards to go.\n\n\n\n\n\n\n\nTip\n\n\n\nCompiling the above filter() requires knowledge of the variable names within nflreadr play-by-play data. This is covered in Chapter 3.\nThat said, it is important to point out that many of the above variables have data values that are structured in binary - that is, the value is either 1 or 0.\nIn the above list, the qb_hit variable is binary. A play with a QB hit is denoted by the inclusion of a numeric 1 in the data, while the inclusion of a numeric 0 indicates that a QB hit did not occur.\n\n\nBuilding the filter() based on the above specifications requires the use of both character and numeric values:\n\nmultiple_filters &lt;- pbp %&gt;%\n  filter(down == 2 & play_type == \"pass\" &\n           qb_hit == 1 &\n           complete_pass == 1 & \n           air_yards &gt;= ydstogo)\n\nThe resulting data set, multiple_filters, indicates that exactly 105 such instances took place during the 2002 NFL season.\nLastly, in some instances, it is necessary to use filter() to remove rows that may provide you with incorrect information if include. To showcase this, it is necessary to also include the summarize() verb, which is covered in greater detail in the following two section. For example, let’s gather information from the play-by-play data that will provide the total amount of yards per scrimmage - combined - for the 2022 regular season. Using our knowledge from the prior example, we can write code that first uses the filter() verb to retrieve just those rows that took place during the regular season and then uses the summarize() verb to distill the information down to the singular, combined total. In the example below, please note the use of na.rm = TRUE. When completing computation operations within the summarize() verb, it is often necessary to include na.rm = TRUE in order to drop any missing values in the data. This is especially important when using the mean() calculation, as any missing data will result in NA values being returned.\n\ntotal_yards &lt;- pbp %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  summarize(yards = sum(yards_gained, na.rm = TRUE))\n\ntotal_yards\n\n# A tibble: 1 x 1\n   yards\n   &lt;dbl&gt;\n1 184444\n\n\nHow many total scrimmage yards did NFL teams earn during the 2022 NFL regular season? According to the results from our above code, the answer is 184,444. However, that is incorrect.\nThe right answer, in fact, is 184,332 total scrimmage yards. How did the results of our above coding come up 112 shorts of the correct answer (as provided by the 2022 Team Stats page on Pro Football Reference)?\nThe difference is the result of the above code including all plays, including 2-point conversion attempts. Because a 2-point conversion is technically considered an extra point attempt - not a play from scrimmage or an official down - no player stats are officially record for the play. We can verify this is the case by using the filter() verb to include only those rows that include regular-season games and those plays that do not include information pertaining to down:\n\ntwo_points &lt;- pbp %&gt;%\n  filter(season_type == \"REG\" & is.na(down)) %&gt;%\n  summarize(total = n(),\n            total_yards = sum(yards_gained, na.rm = TRUE))\n\ntwo_points\n\n# A tibble: 1 x 2\n  total total_yards\n  &lt;int&gt;       &lt;dbl&gt;\n1  8137         112\n\n\nThe results show that exactly 112 scrimmage yards were gained on places missing down information. There was, of course, not 8,137 2-point conversion attempts during the 2022 regular season. There are many other instances within the data that do not include information for down, including: kickoffs, extra points, time outs, the end of a quarter, etc. We can be more specific with our filter() argument in the above code to include only the binary argument for the two_point_attempt variable:\n\ntwo_points_true &lt;- pbp %&gt;%\n  filter(season_type == \"REG\" & two_point_attempt == 1) %&gt;%\n  summarize(total = n(),\n            total_yards = sum(yards_gained, na.rm = TRUE))\n\ntwo_points_true\n\n# A tibble: 1 x 2\n  total total_yards\n  &lt;int&gt;       &lt;dbl&gt;\n1   119         112\n\n\nBy altering the `is.na(down) argument to the binary indicator for two_point_attempt, we see that there were 119 2-point conversion attempts in the 2022 regular season while maintain the correct 112 yards that explains our difference from above. To account for our new-found knowledge that we must remove 2-point conversions to calculate the correct amount of total scrimmage yards, we can again use the !is.na() operator (where !is.na() means “is not missing this specific variable) to remove all rows that do not include information pertaining to down:\n\ntotal_yards_correct &lt;- pbp %&gt;%\n  filter(season_type == \"REG\" & !is.na(down)) %&gt;%\n  summarize(yards = sum(yards_gained, na.rm = TRUE))\n\ntotal_yards_correct\n\n# A tibble: 1 x 1\n   yards\n   &lt;dbl&gt;\n1 184332\n\n\nBy filtering out any row that does not include the down for a given play, we correctly calculated the total amount of scrimmage yards - 184,332 - for the 2022 NFL regular season.\n\n2.5.2 NFL Data and the group_by() verb\nAs mentioned above, the group_by() verb allows you to group data by one or more specific variables in order to retrieve, among other actions, summary statistics. To showcase how group_by() is used within the nflverse data, let’s first gather the 2022 regular season statistics and then use the summarize() verb to get the average success rate on rushing plays.\nAs well, we immediately make use of the filter() function to sort the data : (1.) we first instruct to filter() the data to include just those instances where the play_type equals run, (2.) we then say it must also be play == 1, meaning there was no penalty or other interruption that “cancelled out” the play, and (3.) we lastly pass the argument that the down cannot be missing by using !is.na as a missing down is indicative of a two-point conversion attempt.\n\nrushing_success_ungrouped &lt;- pbp %&gt;%\n  filter(play_type == \"run\" & play == 1 & !is.na(down)) %&gt;%\n  summarize(success_rate = mean(success))\n\nrushing_success_ungrouped\n\n# A tibble: 1 x 1\n  success_rate\n         &lt;dbl&gt;\n1        0.431\n\n\nWithout including the group_by() verb within the above code, the output is the average success rate for rushing plays for all 32 NFL teams, wherein success rate is the percentage of rushing plays that resulted in an EPA above zero. In this case, approximately 43% of NFL rushes had a positive success rate.\nThat said, we are interested in examining the success rate by team, not league-wide average. To do so, we add the posteam variable into the group_by() verb.\n\nrushing_success_grouped &lt;- pbp %&gt;%\n  filter(play_type == \"run\" & play == 1 & !is.na(down)) %&gt;%\n  group_by(posteam) %&gt;%\n  summarize(success_rate = mean(success)) %&gt;%\n  arrange(-success_rate)\n\nrushing_success_grouped %&gt;%\n  slice(1:10)\n\n# A tibble: 10 x 2\n   posteam success_rate\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 PHI            0.526\n 2 BUF            0.488\n 3 BAL            0.478\n 4 GB             0.476\n 5 PIT            0.470\n 6 ATL            0.463\n 7 KC             0.459\n 8 NYG            0.454\n 9 CIN            0.454\n10 LV             0.449\n\n\nIn the above example, we have added the offensive team into the group_by() verb, while also arranging the data in descending order by success_rate, and then used slice() to gather just the ten teams with the highest rushing success rate. The Philadelphia Eagles led the NFL in rushing success rate during the 2022 NFL regular season at 52.3%. By removing the slice() function in the above example, we can see that Tampa Bay maintained the worst rushing success rate in the league at 37.3%.\nWhile determining the rushing success rate of teams is interesting, we can also determine the same metric for individual running backs as well. To do so, we simply replace the variable in the group_by() verb. In the below example, we replace the posteam variable with the rusher variable to see which running backs have the highest success rate.\n\nrunning_back_success &lt;- pbp %&gt;%\n  filter(play_type == \"run\" & play == 1 & !is.na(down)) %&gt;%\n  group_by(rusher) %&gt;%\n  summarize(success_rate = mean(success)) %&gt;%\n  arrange(-success_rate)\n\nrunning_back_success %&gt;%\n  slice(1:10)\n\n# A tibble: 10 x 2\n   rusher      success_rate\n   &lt;chr&gt;              &lt;dbl&gt;\n 1 A.Davis                1\n 2 B.Aiyuk                1\n 3 B.Allen                1\n 4 B.Skowronek            1\n 5 C.Kmet                 1\n 6 C.Sutton               1\n 7 C.Wilson               1\n 8 D.Bellinger            1\n 9 D.Brown                1\n10 D.Gray                 1\n\n\nThe output, unfortunately, is not all that helpful. Because we did not use the filter() verb to stipulate a minimum number of rushing attempts, the output is saying that - for example, Daniel Bellinger, a tight end, was among the most successful rushers in the league with a 100% rushing success rate. To correct this, we must add a second metric to our summarize() verb (we will call it n_rushes) and then use the filter() verb afterwards to include a minimum number of rushes required to be included in the final output.\nAs well, we will provide an additional argument in the first filter() verb that stops the output from including any rushing attempt that does not include the running back’s name. The n_rushes() in the summarize() verb allows us to now include the number of attempts, per individual rusher, that fall within the first filter() parameter. Afterwards, we include a second filter() argument to include just those rushers with at least 200 attempts.\n\nrunning_back_success_min &lt;- pbp %&gt;%\n  filter(play_type == \"run\" & play == 1 &\n           !is.na(down) &\n           !is.na(rusher)) %&gt;%\n  group_by(rusher) %&gt;%\n  summarize(success_rate = mean(success),\n            n_rushes = n()) %&gt;%\n  filter(n_rushes &gt;= 200) %&gt;%\n  arrange(-success_rate)\n\nrunning_back_success_min %&gt;%\n  slice(1:10)\n\n# A tibble: 10 x 3\n   rusher      success_rate n_rushes\n   &lt;chr&gt;              &lt;dbl&gt;    &lt;int&gt;\n 1 M.Sanders          0.483      294\n 2 I.Pacheco          0.469      207\n 3 A.Jones            0.465      213\n 4 J.Jacobs           0.444      340\n 5 N.Chubb            0.434      302\n 6 T.Allgeier         0.429      210\n 7 Ja.Williams        0.427      262\n 8 T.Etienne          0.424      250\n 9 J.Mixon            0.422      249\n10 B.Robinson         0.420      205\n\n\nUnsurprisingly, Miles Sanders - a running back for the Eagles , who lead the NFL in team success rate - is the leader in rushing success among individual players with 49% of his attempts gaining positive EPA.\nMuch like the filter() verb, we are able to supply multiple arguments within one group_by() verb. This is necessary, for example, when you want to examine results over the course of multiple seasons. Let’s explore this by determining each team’s yards after catch between 2010 and 2022. To start, we will retrieve all NFL play-by-play data from those seasons.\n\n\n\n\n\n\nCaution\n\n\n\nLoading in multiple seasons of play-by-play data can be taxing on your computer’s memory. RStudio’s memory usage monitor, located in ‘Environment’ toolbar tell you how much data is currently being stored in memory. As needed, you can click on the usage monitor and select ‘Free Unused Memory’ to purge any data that is no longer also in your ‘Environment.’\n\n\n\nmultiple_seasons &lt;- nflreadr::load_pbp(2010:2022)\n\nOnce the data set is loaded, we can think ahead to what is needed in the group_by() verb. In this case, we are exploring team results on a season basis. Because of this, we will include both posteam and season in the group_by() verb.\n\nyac_seasons &lt;- multiple_seasons %&gt;%\n  filter(season_type == \"REG\" &\n           !is.na(posteam) &\n           !is.na(yards_after_catch)) %&gt;%\n  group_by(season, posteam) %&gt;%\n  summarize(total_yac = sum(yards_after_catch, na.rm = TRUE))\n\n\n2.5.3 NFL Data and the summarize() verb\nAs we’ve seen, the summarize() function can be used to find summary statistics based whichever option we pass to it via the group_by() verb. However, it can also be used to create new metrics built off data included in the nflverse play-by-play data.\nLet’s examine which teams were the most aggressive on 3rd and short passing attempts during the 2022 season. Of course, determining our definition of both what “short” is on 3rd down and “aggressive” is quite subjective. For the purposes of this example, however, let’s assume that 3rd and short is considered 3rd down with five or less yards to go and that “aggressive” is a quarterback’s air yards being to, at minimum, the first-down marker.\nMust like our above examples working with rushing success rate, we begin constructing the metric with the filter() argument. In this case, we are filtering for just pass plays, we want the down to equal 3, the yards to go to be equal to or less than 5, we want it to be an official play, and we do not want it to be missing the down information. After the initial filter() process, we include the posteam variable within our group_by() verb.\nIn our summarize() section, we are first getting the total number of times each team passed the ball on 3rd down with no more than five yards to go. After, we are creating a new aggressiveness column that counts the number of times a quarterback’s air yards were, at minimum, the required yards for a first down. Next, we create another new column titled percentage that takes aggressiveness and divides it by total.\n\nteam_aggressiveness &lt;- pbp %&gt;%\n  filter(play_type == \"pass\" &\n           down == 3 &\n           ydstogo &lt;= 5 &\n           play == 1) %&gt;%\n  group_by(posteam) %&gt;%\n  summarize(total = n(),\n            aggressiveness = sum(air_yards &gt;= ydstogo, na.rm = TRUE),\n            percentage = aggressiveness / total) %&gt;%\n  arrange(-percentage)\n\nteam_aggressiveness %&gt;%\n  slice(1:10)\n\n# A tibble: 10 x 4\n   posteam total aggressiveness percentage\n   &lt;chr&gt;   &lt;int&gt;          &lt;int&gt;      &lt;dbl&gt;\n 1 LV         60             50      0.833\n 2 BUF        61             47      0.770\n 3 ARI        58             44      0.759\n 4 SF         67             50      0.746\n 5 PIT        79             58      0.734\n 6 SEA        60             44      0.733\n 7 NE         54             39      0.722\n 8 TB         92             66      0.717\n 9 MIA        62             43      0.694\n10 CHI        42             29      0.690\n\n\nThe Las Vegas Raiders , based on our definitions, are the most aggressive passing team in the league on 3rd and short as just over 83% of their air yards were at - or past - the required yardage for a first down. On the other end of the spectrum, the New York Giants were the least aggressive team during the 2022 regular season, at 49.1%.\n\n2.5.4 NFL Data and the mutate() verb\nIn the our example above working with the summarize() verb, our output includes only the information contained in our group_by() and then whatever information we provided in the summarize() (such as total, aggressiveness, and percentage).\nWhat if, however, you wanted to create new variables and then summarize() those? That is where the mutate() verb is used.\nAs an example, let’s explore individual quarterback’s average completion percentage over expected for specific air yard distances . To start, we can attempt to do this simply by including both passer and air_yards in the group_by verb.\n\nairyards_cpoe &lt;- pbp %&gt;%\n  group_by(passer, air_yards) %&gt;%\n  summarize(avg_cpoe = mean(cpoe, na.rm = TRUE))\n\nYour output is going to include the avg_cpoe for each quarterback at each and every distance of air_yards. Not only is it difficult to find meaning in, but it would prove to be difficult - if not impossible - to visualize with ggplot(). To correct this issue, we must use the mutate() verb.\nRather than summarize() the completion percentage over expected for each distance of air_yards, we can use the mutate() verb to bundle together a grouping of distances. In the below example, we are using the mutate() verb to create a new variable titled ay_distance using the case_when() verb.\n\nairyards_cpoe_mutate &lt;- pbp %&gt;%\n  filter(!is.na(cpoe)) %&gt;%\n  mutate(\n    ay_distance = case_when(\n      air_yards &lt; 0 ~ \"Negative\",\n      air_yards &gt;= 0 & air_yards &lt; 10 ~ \"Short\",\n      air_yards &gt;= 10 & air_yards &lt; 20 ~ \"Medium\",\n      air_yards &gt;= 20 ~ \"Deep\")) %&gt;%\n  group_by(passer, ay_distance) %&gt;%\n  summarize(avg_cpoe = mean(cpoe))\n\nWith the air_yards data now binned into four different groupings, we can examine quarterbacks at specific distances.\n\nairyards_cpoe_mutate %&gt;%\n  filter(ay_distance == \"Medium\") %&gt;%\n  arrange(-avg_cpoe) %&gt;%\n  slice(1:10)\n\n# A tibble: 82 x 3\n# Groups:   passer [82]\n   passer     ay_distance avg_cpoe\n   &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;\n 1 A.Brown    Medium        -6.09 \n 2 A.Cooper   Medium       -43.1  \n 3 A.Dalton   Medium         5.01 \n 4 A.Rodgers  Medium         2.02 \n 5 B.Allen    Medium        44.5  \n 6 B.Hoyer    Medium       -44.0  \n 7 B.Mayfield Medium       -15.4  \n 8 B.Perkins  Medium         0.266\n 9 B.Purdy    Medium         9.72 \n10 B.Rypien   Medium        18.0  \n# i 72 more rows"
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#core-skills-for-tidy-data",
    "href": "02-nfl-analytics-tidyverse.html#core-skills-for-tidy-data",
    "title": "\n2  Wrangling NFL Data in the tidyverse\n",
    "section": "\n2.6 Core Skills for Tidy Data",
    "text": "2.6 Core Skills for Tidy Data\nWith your new understanding of the tidyverse flow in the R programming language, we are now going to hone these core skills by taking a dataset from ingestion through the cleaning and prepping process so that it is prepared for eventual data visualization (which will be done in Chapter 4 of this book). To do so, we are going to use data provided by Sports Info Solutions. Based out of Allentown, Pennsylvania, SIS collects its play-by-play and then does a manual hand-charting process to provide weekly data that covers a vast array of specific occurrences in football (defensive scheme, pre-snap motion, wide receivers formations, offensive personnel, drop type, and more).\nIn this example, we are going to compare the Boom% and the Bust% for each quarterback in the data, wherein the Boom% correlates to any pass attempt by the quarterback that resulted in an expected points add (EPA) of at least 1 and Bust% is any pass attempt that resulted in an EPA of at least -1. In order to prepare the data for visualization in ggplot, we will:\n\nImport and examine the data.\nDeal with missing values in the data.\nCharge variable types to match what it needed for visualization.\nUsing the mutate() verb to correct and create new metrics within the data.\nMerging the data with a secondary dataset.\n\n\n2.6.1 Importing and Conducting Exploratory Data Analysis\nTo start, let’s examine data from Sports Info Solutions by reading in the data from this book’s Git repository using the vroom package.\n\nsis_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/sis-bb\")\n\nUsing vroom , we are reading in the .csv file into our environment and assigning it the name sis_data. With access to the data, we can begin our exploratory data analysis - the importance of which cannot be understated. The process of EDA results in several key items:\n\nEDA helps to identify potential issues in the data set, such as missing or erroneous data, outliers, or inconsistent values - all of which must be addressed prior to further analysis or visualization.\nConducting an EDA allows you to gain a deeper understanding of the distribution, variability, and relationships between all the variables contained in the data set, thus allowing you to make informed decisions on appropriate statistical techniques, models, and visualizations that can best analyze and communicate the data effectively.\nThe EDA process can help you discover patterns or trends in the data that may be of interest or relevance to your research question, or help you discover to questions to answer.\n\nTo start the process of EDA on our newly created sis_data dataset, let’s examine the current status of the data set:\n\nsis_data\n\n# A tibble: 107 x 14\n   Season Player          Team       Att `Points Earned` `PE Per Play`\n    &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1   2022 Patrick Mahomes Chiefs     648            180.         0.267\n 2   2022 Justin Herbert  Chargers   699            138.         0.187\n 3   2022 Trevor Lawrence Jaguars    584            123.         0.202\n 4   2022 Jared Goff      Lions      587            121.         0.199\n 5   2022 Jalen Hurts     Eagles     460            106.         0.213\n 6   2022 &lt;NA&gt;            &lt;NA&gt;       439            104.         0.12 \n 7   2022 Kirk Cousins    Vikings    643            104.         0.151\n 8   2022 Tua Tagovailoa  Dolphins   400            102.         0.243\n 9   2022 Joe Burrow      Bengals    606            102.         0.158\n10   2022 Josh Allen      Bills      567            100.         0.167\n# i 97 more rows\n# i 8 more variables: `Points Above Avg` &lt;dbl&gt;, ...\n\n\nThe results immediately bring to light several issues in the data:\n\nThere are several instances of missing data (as indicated by NA values in the 6th row).\nMany of the column names are not in tidy format. Specifically, the Boom% and Bust% columns, because of the inclusion of the percentage sign, require the small ticks at either end of the word. The same issue is apparent in Points Earned as it includes a space between the word and, finally, many of the variable names are in full caps.\nThe Boom% and Bust% columns are both listed as &lt;chr&gt; which indicates that they are currently character-based columns, rather than the needed numeric-based column.\nThe above issue is caused by all the values within Boom% and Bust% containing a percentage sign at the tail end.\nLastly, the team variable for Baker Mayfield’s has a value of “2 teams” since he played for both the Carolina Panthers and Los Angeles Rams during the 2022 season.\n\nTo begin dealing with these issues, let’s first tackle cleaning the variable names and then selecting just those columns relevant to our forthcoming data visualization in Chapter 4:\n\nsis_data &lt;- sis_data %&gt;%\n  janitor::clean_names() %&gt;%\n  select(player, team, att, boom_percent, bust_percent)\n\nsis_data\n\n# A tibble: 107 x 5\n   player          team       att boom_percent bust_percent\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n 1 Patrick Mahomes Chiefs     648 25.10%       12.50%      \n 2 Justin Herbert  Chargers   699 21.20%       15.90%      \n 3 Trevor Lawrence Jaguars    584 23.70%       15.20%      \n 4 Jared Goff      Lions      587 23.60%       13.30%      \n 5 Jalen Hurts     Eagles     460 23.50%       16.10%      \n 6 &lt;NA&gt;            &lt;NA&gt;       439 &lt;NA&gt;         10.10%      \n 7 Kirk Cousins    Vikings    643 21.30%       17.40%      \n 8 Tua Tagovailoa  Dolphins   400 28.50%       15.70%      \n 9 Joe Burrow      Bengals    606 23.60%       14.80%      \n10 Josh Allen      Bills      567 23.20%       12.70%      \n# i 97 more rows\n\n\nIn the above code, we are using the clean_names() function within the janitor package to automatically rework the column names into a tidy format. After, we use the select() verb to keep only five of the included columns of data (player, team, att, boom_percent, and bust_percent). With the column names clean and easy to implement within our code, and with the relevant columns selected, we can move to the process of dealing with the missing data.\n\n2.6.2 Dealing with Missing Data\nDealing with missing data is an important step in the exploratory data analysis process, as its inclusion can introduce bias, reduce statistical power, and ultimately influence the validity of your research findings. In any case, a missing value within an RStudio dataset is represented by NA values and there are several different approaches to handling the missing data in an effective manner.\nA data set can present missing data for a variety of reasons, including participant non-responses (such in social science research), measurement error, or simple data processing issues. The process of handling missing data is ultimately the result of the end goal of the analysis. In some case, conducting a imputation (such as replacing all NA values with the mean of all existing data) is used. In other cases, a more complex method can be utilized such as using the mice package, which can impute missing data based on several different machine learning approaches (predictive mean matching, logistic regression, Bayesian polytomous regression, proportional odds model, etc.). Ultimately, the choice of method for handling missing data depends on the attributes of the data set, your research question, and your domain knowledge of what the data represents.\nTo begin exploring missing values in our sis_data dataset, we can run the following code:\n\nsis_data_missing &lt;- sis_data %&gt;%\n  filter_all(any_vars(is.na(.)))\n\nsis_data_missing\n\n# A tibble: 1 x 5\n  player team    att boom_percent bust_percent\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       \n1 &lt;NA&gt;   &lt;NA&gt;    439 &lt;NA&gt;         10.10%      \n\n\nBy using filter_all() across all of our variables (any_vars), we can filter for any missing value, indicated by including is.na in the filter(). The output shows each row within the data set that includes at least one missing value. In this case, we only have one row with missing data, with values for player, team, and boom_percent all missing. Given the nature of our data, it is not prudent to impute a value in place of the missing data as we have no ability to accurately determine who the player is.\nInstead, we will return to our original data set (sis_data) and use the na.omit() function to drop any row that includes missing data:\n\nsis_data &lt;- na.omit(sis_data)\n\nIn the above code, we are simply “recreating” our existing dataset but using na.omit() to drop rows that include NA values. Once complete, we can move onto the next step of the data preparation/cleaning process.\n\n2.6.3 Changing Variable Types\nAs discovered in the exploratory data analysis process, we know that four columns (player, team, boom_percent, and bust_percent) are listed as a character data type while each quarterback’s number of attempts (att) is listed as a numeric. While we do want both player and team names to be a character-based datatype, we need both the boom and bust percentage for each quarterback to be a numeric value rather than a character.\nMoreover, the values in each boom_percent and bust_percent include a percentage sign at the tail-end.\nBecause of this, we have two issues we need to correct:\n\ncorrectly changing the variable type for boom_percent and bust_percent\n\nremoving the % from the end of each value in boom_percent and bust_percent\n\n\nMuch like many things while working in the R programming language, there exists more than one way to tackle the above issues. In this specific case, we will first look at a method that uses the stringr package before switching the columns from character to numeric using an approach rooted in base R. After, we will use a much easier method to do both at one time.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note here that I do not necessarily endorse using the first method below to change variable types and to drop the percentage sign, as the second option is ideal.\nHowever, it is important to see how both methods work as there could be a case, somewhere down the road when you are exploring and preparing data yourself, that the first option is the only or best option.\nAs mentioned, there are typically multiple ways to get to the same endpoint in the R programming language. Showcasing both methods below simply provides you more options in your toolkit for tidying data.\n\n\n\n2.6.3.1 Method #1: Using stringr and Changing Variables\nIn the below example, we are creating an “example” data frame titled sis_data_stringr from our current iteration of SIS data and then pipe into the mutate() verb (covered in more depth in the below Creating New Variables section). It is within this mutate() verb that we can drop the percentage sign.\nWe first indicate that we are going to mutate() our already existing columns (both boom_percent and bust_percent). After, we use an = sign to indicate that the following string is the argument to create or, in our case, edit the existing values in the column.\nWe then use the str_remove() function from the stringr package to locate the % sign in each value and remove it.\nAfterwards, we dive into base R (that is, not tidyverse structure) and use the as.numeric() function to change both of the boom and bust columns to the correct data type (using the $ sign to notate that we are working on just one specific column in the data.\n\nsis_data_stringr &lt;- sis_data %&gt;%\n  mutate(boom_percent = str_remove(boom_percent, \"%\"),\n         bust_percent = str_remove(bust_percent, \"%\"))\n\nsis_data_stringr$boom_percent &lt;-\n  as.numeric(as.character(sis_data_stringr$boom_percent))\nsis_data_stringr$bust_percent &lt;-\n  as.numeric(as.character(sis_data_stringr$bust_percent))\n\nsis_data_stringr\n\n# A tibble: 106 x 5\n   player          team       att boom_percent bust_percent\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Patrick Mahomes Chiefs     648         25.1         12.5\n 2 Justin Herbert  Chargers   699         21.2         15.9\n 3 Trevor Lawrence Jaguars    584         23.7         15.2\n 4 Jared Goff      Lions      587         23.6         13.3\n 5 Jalen Hurts     Eagles     460         23.5         16.1\n 6 Kirk Cousins    Vikings    643         21.3         17.4\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7\n 8 Joe Burrow      Bengals    606         23.6         14.8\n 9 Josh Allen      Bills      567         23.2         12.7\n10 Geno Smith      Seahawks   572         20.6         17.6\n# i 96 more rows\n\n\nOur output shows that att, boom_percent, and bust_percent are now all three numeric and, moreover, the tailing percentage sign in each boom and bust value is now removed. At this point, the data is prepped and properly structured for the visualization process.\n\n2.6.3.2 Method #2: Using the parse_number Function in readr\n\nWhile the above example using stringr and base R is suitable for our needs, we can use the readr package - and its parse_number() function - to achieve the same results in a less verbose manner.\n\nsis_data &lt;- sis_data %&gt;%\n  mutate(boom_percent = readr::parse_number(boom_percent),\n         bust_percent = readr::parse_number(bust_percent))\n\nsis_data\n\n# A tibble: 106 x 5\n   player          team       att boom_percent bust_percent\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1 Patrick Mahomes Chiefs     648         25.1         12.5\n 2 Justin Herbert  Chargers   699         21.2         15.9\n 3 Trevor Lawrence Jaguars    584         23.7         15.2\n 4 Jared Goff      Lions      587         23.6         13.3\n 5 Jalen Hurts     Eagles     460         23.5         16.1\n 6 Kirk Cousins    Vikings    643         21.3         17.4\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7\n 8 Joe Burrow      Bengals    606         23.6         14.8\n 9 Josh Allen      Bills      567         23.2         12.7\n10 Geno Smith      Seahawks   572         20.6         17.6\n# i 96 more rows\n\n\nThe above example, using readr, is quite similar to the first version that used stringr in that we are using the mutate() verb to edit our existing boom_percent and bust_percent columns. In this case, however, we are replacing our stringer argument with the parse_number() function from the readr package. When finished, you can see in the output that not only are both boom and bust correctly listed as numeric, but the parse_number() function automatically recognized and dropped the tailing percentage sign.\nWhile both examples get us to the same endpoint, using the readr package allows us to get there with less work.\n\n2.6.4 Correct and Create New Variables Using mutate\n\nThe mutate() verb is a powerful and widely used tool that allows us to create new variables (columns) based on existing ones within our data set. To that end, the mutate() verb applies a user-supplied argument to each row of the data set to create the new metric. Additionally, the mutate() verb - when combined with the case_when() argument - allows us to make correction to the data (such as the aforementioned issues with Mayfield’s teams).\nThe syntax of the mutate() verb follows the tidyverse flow like many of the other dplyr verbs:\n\nmutate(new_column_name = user_supplied_argument(existing_column_name))\n\nThe mutate() verb is highly flexible, allowing us to create a new column based on a wide-range of functions and arguments, including mathematical operations, logical operations, string operation, and many more - including applying functions from other packages.\nImportantly, the mutate() verb allows us to create new variables without changing the original data set. While the summarize() verb can also create new columns based on various functions and argument, it also distills the data set down to just the columns created within the summarize() function (plus any column including in the group_by()). Conversely, the mutate() verb creates the new column and adds it on to the current iteration of the data set.\nTo showcase this, let’s create a new column in our sis_data that calculate the difference between a quarterback’s boom_percent and bust_percent. This is also a great time to limit the quarterbacks we include based on the number of attempts for each. Not doing so results in oddities in the data, such as Christian McCaffrey (a running back) having a 100-percent boom percentage on one passing attempt. To avoid such occurrences, let’s add the filter() verb prior to our mutate() and limit the data to just those quarterbacks with at least 200 passing attempts:\n\nsis_data &lt;- sis_data %&gt;%\n  filter(att &gt;= 200) %&gt;%\n  mutate(difference = boom_percent - bust_percent)\n\nsis_data\n\n# A tibble: 33 x 6\n   player          team       att boom_percent bust_percent difference\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Patrick Mahomes Chiefs     648         25.1         12.5      12.6 \n 2 Justin Herbert  Chargers   699         21.2         15.9       5.3 \n 3 Trevor Lawrence Jaguars    584         23.7         15.2       8.5 \n 4 Jared Goff      Lions      587         23.6         13.3      10.3 \n 5 Jalen Hurts     Eagles     460         23.5         16.1       7.4 \n 6 Kirk Cousins    Vikings    643         21.3         17.4       3.90\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7      12.8 \n 8 Joe Burrow      Bengals    606         23.6         14.8       8.8 \n 9 Josh Allen      Bills      567         23.2         12.7      10.5 \n10 Geno Smith      Seahawks   572         20.6         17.6       3   \n# i 23 more rows\n\n\nIn the above example, by using the mutate() verb, we created a new column in our existing sis_data data set that calculates the difference between a quarterback’s boom_percent and bust_percent. Importantly, as mentioned, this is done while keeping the rest of the dataset in place unlike using the summarize() verb.\nAs mentioned in the introduction to this section, it is possible to include other function within the mutate() verb. In fact, doing so is necessary if we wish to make the correction to Baker Mayfield’s team. Currently, Mayfield’s value for team is listed as 2 teams. Instead, let’s replace that with his most recent team in the 2022 NFL season (Rams).\nTo make this correction, we will begin by using the mutate() verb on the existing team column in our data set. However, where we calculated the different between boom and bust percent in the above example of mutate(), we will now use the case_when() function.\nThe case_when() function allows us to either correct values, or create new columns, based on multiple conditions and values. The syntax of case_when() is as follows:\n\nmutate(column_name = case_when(\n  condition_1 == value_1 ~ new_value,\n  TRUE ~ default_value))\n\nIn the above code, we are essentially saying: “if condition_1 equals value_1, then replace the column_name’s existing value with this new_value.” The final argument, TRUE ~ default_value, specifies the default value to be used if none of the prior conditions are TRUE.\nTo make this more concrete, let’s work through it using our issue with Baker Mayfield. In order to make the correction, we need to supply the case_when() argument three items from the data: the column name that we are applying the mutate() to (in this case, team), the first condition (in this case, player), and the value of that condition (in this case, Baker Mayfield). Next, we need to include the new value (in this case, Rams), and then supply what to do if the above condition is not TRUE. In other words, if the player is not Baker Mayfield then simply keep the existing team in place.\nFollowing that verbal walk through of the process, we can place the conditions and values into the correct spot in the relevant code:\n\nsis_data &lt;- sis_data %&gt;%\n  mutate(team = case_when(\n    player == \"Baker Mayfield\" ~ \"Rams\",\n    TRUE ~ team))\n\nsis_data %&gt;%\n  filter(player == \"Baker Mayfield\")\n\n# A tibble: 1 x 6\n  player         team    att boom_percent bust_percent difference\n  &lt;chr&gt;          &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Baker Mayfield Rams    335           17         18.9      -1.90\n\n\nBy filtering the data down to include just Baker Mayfield, we can make sure that his team is now listed as the Rams. Further, a continued look at the data shows no other players’ teams were changed in the process:\n\nsis_data\n\n# A tibble: 33 x 6\n   player          team       att boom_percent bust_percent difference\n   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 Patrick Mahomes Chiefs     648         25.1         12.5      12.6 \n 2 Justin Herbert  Chargers   699         21.2         15.9       5.3 \n 3 Trevor Lawrence Jaguars    584         23.7         15.2       8.5 \n 4 Jared Goff      Lions      587         23.6         13.3      10.3 \n 5 Jalen Hurts     Eagles     460         23.5         16.1       7.4 \n 6 Kirk Cousins    Vikings    643         21.3         17.4       3.90\n 7 Tua Tagovailoa  Dolphins   400         28.5         15.7      12.8 \n 8 Joe Burrow      Bengals    606         23.6         14.8       8.8 \n 9 Josh Allen      Bills      567         23.2         12.7      10.5 \n10 Geno Smith      Seahawks   572         20.6         17.6       3   \n# i 23 more rows\n\n\nWith that, our sis_data data set is nearly complete and ready for visualization. Before doing so, though, we can use the load_teams function within nflreadr to combine the appropriate team colors to each quarterback in the data, allowing us to customize the appearance of the data visualization.\n\n2.6.5 Merging Datasets with dplyr and Mutating Joins\nMutating joins are a manipulation technique that allows us to add, modify, or remove columns in a data set based on a shared variable or a set of variables. The process is done by matching rows in the data sets based on an matching identifier between the two. Once completed, the matching data from the second data set is added to the first or, conversely, the data within the first dataset is modified as needed to include the incoming data from the second data set.\nThere are four common mutating joins in the dplyr package:\n\n\nleft_join(): a left_join() adds all the columns from the first dataset by only matching columns from the second data set.\n\nright_join(): a right_join() is the opposite of the left_join() process in that it adds all the columns from the second data set and only matches those columns from the first.\n\ninner_join(): a inner_join() will only match rows found in both data sets.\n\nfull_join(): a full_join() will include all the rows and columns from both data sets.\n\nIn this instance, using our sis_data data set, we will need to conduct a left_join() to bring in NFL team color information. To start, we can use the load_teams() function in nflreadr to load the team information and then use select() to keep just the information we needed for the merging process:\n\n\n\n\n\n\nNote\n\n\n\nPlease note that a much deeper dive into the nflverse functions is included in Chapter 3, including a more detailed approach to working with individual team data. The below code will be expanded upon in the next chapter but, for now, allows us to walk through the merging process.\n\n\n\nteams &lt;- nflreadr::load_teams(current = TRUE) %&gt;%\n  select(team_abbr, team_nick, team_name, team_color, team_color2)\n\nteams\n\n# A tibble: 32 x 5\n   team_abbr team_nick team_name          team_color team_color2\n   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;      \n 1 ARI       Cardinals Arizona Cardinals  #97233F    #000000    \n 2 ATL       Falcons   Atlanta Falcons    #A71930    #000000    \n 3 BAL       Ravens    Baltimore Ravens   #241773    #9E7C0C    \n 4 BUF       Bills     Buffalo Bills      #00338D    #C60C30    \n 5 CAR       Panthers  Carolina Panthers  #0085CA    #000000    \n 6 CHI       Bears     Chicago Bears      #0B162A    #C83803    \n 7 CIN       Bengals   Cincinnati Bengals #FB4F14    #000000    \n 8 CLE       Browns    Cleveland Browns   #FF3C00    #311D00    \n 9 DAL       Cowboys   Dallas Cowboys     #002244    #B0B7BC    \n10 DEN       Broncos   Denver Broncos     #002244    #FB4F14    \n# i 22 more rows\n\n\nFor educational purposes, we use the above code to bring in each NFL’s team abbreviation, nickname, and full name. Along with identifying information for each team, we also have selected team_color and team_color2 (which correspond to each team’s primary and secondary colors).\nAs mentioned, we will use the left_join() function to merge this information into our sis_data. The basic syntax of a left_join includes a call to the original data set, the data set to be merged in, and then an argument to stipulate which variable the join should match on:\n\ndata_set1 &lt;- data_set1 %&gt;%\n  left_join(data_set2, by = c(\"variable_1\" = \"variable_2\"))\n\nThe above code is stating: “use data_set1, and then conduct a left_join with data_set2, by matching variable_1 from data_set1 to corresponding variable_2 from data_set2.”\nTo conduct the join, we must determine which variable in teams matching a corresponding variable in sis_data. We can use the slice() function to compare the first row of data from each:\n\nsis_data %&gt;%\n  slice(1)\n\n# A tibble: 1 x 6\n  player          team     att boom_percent bust_percent difference\n  &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 Patrick Mahomes Chiefs   648         25.1         12.5       12.6\n\nteams %&gt;%\n  slice(1)\n\n# A tibble: 1 x 5\n  team_abbr team_nick team_name         team_color team_color2\n  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;             &lt;chr&gt;      &lt;chr&gt;      \n1 ARI       Cardinals Arizona Cardinals #97233F    #000000    \n\n\nIn sis_data, the team column includes the nickname of the NFL team (in this case, the Chiefs). The corresponding variable in our teams data set is the column titled team_nick. Knowing that, we can conduct the left_join using team for variable_1 from the above example syntax and team_nick for variable_2.\n\n\n\n\n\n\nTip\n\n\n\nNotice that the columns containing the corresponding data do not have the same name (team vs. team_nick).\nIt is important to remember that the name of the columns do not have to match. It is the values contained within the column that must match.\n\n\n\nsis_data &lt;- sis_data %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_nick\"))\n\nsis_data\n\n# A tibble: 33 x 10\n   player   team    att boom_percent bust_percent difference team_abbr\n   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n 1 Patrick~ Chie~   648         25.1         12.5      12.6  KC       \n 2 Justin ~ Char~   699         21.2         15.9       5.3  LAC      \n 3 Trevor ~ Jagu~   584         23.7         15.2       8.5  JAX      \n 4 Jared G~ Lions   587         23.6         13.3      10.3  DET      \n 5 Jalen H~ Eagl~   460         23.5         16.1       7.4  PHI      \n 6 Kirk Co~ Viki~   643         21.3         17.4       3.90 MIN      \n 7 Tua Tag~ Dolp~   400         28.5         15.7      12.8  MIA      \n 8 Joe Bur~ Beng~   606         23.6         14.8       8.8  CIN      \n 9 Josh Al~ Bills   567         23.2         12.7      10.5  BUF      \n10 Geno Sm~ Seah~   572         20.6         17.6       3    SEA      \n# i 23 more rows\n# i 3 more variables: team_name &lt;chr&gt;, team_color &lt;chr&gt;, ...\n\n\nYou can see in the output that our two data sets correctly merged on the team and team_nick columns. Moreover, to illustrate what happens doing the joining process, please note these two items:\n\nThe team_nick column from our teams data set is not included in our merged sis_data, as it is dropped after matching/merging with the team information from our original data set.\nWhile the team_nick column was dropped, the other team identifiers (team_abbr and team_name) were kept intact, as they were not used in the matching process. As well, team_color and team_color2 are now correctly matched with each quarterback’s team."
  },
  {
    "objectID": "02-nfl-analytics-tidyverse.html#exercises",
    "href": "02-nfl-analytics-tidyverse.html#exercises",
    "title": "\n2  Wrangling NFL Data in the tidyverse\n",
    "section": "\n2.7 Exercises",
    "text": "2.7 Exercises\nThe answers for the following answers can be found here: http://nfl-book.bradcongelio.com/ch2-answers.\nCreate a data frame titled pbp that contains the play-by-play information for the 2022 regular season by running the following code:\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\nAfter, using the data in pbp, complete the following:\n\n2.7.1 Exercise 1\n\n\nUse filter() to select only Patrick Mahomes and all passes that resulted in interceptions.\nAdd a summarize() to determine the average air yards (air_yards) for his combined interceptions.\n\n2.7.2 Exercise 2\n\n\nIn a data frame titled wide_receivers, find the average yards after catch (yards_after_catch) and average yards after catch EPA (yac_epa) for the top-10 receivers in total receiving yards. In doing so, you will need to use group_by() on receiver.\n\n2.7.3 Exercise 3\n\n\nIn a data frame titled qb_hits, find the total number of times each defensive unit (defteam) recorded a QB hit (qb_hit).\n\n2.7.4 Exercise 4\n\n\n\nAfter running the following code, add a column titled difference using that mutate() verb that calculates the difference between each quarterback’s total_completions and total_incompletions.\n\ncmp_inc &lt;- pbp %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1, !is.na(down)) %&gt;%\n  group_by(passer) %&gt;%\n  summarize(total_completions = sum(complete_pass == 1,\n                                    na.rm = TRUE),\n            total_incompletions = sum(incomplete_pass == 1,\n                                      na.rm = TRUE)) %&gt;%\n  filter(total_completions &gt;= 180)\n\n\n\n2.7.5 Exercise 5\n\n\n\nAfter running the following code, use the left_join() function to merge the two data frames together into a single data frame titled rb_success_combined.\n\nroster_information &lt;- nflreadr::load_rosters(2022) %&gt;%\n  select(full_name, gsis_id, headshot_url)\n\nrb_success &lt;- pbp %&gt;%\n  group_by(rusher_player_id) %&gt;%\n  filter(!is.na(rusher) & !is.na(success)) %&gt;%\n  summarize(total_success = sum(success, na.rm = TRUE)) %&gt;%\n  arrange(-total_success) %&gt;%\n  slice(1:10)\n\n\n\n\n\n\n\nWickham, H. (2022). Tidyverse packages. Retrieved from https://www.tidyverse.org/packages/"
  },
  {
    "objectID": "03-nfl-analytics-functions.html#nflreadr-an-introduction-to-the-data",
    "href": "03-nfl-analytics-functions.html#nflreadr-an-introduction-to-the-data",
    "title": "\n3  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n3.1 nflreadr: An Introduction to the Data",
    "text": "3.1 nflreadr: An Introduction to the Data\nThe most important part of the nflverse is, of course, the data. To begin, we will examine the core data that underpins the nflverse: weekly player stats and the more detailed play-by–play data. Using nflreadr, the end user is able to collect weekly top-level stats via the load_player_stats() function or the much more robust play-by-play numbers by using the load_pbp() function.\nAs you may imagine, there is a very important distinction between the load_player_stats() and load_pbp(). As mentioned, load_player_stats() will provide you with weekly, pre-calculated statistics for either offense or kicking. Conversely, load_pbp() will provide over 350 metrics for every single play of every single game dating back to 1999.\nThe load_player_stats() function includes the following offensive information:\n\noffensive.stats &lt;- nflreadr::load_player_stats(2021)\n\nls(offensive.stats)\n\n [1] \"air_yards_share\"             \"attempts\"                   \n [3] \"carries\"                     \"completions\"                \n [5] \"dakota\"                      \"fantasy_points\"             \n [7] \"fantasy_points_ppr\"          \"headshot_url\"               \n [9] \"interceptions\"               \"pacr\"                       \n[11] \"passing_2pt_conversions\"     \"passing_air_yards\"          \n[13] \"passing_epa\"                 \"passing_first_downs\"        \n[15] \"passing_tds\"                 \"passing_yards\"              \n[17] \"passing_yards_after_catch\"   \"player_display_name\"        \n[19] \"player_id\"                   \"player_name\"                \n[21] \"position\"                    \"position_group\"             \n[23] \"racr\"                        \"receiving_2pt_conversions\"  \n[25] \"receiving_air_yards\"         \"receiving_epa\"              \n[27] \"receiving_first_downs\"       \"receiving_fumbles\"          \n[29] \"receiving_fumbles_lost\"      \"receiving_tds\"              \n[31] \"receiving_yards\"             \"receiving_yards_after_catch\"\n[33] \"recent_team\"                 \"receptions\"                 \n[35] \"rushing_2pt_conversions\"     \"rushing_epa\"                \n[37] \"rushing_first_downs\"         \"rushing_fumbles\"            \n[39] \"rushing_fumbles_lost\"        \"rushing_tds\"                \n[41] \"rushing_yards\"               \"sack_fumbles\"               \n[43] \"sack_fumbles_lost\"           \"sack_yards\"                 \n[45] \"sacks\"                       \"season\"                     \n[47] \"season_type\"                 \"special_teams_tds\"          \n[49] \"target_share\"                \"targets\"                    \n[51] \"week\"                        \"wopr\"                       \n\n\nAs well, switching the stat_type to “kicking” provides the following information:\n\nkicking.stats &lt;- nflreadr::load_player_stats(2021,\n                                             stat_type = \"kicking\")\n\nls(kicking.stats)\n\n [1] \"fg_att\"              \"fg_blocked\"          \"fg_blocked_distance\"\n [4] \"fg_blocked_list\"     \"fg_long\"             \"fg_made\"            \n [7] \"fg_made_0_19\"        \"fg_made_20_29\"       \"fg_made_30_39\"      \n[10] \"fg_made_40_49\"       \"fg_made_50_59\"       \"fg_made_60_\"        \n[13] \"fg_made_distance\"    \"fg_made_list\"        \"fg_missed\"          \n[16] \"fg_missed_0_19\"      \"fg_missed_20_29\"     \"fg_missed_30_39\"    \n[19] \"fg_missed_40_49\"     \"fg_missed_50_59\"     \"fg_missed_60_\"      \n[22] \"fg_missed_distance\"  \"fg_missed_list\"      \"fg_pct\"             \n[25] \"gwfg_att\"            \"gwfg_blocked\"        \"gwfg_distance\"      \n[28] \"gwfg_made\"           \"gwfg_missed\"         \"pat_att\"            \n[31] \"pat_blocked\"         \"pat_made\"            \"pat_missed\"         \n[34] \"pat_pct\"             \"player_id\"           \"player_name\"        \n[37] \"season\"              \"season_type\"         \"team\"               \n[40] \"week\"               \n\n\nWhile the data returned is not as rich as the play-by-play data we will covering next, the load_player_stats() function is extremely helpful when you need to quickly (and correctly!) recreate the official stats listed on either the NFL’s website or on Pro Football Reference.\nAs an example, let’s say you need to get Patrick Mahomes’ total passing yard and attempts from the 2022 season. You could do so via load_pbp() but, if you do not need further context (such as down, distance, quarter, etc.), using load_player_stats() is much more efficient.\n\n3.1.1 Getting Player Stats via load_player_stats()\n\nWe can generate a data frame titled mahomes_pyards but running the following code:\n\nmahomes_pyards &lt;- nflreadr::load_player_stats(seasons = 2022)\n\nIn the above example, we specified that the returned data be from the 2022 season. It needs to be noted that running load_player_stats() (without a specific season) will return the newest season in the data. Moreover, separating two seasons with a colon (:) will provide multiple seasons of data. In our working example, the mahomes_pyards data frame contains statistics for every player from every week of the 2022 season.\nIt is important to note that the structure of the data returned by load_player_stats() includes passing yards, rushing yards, receiving yards, etc. for each player. As a result, the data returns - for example - the week-by-week statistics for rushing and receiving for Peyton Manning, not just his passing numbers. Because of this, we can calculate Peyton’s rushing statistics from 2000 to 2022.\n# A tibble: 1 x 4\n  carries rushing_yards rushing_tds rush_epa\n    &lt;int&gt;         &lt;dbl&gt;       &lt;int&gt;    &lt;dbl&gt;\n1     409           531          18   -0.740\nKnowing that Peyton’s average rushing EPA from 2000 to the end of his career was -0.740 is not going to win you final Jeopardy nor is it going to be helpful in any sort of Twitter football discourse (as opposed to knowing the rushing EPA for Patrick Mahomes who is, for lack of a better term, a bit more nimble on the run). When working with load_player_stats(), it is important that you know the variable names that are useful to the research you are doing.\n\n\n\n\n\n\nTip\n\n\n\nThe relevant passing statistics housed in load_player_stats() includes:\n\n\n [1] \"completions\"               \"attempts\"                 \n [3] \"passing_yards\"             \"passing_tds\"              \n [5] \"interceptions\"             \"sacks\"                    \n [7] \"sack_yards\"                \"sack_fumbles\"             \n [9] \"sack_fumbles_lost\"         \"passing_air_yards\"        \n[11] \"passing_yards_after_catch\" \"passing_first_downs\"      \n[13] \"passing_epa\"               \"passing_2pt_conversions\"  \n[15] \"pacr\"                      \"dakota\"                   \n\n\nThe columns relevant to rushing statistics in load_player_stats() are:\n\n\n[1] \"carries\"                 \"rushing_yards\"          \n[3] \"rushing_tds\"             \"rushing_fumbles\"        \n[5] \"rushing_fumbles_lost\"    \"rushing_first_downs\"    \n[7] \"rushing_epa\"             \"rushing_2pt_conversions\"\n\n\nLastly, the columns for receiving statistics in load_player_stats() include:\n\n\n [1] \"receptions\"                  \"targets\"                    \n [3] \"receiving_yards\"             \"receiving_tds\"              \n [5] \"receiving_fumbles\"           \"receiving_fumbles_lost\"     \n [7] \"receiving_air_yards\"         \"receiving_yards_after_catch\"\n [9] \"receiving_first_downs\"       \"receiving_epa\"              \n[11] \"receiving_2pt_conversions\"   \"racr\"                       \n[13] \"target_share\"                \"air_yards_share\"            \n\n\n\n\nContinuing with our above example working with load_player_stats() and Patrick Mahomes, we can find his total passing yards during the 2022 regular season by using the filter() function to sort the data for either player_name or player_display_name as well the the season_type and then use summarize() to get the total of his passing_yards over the course of the regular season.\n\nmahomes_pyards &lt;- mahomes_pyards %&gt;%\n  filter(player_name == \"P.Mahomes\" & season_type == \"REG\") %&gt;%\n  summarize(passing_yards = sum(passing_yards))\n\nmahomes_pyards\n\n# A tibble: 1 x 1\n  passing_yards\n          &lt;dbl&gt;\n1          5250\n\n\nThe above code returns a result of 5,250 passing yards which is an exact match from the data provided by Pro Football Reference. In the above example, we used the filter() function on the player_name column in the data. You will get the same result if you replace that with player_display_name == \"Patrick Mahomes\". While there is no difference in how the data is collected, there will be a difference if you were to visualize the data frame and wished to display the player names along with their respective statistics. In most cases, in order to maintain a clean design, using the player_name option is best as it allows you to plot just the player’s first initial and last name.\nWe can build upon the passing yards example to replicate the vast majority of statistics found in the“Passing” data table for Mahomes from Pro Football Reference.\n\nmahomes_pfr &lt;- nflreadr::load_player_stats(seasons = 2022) %&gt;%\n  filter(player_name == \"P.Mahomes\" & season_type == \"REG\") %&gt;%\n  summarize(\n    completions = sum(completions),\n    attempts = sum(attempts),\n    cmp_pct = completions / attempts,\n    yards = sum(passing_yards),\n    touchdowns = sum(passing_tds),\n    td_pct = touchdowns / attempts * 100,\n    interceptions = sum(interceptions),\n    int_pct = interceptions / attempts * 100,\n    first_down = sum(passing_first_downs),\n    yards_attempt = yards / attempts,\n    adj_yards_attempt = (yards + 20 * touchdowns - 45 * interceptions) / attempts,\n    yards_completions = yards / completions,\n    yards_game = yards / 17,\n    sacks = sum(sacks),\n    sack_yards = sum(sack_yards),\n    sack_pct = sacks / (attempts + sacks) * 100)\n\nmahomes_pfr\n\n# A tibble: 1 x 16\n  completions attempts cmp_pct yards touchdowns td_pct interceptions\n        &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;int&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n1         435      648   0.671  5250         41   6.33            12\n# i 9 more variables: int_pct &lt;dbl&gt;, first_down &lt;dbl&gt;,\n#   yards_attempt &lt;dbl&gt;, adj_yards_attempt &lt;dbl&gt;, ...\n\n\nWhat if we wanted to find Mahomes’ adjusted yards gained per pass attempt for every one of his seasons between 2018 and 2022? It is as simple as gathering all the data from years (seasons = 2018:2022), using the same filter() from above, and then using group_by() on the season variable.\n\nmahomes_adjusted_yards &lt;- nflreadr::load_player_stats(seasons = 2018:2022) %&gt;%\n  filter(player_name == \"P.Mahomes\" & season_type == \"REG\") %&gt;%\n  group_by(season) %&gt;%\n  summarize(\n    adj_yards_attempt = (sum(passing_yards) + 20 *\n                           sum(passing_tds) - 45 *\n                           sum(interceptions)) / sum(attempts))\n\nmahomes_adjusted_yards\n\n# A tibble: 5 x 2\n  season adj_yards_attempt\n   &lt;int&gt;             &lt;dbl&gt;\n1   2018              9.58\n2   2019              8.94\n3   2020              8.89\n4   2021              7.59\n5   2022              8.53\n\n\nBased on the data output, Mahomes’ best year for adjusted yards gained per pass attempt was 2018 with 9.58 adjusted yards. How does this compare to other quarterbacks from the same season? We can find the answer by doing slight modifications to our above code. Rather than filtering the information out to just Patrick Mahomes, we will add the player_name variable as the argument in the group_by() function.\n\nall_qbs_adjusted &lt;- nflreadr::load_player_stats(seasons = 2018) %&gt;%\n  filter(season_type == \"REG\" & position == \"QB\") %&gt;%\n  group_by(player_name) %&gt;%\n  summarize(\n    adj_yards_attempt = (sum(passing_yards) + 20 *\n                           sum(passing_tds) - 45 *\n                           sum(interceptions)) / sum(attempts)) %&gt;%\n  arrange(-adj_yards_attempt)\n\nall_qbs_adjusted\n\n# A tibble: 73 x 2\n   player_name   adj_yards_attempt\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 N.Sudfeld                 21   \n 2 G.Gilbert                 13.3 \n 3 M.Barkley                 11.7 \n 4 K.Allen                    9.87\n 5 C.Henne                    9.67\n 6 P.Mahomes                  9.58\n 7 M.Glennon                  9.24\n 8 D.Brees                    9.01\n 9 R.Wilson                   8.98\n10 R.Fitzpatrick              8.80\n# i 63 more rows\n\n\nMahomes, with an adjusted yards per pass attempt of 9.58, finishes in sixth place in the 2018 season behind C. Henne (9.67), K. Allen (9.87), and then three others that have results between 10 and 20. This is a situation where I tell my students the results do not pass the “eye test.” Why? It is unlikely that an adjusted yards per pass attempt of 21 and 13.3 for Nate Sudfeld and Garrett Gilbert, respectively, are from a season worth of data. The results for Kyle Allen and Matt Barkley are questionable, as well.\n\n\n\n\n\n\nImportant\n\n\n\nYou will often discover artificially-inflated statistics like above when the players have limited number of pass attempts/rushing attempts/receptions/etc. compared to the other players on the list. To confirm this, we can add pass_attempts = sum(attempts) to the above code to compare the number of attempts Sudfeld and Gilbert had compared to the rest of the list.\n\nall_qbs_adjusted_with_attempts &lt;-\n  nflreadr::load_player_stats(seasons = 2018) %&gt;%\n  filter(season_type == \"REG\" & position == \"QB\") %&gt;%\n  group_by(player_name) %&gt;%\n  summarize(\n    pass_attempts = sum(attempts),\n    adj_yards_attempt = (sum(passing_yards) + 20 *\n                           sum(passing_tds) - 45 *\n                           sum(interceptions)) / sum(attempts)) %&gt;%\n  arrange(-adj_yards_attempt)\n\nall_qbs_adjusted_with_attempts\n\n# A tibble: 73 x 3\n   player_name   pass_attempts adj_yards_attempt\n   &lt;chr&gt;                 &lt;int&gt;             &lt;dbl&gt;\n 1 N.Sudfeld                 2             21   \n 2 G.Gilbert                 3             13.3 \n 3 M.Barkley                25             11.7 \n 4 K.Allen                  31              9.87\n 5 C.Henne                   3              9.67\n 6 P.Mahomes               580              9.58\n 7 M.Glennon                21              9.24\n 8 D.Brees                 489              9.01\n 9 R.Wilson                427              8.98\n10 R.Fitzpatrick           246              8.80\n# i 63 more rows\n\n\nThe results are even worse than initially thought. There are a total of six players with an inflated adjusted yards per pass attempt:\n\nNate Sudfeld (2 attempts, 21 adjusted yards).\nGarrett Gilbert (3 attempts, 13.3 adjusted yards).\nMatt Barkley (25 attempts, 11.7 adjusted yards).\nKyle Allen (31 attempts, 9.87 adjusted yards).\nChris Henne (3 attempts, 9.67 adjusted yards).\n\nTo remove those players with inflated statistics resulting from a lack of attempts, we can apply a second filter() at the end of our above code to limit the results to just those quarterbacks with no less than 100 pass attempts in the 2018 season.\n\nall_qbs_attempts_100 &lt;-\n  nflreadr::load_player_stats(seasons = 2018) %&gt;%\n  filter(season_type == \"REG\" & position == \"QB\") %&gt;%\n  group_by(player_name) %&gt;%\n  summarize(\n    pass_attempts = sum(attempts),\n    adj_yards_attempt = (sum(passing_yards) + 20 *\n                           sum(passing_tds) - 45 *\n                           sum(interceptions)) / sum(attempts)) %&gt;%\n  filter(pass_attempts &gt;= 100) %&gt;%\n  arrange(-adj_yards_attempt)\n\nall_qbs_attempts_100\n\n\n\nAfter including a filter() to the code to remove those quarterbacks with less than 100 attempts, it is clear that Mahomes has the best adjusted pass yards per attempt (9.58) in the 2018 season with Drew Brees in second place with 9.01. Based on our previous examples, we know that a Mahomes 9.58 adjusted pass yards per attempt in 2018 was his career high (and is also the highest in the 2018 season). How do his other seasons stack up? Is his lowest adjusted pass yards (7.59) in 2021 still the best among NFL quarterbacks in that specific season?\nTo find the answer, we can make slight adjustments to our existing code.\n\nbest_adjusted_yards &lt;-\n  nflreadr::load_player_stats(seasons = 2018:2022) %&gt;%\n  filter(season_type == \"REG\" & position == \"QB\") %&gt;%\n  group_by(season, player_name) %&gt;%\n  summarize(\n    pass_attempts = sum(attempts),\n    adj_yards_attempt = (sum(passing_yards) + 20 *\n                           sum(passing_tds) - 45 *\n                           sum(interceptions)) / sum(attempts)) %&gt;%\n  filter(pass_attempts &gt;= 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(season) %&gt;%\n  filter(adj_yards_attempt == max(adj_yards_attempt))\n  \n  \nbest_adjusted_yards\n\n# A tibble: 5 x 4\n# Groups:   season [5]\n  season player_name  pass_attempts adj_yards_attempt\n   &lt;int&gt; &lt;chr&gt;                &lt;int&gt;             &lt;dbl&gt;\n1   2018 P.Mahomes              580              9.58\n2   2019 R.Tannehill            286             10.2 \n3   2020 A.Rodgers              526              9.57\n4   2021 J.Burrow               520              8.96\n5   2022 T.Tagovailoa           400              9.22\n\n\nTo get the answer, we’ve created code that follows along with our prior examples. However, after using filter() to limit the number of attempts each quarterback must have, we use ungroup() to remove the previous grouping between season and player_name and then use group_by() again on just the season information in the data. After, we use filter() to select just the highest adjusted yards per attempt for each individual season. As a result, we can see that Mahomes only led the NFL in this specific metric in the 2018 season. In fact, between the 2018 and 2022 seasons, Ryan Tannehill had the highest adjusted yards per attempt with 10.2 (but notice he had just 286 passing attempts).\nAs mentioned, the load_player_stats() information is provided on a week-by-week basis. In our above example, we are aggregating all regular season weeks into a season-long metric. Given the structure of the data, we can take a similar approach but to determine the leaders on a week-by-week basis. To showcase this, we can determine the leader in rushing EPA for every week during the 2022 regular season.\n\nrushing_epa_leader &lt;- nflreadr::load_player_stats(season = 2022) %&gt;%\n  filter(season_type == \"REG\" & position == \"RB\") %&gt;%\n  filter(!is.na(rushing_epa)) %&gt;%\n  group_by(week, player_name) %&gt;%\n  summarize(\n    carries = carries,\n    rush_epa = rushing_epa) %&gt;%\n  filter(carries &gt;= 10) %&gt;%\n  ungroup() %&gt;%\n  group_by(week) %&gt;%\n  filter(rush_epa == max(rush_epa))\n\n\n\n# A tibble: 18 x 4\n# Groups:   week [18]\n    week player_name carries rush_epa\n   &lt;int&gt; &lt;chr&gt;         &lt;int&gt;    &lt;dbl&gt;\n 1     1 D.Swift          15    10.0 \n 2     2 A.Jones          15     8.08\n 3     3 C.Patterson      17     5.76\n 4     4 R.Penny          17     9.73\n 5     5 A.Ekeler         16    11.6 \n 6     6 K.Drake          10     6.72\n 7     7 J.Jacobs         20     6.93\n 8     8 T.Etienne        24     9.68\n 9     9 J.Mixon          22    11.3 \n10    10 A.Jones          24     5.47\n11    11 J.Cook           11     2.92\n12    12 M.Sanders        21     6.72\n13    13 T.Pollard        12     3.97\n14    14 M.Sanders        17     8.95\n15    15 T.Allgeier       17    10.5 \n16    16 D.Foreman        21     7.22\n17    17 A.Ekeler         10    10.0 \n18    18 A.Mattison       10     5.85\n\n\nRather than using the season variable in the group_by() function, we instead use week to calculate the leader in rushing EPA. The results show that the weekly leader is varied, with only Austin Ekeler and Aaron Jones appearing on the list more than once. The Bengals’ Joe Mixon produced the largest rushing EPA in the 2022 season, recording 11.3 in week 9 while James Cook’s 2.92 output in week 11 was the “least” of the best weekly performances.\nMuch like we did with the passing statistics, we can use the rushing data in load_player_stats() to replicate the majority found on Pro Football Reference. To do so, let’s use Joe Mixon’s week 9 performance from the 2022 season.\n\nmixon_week_9 &lt;- nflreadr::load_player_stats(seasons = 2022) %&gt;%\n  filter(player_name == \"J.Mixon\" & week == 9) %&gt;%\n  summarize(\n    rushes = sum(carries),\n    yards = sum(rushing_yards),\n    yards_att = yards / rushes,\n    tds = sum(rushing_tds),\n    first_down = sum(rushing_first_downs))\n\nmixon_week_9\n\n# A tibble: 1 x 5\n  rushes yards yards_att   tds first_down\n   &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;\n1     22   153      6.95     4         12\n\n\nMixon gained a total of 153 yards on the ground on 22 carries, which is just under 7 yards per attempt. Four of those carries resulting in touchdowns, while 12 of them kept drives alive with first downs. Other contextual statistics for this performance, such as yards after contact, are not included in load_player_stats() but can be retrieved using other functions within the nflverse() (which are covered later in this chapter).\nFinally, we can use the load_player_stats() function to explore wide receiver performances. As listed above, the wide receivers statistics housed within load_player_stats() include those to be expected: yards, touchdowns, air yards, yards after catch, etc. Rather than working with those in an example, let’s use wopr which stands for Weighted Opportunity Rating, which is a weighted average that contextualizes how much value any one wide receivers bring to a fantasy football team. The equation is already built into the data, but for clarity it is as follows:\n\\[\n1.5 * target share + 0.7 * air yards share\n\\]\nWe can use wopr to determine which wide receiver was the most valuable to fantasy football owners during the 2022 season.\n\nreceivers_wopr &lt;- nflreadr::load_player_stats(seasons = 2022) %&gt;%\n  filter(season_type == \"REG\" & position == \"WR\") %&gt;%\n  group_by(player_name) %&gt;%\n  summarize(\n    total_wopr = sum(wopr)) %&gt;%\n  arrange(-total_wopr)\n\nreceivers_wopr\n\n# A tibble: 225 x 2\n   player_name total_wopr\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 D.Moore           13.9\n 2 D.Adams           13.4\n 3 T.Hill            12.4\n 4 A.Brown           12.2\n 5 J.Jefferson       11.9\n 6 C.Lamb            11.8\n 7 A.Cooper          11.8\n 8 D.Johnson         11.2\n 9 D.London          10.9\n10 D.Metcalf         10.7\n# i 215 more rows\n\n\nD.J. Moore led the NFL in the 2022 season with a 13.9 Weighted Opportunity Rating, followed closely by Davante Adams with 13.4. However, before deciding that D.J. Moore is worth the number one pick in your upcoming fantasy football draft, it is worth exploring if there is a relationship between a high Weight Opportunity Rating and a player’s total fantasy points. To do so, we can take our above code that was used to gather each player’s WOPR over the course of the season and add total_ff = sum(fantasy_points) and also calculate a new metric called “Fantasy Points per Weighted Opportunity” which is simply the total of a player’s points divided by the wopr.\n\nreceivers_wopr_ff_context &lt;-\n  nflreadr::load_player_stats(seasons = 2022) %&gt;%\n  filter(season_type == \"REG\" & position == \"WR\") %&gt;%\n  group_by(player_name) %&gt;%\n  summarize(\n    total_wopr = sum(wopr),\n    total_ff = sum(fantasy_points),\n    ff_per_wopr = total_ff / total_wopr) %&gt;%\n  arrange(-total_wopr)\n\nreceivers_wopr_ff_context\n\n# A tibble: 225 x 4\n   player_name total_wopr total_ff ff_per_wopr\n   &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n 1 D.Moore           13.9    136.         9.76\n 2 D.Adams           13.4    236.        17.6 \n 3 T.Hill            12.4    222.        17.9 \n 4 A.Brown           12.2    212.        17.4 \n 5 J.Jefferson       11.9    241.        20.3 \n 6 C.Lamb            11.8    195.        16.5 \n 7 A.Cooper          11.8    168         14.2 \n 8 D.Johnson         11.2     94.7        8.48\n 9 D.London          10.9    107.         9.77\n10 D.Metcalf         10.7    137.        12.7 \n# i 215 more rows\n\n\nThere are several insights evident in the results. Of the top wopr earners in the 2022 season. only Diontae Johnson scored fewer fantasy points than D.J. Moore (94.7 to 136). Given the relationship between total_wopr and total_ff, a lower ff_per_wopr is indicative of less stellar play. In this case, Moore maintained a large amount of both the team’s target_share and air_yards_share, but was not able to translate that into a higher amount of fantasy points (as evidenced by a lower ff_per_wopr score). On the other hand, Justin Jefferson’s ff_per_wopr score of 20.3 argues that he used his amount of target_share and air_yards_share to increase the amount of fantasy points he produced.\nBased on the above examples, it is clear that the load_player_stats() function is useful when needing to aggregate statistics on a weekly or season-long basis. While this does allow for easy matching of official statistics, it does not provide the ability to add context to the findings. For example, we know that Patrick Mahomes had the highest adjusted pass yards per attempt during the 2018 regular season with 9.58. But, what if we wanted to explore that same metric but only on passes that took place on 3rd down with 10 or less yards to go?\nUnfortunately, the load_player_stats() function does not provide ability to distill the statistics into specific situations. Because of this, we must turn to the load_pbp() function."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#using-load_pbp-to-add-context-to-statistics",
    "href": "03-nfl-analytics-functions.html#using-load_pbp-to-add-context-to-statistics",
    "title": "\n3  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n3.2 Using load_pbp() to Add Context to Statistics",
    "text": "3.2 Using load_pbp() to Add Context to Statistics\nUsing the load_pbp() function is preferable when you are looking to add context to a player’s statistics, as the load_player_stats() function is, for all intents and purposes, aggregated statistics that limit your ability to find deeper meaning.\nThe load_pbp() function provides over 350 various metrics, as listed below:\n\npbp.data &lt;- nflreadr::load_pbp(2022)\nls(pbp.data)\n\n  [1] \"aborted_play\"                        \n  [2] \"air_epa\"                             \n  [3] \"air_wpa\"                             \n  [4] \"air_yards\"                           \n  [5] \"assist_tackle\"                       \n  [6] \"assist_tackle_1_player_id\"           \n  [7] \"assist_tackle_1_player_name\"         \n  [8] \"assist_tackle_1_team\"                \n  [9] \"assist_tackle_2_player_id\"           \n [10] \"assist_tackle_2_player_name\"         \n [11] \"assist_tackle_2_team\"                \n [12] \"assist_tackle_3_player_id\"           \n [13] \"assist_tackle_3_player_name\"         \n [14] \"assist_tackle_3_team\"                \n [15] \"assist_tackle_4_player_id\"           \n [16] \"assist_tackle_4_player_name\"         \n [17] \"assist_tackle_4_team\"                \n [18] \"away_coach\"                          \n [19] \"away_score\"                          \n [20] \"away_team\"                           \n [21] \"away_timeouts_remaining\"             \n [22] \"away_wp\"                             \n [23] \"away_wp_post\"                        \n [24] \"blocked_player_id\"                   \n [25] \"blocked_player_name\"                 \n [26] \"comp_air_epa\"                        \n [27] \"comp_air_wpa\"                        \n [28] \"comp_yac_epa\"                        \n [29] \"comp_yac_wpa\"                        \n [30] \"complete_pass\"                       \n [31] \"cp\"                                  \n [32] \"cpoe\"                                \n [33] \"def_wp\"                              \n [34] \"defensive_extra_point_attempt\"       \n [35] \"defensive_extra_point_conv\"          \n [36] \"defensive_two_point_attempt\"         \n [37] \"defensive_two_point_conv\"            \n [38] \"defteam\"                             \n [39] \"defteam_score\"                       \n [40] \"defteam_score_post\"                  \n [41] \"defteam_timeouts_remaining\"          \n [42] \"desc\"                                \n [43] \"div_game\"                            \n [44] \"down\"                                \n [45] \"drive\"                               \n [46] \"drive_end_transition\"                \n [47] \"drive_end_yard_line\"                 \n [48] \"drive_ended_with_score\"              \n [49] \"drive_first_downs\"                   \n [50] \"drive_game_clock_end\"                \n [51] \"drive_game_clock_start\"              \n [52] \"drive_inside20\"                      \n [53] \"drive_play_count\"                    \n [54] \"drive_play_id_ended\"                 \n [55] \"drive_play_id_started\"               \n [56] \"drive_quarter_end\"                   \n [57] \"drive_quarter_start\"                 \n [58] \"drive_real_start_time\"               \n [59] \"drive_start_transition\"              \n [60] \"drive_start_yard_line\"               \n [61] \"drive_time_of_possession\"            \n [62] \"drive_yards_penalized\"               \n [63] \"end_clock_time\"                      \n [64] \"end_yard_line\"                       \n [65] \"ep\"                                  \n [66] \"epa\"                                 \n [67] \"extra_point_attempt\"                 \n [68] \"extra_point_prob\"                    \n [69] \"extra_point_result\"                  \n [70] \"fantasy\"                             \n [71] \"fantasy_id\"                          \n [72] \"fantasy_player_id\"                   \n [73] \"fantasy_player_name\"                 \n [74] \"fg_prob\"                             \n [75] \"field_goal_attempt\"                  \n [76] \"field_goal_result\"                   \n [77] \"first_down\"                          \n [78] \"first_down_pass\"                     \n [79] \"first_down_penalty\"                  \n [80] \"first_down_rush\"                     \n [81] \"fixed_drive\"                         \n [82] \"fixed_drive_result\"                  \n [83] \"forced_fumble_player_1_player_id\"    \n [84] \"forced_fumble_player_1_player_name\"  \n [85] \"forced_fumble_player_1_team\"         \n [86] \"forced_fumble_player_2_player_id\"    \n [87] \"forced_fumble_player_2_player_name\"  \n [88] \"forced_fumble_player_2_team\"         \n [89] \"fourth_down_converted\"               \n [90] \"fourth_down_failed\"                  \n [91] \"fumble\"                              \n [92] \"fumble_forced\"                       \n [93] \"fumble_lost\"                         \n [94] \"fumble_not_forced\"                   \n [95] \"fumble_out_of_bounds\"                \n [96] \"fumble_recovery_1_player_id\"         \n [97] \"fumble_recovery_1_player_name\"       \n [98] \"fumble_recovery_1_team\"              \n [99] \"fumble_recovery_1_yards\"             \n[100] \"fumble_recovery_2_player_id\"         \n[101] \"fumble_recovery_2_player_name\"       \n[102] \"fumble_recovery_2_team\"              \n[103] \"fumble_recovery_2_yards\"             \n[104] \"fumbled_1_player_id\"                 \n[105] \"fumbled_1_player_name\"               \n[106] \"fumbled_1_team\"                      \n[107] \"fumbled_2_player_id\"                 \n[108] \"fumbled_2_player_name\"               \n[109] \"fumbled_2_team\"                      \n[110] \"game_date\"                           \n[111] \"game_half\"                           \n[112] \"game_id\"                             \n[113] \"game_seconds_remaining\"              \n[114] \"game_stadium\"                        \n[115] \"goal_to_go\"                          \n[116] \"half_sack_1_player_id\"               \n[117] \"half_sack_1_player_name\"             \n[118] \"half_sack_2_player_id\"               \n[119] \"half_sack_2_player_name\"             \n[120] \"half_seconds_remaining\"              \n[121] \"home_coach\"                          \n[122] \"home_opening_kickoff\"                \n[123] \"home_score\"                          \n[124] \"home_team\"                           \n[125] \"home_timeouts_remaining\"             \n[126] \"home_wp\"                             \n[127] \"home_wp_post\"                        \n[128] \"id\"                                  \n[129] \"incomplete_pass\"                     \n[130] \"interception\"                        \n[131] \"interception_player_id\"              \n[132] \"interception_player_name\"            \n[133] \"jersey_number\"                       \n[134] \"kick_distance\"                       \n[135] \"kicker_player_id\"                    \n[136] \"kicker_player_name\"                  \n[137] \"kickoff_attempt\"                     \n[138] \"kickoff_downed\"                      \n[139] \"kickoff_fair_catch\"                  \n[140] \"kickoff_in_endzone\"                  \n[141] \"kickoff_inside_twenty\"               \n[142] \"kickoff_out_of_bounds\"               \n[143] \"kickoff_returner_player_id\"          \n[144] \"kickoff_returner_player_name\"        \n[145] \"lateral_interception_player_id\"      \n[146] \"lateral_interception_player_name\"    \n[147] \"lateral_kickoff_returner_player_id\"  \n[148] \"lateral_kickoff_returner_player_name\"\n[149] \"lateral_punt_returner_player_id\"     \n[150] \"lateral_punt_returner_player_name\"   \n[151] \"lateral_receiver_player_id\"          \n[152] \"lateral_receiver_player_name\"        \n[153] \"lateral_receiving_yards\"             \n[154] \"lateral_reception\"                   \n[155] \"lateral_recovery\"                    \n[156] \"lateral_return\"                      \n[157] \"lateral_rush\"                        \n[158] \"lateral_rusher_player_id\"            \n[159] \"lateral_rusher_player_name\"          \n[160] \"lateral_rushing_yards\"               \n[161] \"lateral_sack_player_id\"              \n[162] \"lateral_sack_player_name\"            \n[163] \"location\"                            \n[164] \"name\"                                \n[165] \"nfl_api_id\"                          \n[166] \"no_huddle\"                           \n[167] \"no_score_prob\"                       \n[168] \"old_game_id\"                         \n[169] \"opp_fg_prob\"                         \n[170] \"opp_safety_prob\"                     \n[171] \"opp_td_prob\"                         \n[172] \"order_sequence\"                      \n[173] \"out_of_bounds\"                       \n[174] \"own_kickoff_recovery\"                \n[175] \"own_kickoff_recovery_player_id\"      \n[176] \"own_kickoff_recovery_player_name\"    \n[177] \"own_kickoff_recovery_td\"             \n[178] \"pass\"                                \n[179] \"pass_attempt\"                        \n[180] \"pass_defense_1_player_id\"            \n[181] \"pass_defense_1_player_name\"          \n[182] \"pass_defense_2_player_id\"            \n[183] \"pass_defense_2_player_name\"          \n[184] \"pass_length\"                         \n[185] \"pass_location\"                       \n[186] \"pass_oe\"                             \n[187] \"pass_touchdown\"                      \n[188] \"passer\"                              \n[189] \"passer_id\"                           \n[190] \"passer_jersey_number\"                \n[191] \"passer_player_id\"                    \n[192] \"passer_player_name\"                  \n[193] \"passing_yards\"                       \n[194] \"penalty\"                             \n[195] \"penalty_player_id\"                   \n[196] \"penalty_player_name\"                 \n[197] \"penalty_team\"                        \n[198] \"penalty_type\"                        \n[199] \"penalty_yards\"                       \n[200] \"play\"                                \n[201] \"play_clock\"                          \n[202] \"play_deleted\"                        \n[203] \"play_id\"                             \n[204] \"play_type\"                           \n[205] \"play_type_nfl\"                       \n[206] \"posteam\"                             \n[207] \"posteam_score\"                       \n[208] \"posteam_score_post\"                  \n[209] \"posteam_timeouts_remaining\"          \n[210] \"posteam_type\"                        \n[211] \"punt_attempt\"                        \n[212] \"punt_blocked\"                        \n[213] \"punt_downed\"                         \n[214] \"punt_fair_catch\"                     \n[215] \"punt_in_endzone\"                     \n[216] \"punt_inside_twenty\"                  \n[217] \"punt_out_of_bounds\"                  \n[218] \"punt_returner_player_id\"             \n[219] \"punt_returner_player_name\"           \n[220] \"punter_player_id\"                    \n[221] \"punter_player_name\"                  \n[222] \"qb_dropback\"                         \n[223] \"qb_epa\"                              \n[224] \"qb_hit\"                              \n[225] \"qb_hit_1_player_id\"                  \n[226] \"qb_hit_1_player_name\"                \n[227] \"qb_hit_2_player_id\"                  \n[228] \"qb_hit_2_player_name\"                \n[229] \"qb_kneel\"                            \n[230] \"qb_scramble\"                         \n[231] \"qb_spike\"                            \n[232] \"qtr\"                                 \n[233] \"quarter_end\"                         \n[234] \"quarter_seconds_remaining\"           \n[235] \"receiver\"                            \n[236] \"receiver_id\"                         \n[237] \"receiver_jersey_number\"              \n[238] \"receiver_player_id\"                  \n[239] \"receiver_player_name\"                \n[240] \"receiving_yards\"                     \n[241] \"replay_or_challenge\"                 \n[242] \"replay_or_challenge_result\"          \n[243] \"result\"                              \n[244] \"return_team\"                         \n[245] \"return_touchdown\"                    \n[246] \"return_yards\"                        \n[247] \"roof\"                                \n[248] \"run_gap\"                             \n[249] \"run_location\"                        \n[250] \"rush\"                                \n[251] \"rush_attempt\"                        \n[252] \"rush_touchdown\"                      \n[253] \"rusher\"                              \n[254] \"rusher_id\"                           \n[255] \"rusher_jersey_number\"                \n[256] \"rusher_player_id\"                    \n[257] \"rusher_player_name\"                  \n[258] \"rushing_yards\"                       \n[259] \"sack\"                                \n[260] \"sack_player_id\"                      \n[261] \"sack_player_name\"                    \n[262] \"safety\"                              \n[263] \"safety_player_id\"                    \n[264] \"safety_player_name\"                  \n[265] \"safety_prob\"                         \n[266] \"score_differential\"                  \n[267] \"score_differential_post\"             \n[268] \"season\"                              \n[269] \"season_type\"                         \n[270] \"series\"                              \n[271] \"series_result\"                       \n[272] \"series_success\"                      \n[273] \"shotgun\"                             \n[274] \"side_of_field\"                       \n[275] \"solo_tackle\"                         \n[276] \"solo_tackle_1_player_id\"             \n[277] \"solo_tackle_1_player_name\"           \n[278] \"solo_tackle_1_team\"                  \n[279] \"solo_tackle_2_player_id\"             \n[280] \"solo_tackle_2_player_name\"           \n[281] \"solo_tackle_2_team\"                  \n[282] \"sp\"                                  \n[283] \"special\"                             \n[284] \"special_teams_play\"                  \n[285] \"spread_line\"                         \n[286] \"st_play_type\"                        \n[287] \"stadium\"                             \n[288] \"stadium_id\"                          \n[289] \"start_time\"                          \n[290] \"success\"                             \n[291] \"surface\"                             \n[292] \"tackle_for_loss_1_player_id\"         \n[293] \"tackle_for_loss_1_player_name\"       \n[294] \"tackle_for_loss_2_player_id\"         \n[295] \"tackle_for_loss_2_player_name\"       \n[296] \"tackle_with_assist\"                  \n[297] \"tackle_with_assist_1_player_id\"      \n[298] \"tackle_with_assist_1_player_name\"    \n[299] \"tackle_with_assist_1_team\"           \n[300] \"tackle_with_assist_2_player_id\"      \n[301] \"tackle_with_assist_2_player_name\"    \n[302] \"tackle_with_assist_2_team\"           \n[303] \"tackled_for_loss\"                    \n[304] \"td_player_id\"                        \n[305] \"td_player_name\"                      \n[306] \"td_prob\"                             \n[307] \"td_team\"                             \n[308] \"temp\"                                \n[309] \"third_down_converted\"                \n[310] \"third_down_failed\"                   \n[311] \"time\"                                \n[312] \"time_of_day\"                         \n[313] \"timeout\"                             \n[314] \"timeout_team\"                        \n[315] \"total\"                               \n[316] \"total_away_comp_air_epa\"             \n[317] \"total_away_comp_air_wpa\"             \n[318] \"total_away_comp_yac_epa\"             \n[319] \"total_away_comp_yac_wpa\"             \n[320] \"total_away_epa\"                      \n[321] \"total_away_pass_epa\"                 \n[322] \"total_away_pass_wpa\"                 \n[323] \"total_away_raw_air_epa\"              \n[324] \"total_away_raw_air_wpa\"              \n[325] \"total_away_raw_yac_epa\"              \n[326] \"total_away_raw_yac_wpa\"              \n[327] \"total_away_rush_epa\"                 \n[328] \"total_away_rush_wpa\"                 \n[329] \"total_away_score\"                    \n[330] \"total_home_comp_air_epa\"             \n[331] \"total_home_comp_air_wpa\"             \n[332] \"total_home_comp_yac_epa\"             \n[333] \"total_home_comp_yac_wpa\"             \n[334] \"total_home_epa\"                      \n[335] \"total_home_pass_epa\"                 \n[336] \"total_home_pass_wpa\"                 \n[337] \"total_home_raw_air_epa\"              \n[338] \"total_home_raw_air_wpa\"              \n[339] \"total_home_raw_yac_epa\"              \n[340] \"total_home_raw_yac_wpa\"              \n[341] \"total_home_rush_epa\"                 \n[342] \"total_home_rush_wpa\"                 \n[343] \"total_home_score\"                    \n[344] \"total_line\"                          \n[345] \"touchback\"                           \n[346] \"touchdown\"                           \n[347] \"two_point_attempt\"                   \n[348] \"two_point_conv_result\"               \n[349] \"two_point_conversion_prob\"           \n[350] \"vegas_home_wp\"                       \n[351] \"vegas_home_wpa\"                      \n[352] \"vegas_wp\"                            \n[353] \"vegas_wpa\"                           \n[354] \"weather\"                             \n[355] \"week\"                                \n[356] \"wind\"                                \n[357] \"wp\"                                  \n[358] \"wpa\"                                 \n[359] \"xpass\"                               \n[360] \"xyac_epa\"                            \n[361] \"xyac_fd\"                             \n[362] \"xyac_mean_yardage\"                   \n[363] \"xyac_median_yardage\"                 \n[364] \"xyac_success\"                        \n[365] \"yac_epa\"                             \n[366] \"yac_wpa\"                             \n[367] \"yardline_100\"                        \n[368] \"yards_after_catch\"                   \n[369] \"yards_gained\"                        \n[370] \"ydsnet\"                              \n[371] \"ydstogo\"                             \n[372] \"yrdln\"                               \n\n\nThe amount of information contained in the nflverse play-by-play data can be overwhelming. Luckily, the nflreadr website includes a searchable directory of all the variables with a brief description of what each one means. You can visit that here: nflreadr Field Descriptions.\nWe can recreate our examination of 2018 adjusted pass yards per attempt to just those passes on 3rd down with 10 or less yards to go by running the following code:\n\npbp &lt;- nflreadr::load_pbp(2018) %&gt;%\n  filter(season_type == \"REG\")\n\nadjusted_yards &lt;- pbp %&gt;%\n  group_by(passer_player_name) %&gt;%\n  filter(down == 3 & ydstogo &lt;= 10) %&gt;%\n  filter(complete_pass == 1 | incomplete_pass == 1 |\n           interception == 1 & \n           !is.na(down)) %&gt;%\n  summarize(\n    total_attempts = n(),\n    adj_yards = (sum(yards_gained) + 20 * sum(touchdown == 1) - 45 * \n                   sum(interception == 1)) / total_attempts) %&gt;%\n  filter(total_attempts &gt;= 50) %&gt;%\n  arrange(-adj_yards)\n\nadjusted_yards\n\n# A tibble: 32 x 3\n   passer_player_name total_attempts adj_yards\n   &lt;chr&gt;                       &lt;int&gt;     &lt;dbl&gt;\n 1 A.Rodgers                      98     11.3 \n 2 P.Mahomes                     102     10.3 \n 3 R.Wilson                      104     10.3 \n 4 E.Manning                     119      9.18\n 5 M.Mariota                      79      8.43\n 6 M.Ryan                        117      8.26\n 7 J.Winston                      66      8.05\n 8 D.Brees                        97      7.99\n 9 D.Prescott                    104      7.87\n10 J.Flacco                       74      7.84\n# i 22 more rows\n\n\nBecause the load_pbp() data is not pre-aggregated, we must do a bit of the work ourselves before we are able to achieve our answer. To start, we load the 2018 play-by-play data into a data frame titled pbp but using the load_pbp() function with the season argument set to “2018.” As well, we use the filter() function to make sure the data gathered into our pbp data frame is only the regular season statistics. Once the play-by-play is loaded, we create a data frame off of it titled adjusted_yards wherein we use group_by() to calculate the metric for each individual quarterback, then ensure that we are collecting the play-by-play data for only those plays that took place on 3rd down with ten or less yards to go.\nWe then use a second filter() to gather those plays where complete_pass == 1 or incomplete_pass == 1 or interception == 1 and the down is not missing (which usually indicates a 2-point conversion attempt). As a result, each quarterback has an exact match to the number of complete passes, attempts, and interceptions as found in the official statistics of the season.\nDespite leading the league in adjusted pass yard per attempt in 2018, Mahomes finished a whole yard behind Aaron Rodgers when exploring the same metric on 3rd down with 10 or less yards to go (and was tied for second place with Russel Wilson).\n\n\n\n\n\n\nTip\n\n\n\nWhy is it necessary to use such a long list of specifications in the filter() function in order to match the official QB statistics found elsewhere?\nThat is a good question, especially since there are other variables within the play-by-play data that would seemingly return the correct results (such as pass == 1 or play_type == \"pass\". We can view the differences in all three methods by doing the following.\n\npbp &lt;- nflreadr::load_pbp(2018) %&gt;%\n  filter(season_type == \"REG\")\n\npbp %&gt;%\n  filter(complete_pass == 1 | incomplete_pass == 1 |\n           interception == 1 &\n           !is.na(down)) %&gt;%\n  filter(passer_player_name == \"P.Mahomes\") %&gt;%\n  summarize(total_attempts = n())\n\n# A tibble: 1 x 1\n  total_attempts\n           &lt;int&gt;\n1            580\n\npass_numbers &lt;- pbp %&gt;%\n  filter(passer_player_name == \"P.Mahomes\") %&gt;%\n  summarize(\n    using_pass = sum(pass == 1, na.rm = TRUE),\n    using_playtype = sum(play_type == \"pass\"),\n    using_filter = 580)\n\npass_numbers\n\n# A tibble: 1 x 3\n  using_pass using_playtype using_filter\n       &lt;int&gt;          &lt;int&gt;        &lt;dbl&gt;\n1        606            606          580\n\n\nWe know that Patrick Mahomes attempted 580 passes during the 2022 regular season. We can match that number exactly using the filter() method for complete_pass, incomplete_pass, interception, and removing plays with a missing down observation. When trying to replicate this number using either the pass == 1 variable in the play-by-play data or play_type == \"pass\", we get just over 20 more passes than expected.\nThe main reason for this is the inclusion of the qb_spike, qb_scramble, and sack variables. We can actually use the code above that was used to create the pass_numbers data frame, add an additional filter() to account for these, and get the correct number of passing attempts for each method.\n\npass_numbers_correct &lt;- pbp %&gt;%\n  filter(passer_player_name == \"P.Mahomes\") %&gt;%\n  filter(qb_spike == 0 & qb_scramble == 0 & sack == 0) %&gt;%\n  summarize(\n    using_pass = sum(pass == 1, na.rm = TRUE),\n    using_playtype = sum(play_type == \"pass\"),\n    using_filter = 580)\n\npass_numbers_correct\n\n# A tibble: 1 x 3\n  using_pass using_playtype using_filter\n       &lt;int&gt;          &lt;int&gt;        &lt;dbl&gt;\n1        580            580          580\n\n\nBy using filter() to remove any play that included a QB spike, a QB scramble, or a sack, we get 580 attempts when using both pass == 1 and play_type == \"pass\" which replicates the official statistics from the 2018 season for Patrick Mahomes.\nWhat about the use of passer vs. passer_player_name?\nIt is also important to remember that you will receive differing numbers based on your use of passer and passer_player_name. The passer variable is created internally by the nflverse system and is used to “mimic” the statistics without spikes, scrambles, or sacks in the data. The passer_player_name variable comes from the official statistics and inherently includes this information.\nThe following, with the first method using group_by(passer) and the second using group_by(passer_player_name) returns differing results.\n\npasser_grouping &lt;- pbp %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1 & !is.na(down)) %&gt;%\n  group_by(passer) %&gt;%\n  summarize(total_attempts = n()) %&gt;%\n  arrange(-total_attempts) %&gt;%\n  slice(1:5)\n\npasser_player_grouping &lt;- pbp %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1 & !is.na(down)) %&gt;%\n  group_by(passer_player_name) %&gt;%\n  summarize(total_attempts = n()) %&gt;%\n  arrange(-total_attempts) %&gt;%\n  slice(1:5)\n\npasser_grouping\n\n# A tibble: 5 x 2\n  passer           total_attempts\n  &lt;chr&gt;                     &lt;int&gt;\n1 B.Roethlisberger            672\n2 A.Luck                      637\n3 M.Ryan                      607\n4 K.Cousins                   603\n5 A.Rodgers                   595\n\npasser_player_grouping\n\n# A tibble: 5 x 2\n  passer_player_name total_attempts\n  &lt;chr&gt;                       &lt;int&gt;\n1 B.Roethlisberger              675\n2 A.Luck                        639\n3 M.Ryan                        608\n4 K.Cousins                     606\n5 A.Rodgers                     597\n\n\nWhen grouping by passer, we get a total attempts of 672 for Ben Roethlisberger. That number increases to 675 when grouping by passer_player_name. Again, this is because the passer variable is created by the nflverse and automatically removes spikes, scrambles, and sacks while passer_player_name includes all three.\nWe can run the following code to determine where the difference of three passing attempts is coming from between passer and passer_player_name.\n\nroethlisberger_difference &lt;- pbp %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1 & !is.na(down)) %&gt;%\n  group_by(passer_player_name) %&gt;%\n  summarize(total_attempts = n(),\n            spikes = sum(qb_spike == 1),\n            scramble = sum(qb_scramble == 1),\n            sacks = sum(sack == 1)) %&gt;%\n  arrange(-total_attempts) %&gt;%\n  slice(1:5)\n\nroethlisberger_difference\n\n# A tibble: 5 x 5\n  passer_player_name total_attempts spikes scramble sacks\n  &lt;chr&gt;                       &lt;int&gt;  &lt;int&gt;    &lt;int&gt; &lt;int&gt;\n1 B.Roethlisberger              675      3        0     0\n2 A.Luck                        639      2        0     0\n3 M.Ryan                        608      1        0     0\n4 K.Cousins                     606      3        0     0\n5 A.Rodgers                     597      2        0     0\n\n\nIn the case of Roethlisberger, the three pass attempt difference was the result of the passer_player_name grouping including three QB spikes in the data. Aside from when attempting to replicate the official statistics, it is better to use just passer as it removes those instances where a QB spike, scramble, or sack my skew the results of your data.\n\n\nTo continue working with load_pbp() data, let’s create a metric that examines a QB’s aggressiveness on 3rd down passing attempts. The metric is designed to determine which QBs in the NFL are most aggressive in 3rd down situations by gauging how often they throw the ball to, or pass, the first down line. It is an interesting metric to explore as, just like many metrics in the NFL, not all air yards are created equal. For example, eight air yards on 1st and 10 are less valuable than the same eight air yards on 3rd and 5.\n\npbp &lt;- nflreadr::load_pbp(2022)\n\naggressiveness &lt;- pbp %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1 &\n           !is.na(down)) %&gt;%\n  filter(down == 3 & ydstogo &gt;= 5 & ydstogo &lt;= 10) %&gt;%\n  group_by(passer) %&gt;%\n  summarize(\n    team = last(posteam),\n    total = n(),\n    aggressive = sum(air_yards &gt;= ydstogo, na.rm = TRUE),\n    percentage = aggressive / total) %&gt;%\n  filter(total &gt;= 50) %&gt;%\n  arrange(-percentage) %&gt;%\n  slice(1:10)\n\naggressiveness\n\n# A tibble: 10 x 5\n   passer    team  total aggressive percentage\n   &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;\n 1 P.Mahomes KC       75         55      0.733\n 2 K.Pickett PIT      54         36      0.667\n 3 M.Jones   NE       56         36      0.643\n 4 D.Carr    LV       74         47      0.635\n 5 R.Wilson  DEN      63         40      0.635\n 6 G.Smith   SEA      65         41      0.631\n 7 A.Dalton  NO       55         34      0.618\n 8 J.Hurts   PHI      65         40      0.615\n 9 K.Cousins MIN      84         51      0.607\n10 J.Allen   BUF      61         35      0.574\n\n\nFollowing our above example, we use filter() on the play-by-play data to calculate the total number of attempts for each quarterback on 3rd down when there were between 5 and 10 yards to go for a first down. Because we want to make sure we are not gathering attempts with spikes, scrambles, etc., we use passer in the group_by() function and then calculate the total attempts for each quarterback. After, we find the total number of times each quarterback was aggressive by finding the sum of attempts where the air_yards of the pass were greater than or equal to the required ydstogo. The two numbers are then divided (aggressive / total) to get each quarterback’s percentage.\nAs you can see in the output of aggressiveness, Mahomes was the most aggressive quarterback in 3rd down passing situation in the 2022 season, passing to, or beyond, the line of gain just over 73% of the time.\nOne item to consider, however, is the concept of “garbage time.” Are the above results, perhaps, skewed by including quarterbacks that were forced to work the ball downfield while attempting a game-winning drive?\n\n3.2.0.1 QB Aggressiveness: Filtering for “Garbage Time?”\nTo examine the impact of “garbage time” statistics, we can add an additional filter() that removes those plays that take place within the last 2-minutes of the half and when the probability of winning for either team is over 95% or under 5%.\nLet’s add the “garbage time” filter to the code we’ve already prepared:\n\naggressiveness_garbage &lt;- pbp %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1 &\n           !is.na(down)) %&gt;%\n  filter(down == 3 & ydstogo &gt;= 5 & ydstogo &lt;= 10) %&gt;%\n  filter(wp &gt; .05 & wp &lt; .95 & half_seconds_remaining &gt; 120) %&gt;%\n  group_by(passer) %&gt;%\n  summarize(\n    team = last(posteam),\n    total = n(),\n    aggressive = sum(air_yards &gt;= ydstogo, na.rm = TRUE),\n    percentage = aggressive / total) %&gt;%\n  filter(total &gt;= 50) %&gt;%\n  arrange(-percentage) %&gt;%\n  slice(1:10)\n  \naggressiveness_garbage\n\n# A tibble: 10 x 5\n   passer     team  total aggressive percentage\n   &lt;chr&gt;      &lt;chr&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;\n 1 P.Mahomes  KC       67         48      0.716\n 2 K.Cousins  MIN      63         41      0.651\n 3 G.Smith    SEA      50         32      0.64 \n 4 D.Carr     LV       61         38      0.623\n 5 D.Prescott DAL      52         28      0.538\n 6 J.Herbert  LAC      71         38      0.535\n 7 J.Burrow   CIN      67         35      0.522\n 8 T.Lawrence JAX      69         35      0.507\n 9 T.Brady    TB       70         35      0.5  \n10 A.Rodgers  GB       53         26      0.491\n\n\nWe are now using the same code, but have included three new items to the filter(). First, we are stipulating that, aside from the down and distance inclusion, we only want those plays that occurred when the offense’s win probability (the wp variable) was between 5% and 95%, as well as ensuring that the plays did not happen after the two-minute warning of either half.\nThe decision on range of the win probability numbers is very much a personal preference. When nflfastR was first released, analyst often used a 20-80% range for the win probability cutoff point. However, Sebastian Carl - one of the creators of the nflverse explained in the package’s Discord:\n\n\nSebastian Carl: “I am generally very conservative with filtering plays using wp. Especially the vegas wp model can reach &gt;85% probs early in the game because it incorporates market lines. I never understood the 20% &lt;= wp &lt;= 80%”garbage time” filter. This is removing a ton of plays. My general advice is a lower boundary of something around 5% (i.e., 5% &lt;= wp &lt;= 95%).\n\n\nBen Baldwin followed up on Carl’s thoughts:\n\n\nBen Baldwin: “agree with this. 20-80% should only be used as a filter for looking at how run-heavy a team is (because outside of this range is when teams change behavior a lot). and possibly how teams behave on 4th downs. but not for team or player performance.”\n\n\nBased on that advice, I typically stick to the 5-95% range when filtering for win probability in the nflverse play-by-play data. And, in this case, it did have an impact. In fact, when accounting for “garbage time,” Kenny Pickett went from being the second most aggressive QB in the league to not even being in the top ten. Conversely, Kirk Cousin went from being the ninth most aggressive quarterback to the second when accounting for win probability and the time left in the half.\nThat said, I more often than not do not concern myself with removing “garbage time” statistics. Despite the robust amount of data provided by the nflverse play-by-play function, the information still lacks great amount of granularity and, because of this, I believe removing “garbage time” plays often does more harm than good in the data analysis process."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#more-examples-using-load_pbp-data",
    "href": "03-nfl-analytics-functions.html#more-examples-using-load_pbp-data",
    "title": "\n3  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n3.3 More Examples Using load_pbp() Data",
    "text": "3.3 More Examples Using load_pbp() Data\n\n3.3.1 QB Air Yards EPA by Down\nTo continue working with quarterback information using the load_pbp() function, we can explore each quarterback’s air yards EPA for 1st, 2nd, and 3rd down.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\nqb_ay_by_down &lt;- pbp %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1 &\n           !is.na(down)) %&gt;%\n  filter(down &lt;= 3) %&gt;%\n  group_by(passer, down) %&gt;%\n  summarize(\n    attempts = n(),\n    team = last(posteam),\n    mean_airepa = mean(air_epa, na.rm = TRUE)) %&gt;%\n  filter(attempts &gt;= 80) %&gt;%\n  arrange(-mean_airepa)\n\nqb_ay_by_down\n\nJosh Allen, Dak Prescott, and Tua Tagovailoa were all outstanding on 3rd down during the 2022 season, all recording over 1.00 in mean air yards EPA.\n\n3.3.2 Measuring Impact of Offensive Line\nBrian Burke - prior to moving to ESPN - used to host his own blog where he showcased his analytics work. In November of 2014, he wrote a post that detailed his process in determining how to value an offensive line’s performance over the course of an NFL season.\nThe process of making such a valuation is understandably difficult as, as Burke pointed out, an offensive line’s performance on the field is often characterized by the overall absence of stats. The less sacks, short yardage plays, tackles for loss, and quarterback scrambles - for example - the better. But how can the performance of an offensive line be quantified based on statistics that are absent?\nTo do so, Burke devised a rather ingenious method that is simple at its core: to measure the value of an offensive line’s play, we must determine the impact in which the opposing defensive line had on the game.\nIn his 2014 work on the topic, Burke used quarterback sacks, tackles for losses, short gains, tipped passes, and quarterback hits to provide a quantifiable valuation to a defensive line to then calculate the opposing valuation of the offensive line. Building upon this philosophy, we can build the same sort of study using the publicly available data in the nflverse play-by-play data by first using the following standard metrics:\n\nsacks\nQB hits\ntackles for loss\nyards gained less than/equal to 2\nforcing a QB scramble\n\nTo add more context, we can also create two new variables in the data that dictate whether a qb_hit and incomplete_pass are present in the same play, and the same for qb_hit and interception.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  mutate(qbh_inc = ifelse(qb_hit == 1 & incomplete_pass == 1, 1,0),\n         qb_int = ifelse(qb_hit == 1 & interception == 1, 1,0))\n\npitt_plays &lt;- pbp %&gt;%\n  filter(posteam == \"PIT\") %&gt;%\n  group_by(week) %&gt;%\n  summarize(\n    total_plays = sum(play_type == \"run\" |\n                        play_type == \"pass\", na.rm = TRUE))\n\npitt_line_value &lt;- pbp %&gt;%\n  filter(sack == 1 |\n           tackled_for_loss == 1 |\n           yards_gained &lt;= 2 |\n           qb_scramble == 1 |\n           qb_hit == 1 |\n           qbh_inc == 1 |\n           qb_int == 1) %&gt;%\n  filter(posteam == \"PIT\") %&gt;%\n  group_by(posteam, week) %&gt;%\n  left_join(pitt_plays, by = c(\"week\" = \"week\")) %&gt;%\n  summarize(opponent = unique(defteam),\n            sum_wpa = sum(wpa, na.rm = TRUE),\n            avg_wpa = (sum_wpa / unique(total_plays) * 100))\n\nHowever, as Burke pointed out in his initial study of the topic, calculating the number for just Pittsburgh, as above, does not provide enough information to draw a meaningful conclusion because even the most elite offensive lines in the NFL cannot avoid negative-pointed plays. In order to build this assumption into the data, we can gather the same information as above, but for the entire NFL minus the Steelers, and then calculate the difference in the Steelers’ weekly WPA to the league-wide average WPA.\n\nnfl_plays &lt;- pbp %&gt;%\n  filter(posteam != \"PIT\") %&gt;%\n  group_by(week, posteam) %&gt;%\n  summarize(total_plays = sum(play_type == \"run\" | play_type == \"pass\",\n                              na.rm = TRUE))\n\nnfl_line_value &lt;- pbp %&gt;%\n  filter(posteam != \"PIT\") %&gt;%\n  filter(sack == 1 |\n           tackled_for_loss == 1 |\n           yards_gained &lt;= 2 |\n           qb_scramble == 1 |\n           qb_hit == 1 |\n           qbh_inc == 1 |\n           qb_int == 1) %&gt;%\n  left_join(nfl_plays,\n            by = c(\"posteam\" = \"posteam\", \"week\" = \"week\")) %&gt;%\n  group_by(week) %&gt;%\n  mutate(nfl_weekly_plays = sum(unique(total_plays))) %&gt;%\n  summarize(nfl_sum_wpa = sum(wpa, na.rm = TRUE),\n            nfl_avg_wpa = (nfl_sum_wpa / unique(nfl_weekly_plays) * 100))\n\nWe can now merge the NFL data back into the Steelers data to conduct the final calculation.\n\npitt_line_value &lt;- pitt_line_value %&gt;%\n  left_join(nfl_line_value, by = c(\"week\" = \"week\"))\n\npitt_line_value &lt;- pitt_line_value %&gt;%\n  mutate(final_value = avg_wpa - nfl_avg_wpa) %&gt;%\n  select(week, opponent, final_value)\n\npitt_line_value %&gt;%\n  print(n = 17)\n\nThe output provides a quantitative value of the Steelers’ offensive line performance over the course of the season in the final_value column. We’ve structured the data so that we can make the argument that the Steelers’ offensive line, in week 1, provided 1.55% more WPA than the rest of the NFL. In week 11, against New Orleans, the offensive line added a 1.11% chance of winning the game compared to all the other offensive line performances that specific week.\nConversely, negative numbers indicate that the play of the offensive line took away that percentage amount towards the probability of winning.\nUnsurprisingly, the numbers across the board for the Steelers’ offensive line remain small, indicated that the group’s performance provided little to no help in earning victories for Pittsburgh."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#retrieving-working-with-data-for-multiple-seasons",
    "href": "03-nfl-analytics-functions.html#retrieving-working-with-data-for-multiple-seasons",
    "title": "\n3  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n3.4 Retrieving & Working With Data for Multiple Seasons",
    "text": "3.4 Retrieving & Working With Data for Multiple Seasons\nIn the case of both load_pbp() and load_player_stats(), it is possible to load data over multiple seasons.\nIn our above example calculating average air yard per attempt, it is important to note that Russell Wilson’s league-leading average of 9.89 air yards per attempt is calculated using all passing attempts, meaning pass attempts that were both complete and incomplete.\nIn our first example of working with data across multiple seasons, let’s examine average air yards for only completed passes. To begin, we will retrieve the play-by-play data for the last five seasons:\n\nay_five_years &lt;- nflreadr::load_pbp(2017:2022)\n\nTo retrieve multiple seasons of data, a colon : is placed between the years that you want. When you run the code, nflreadr will output the data to include the play-by-play data starting with the oldest season (in this case, the 2017 NFL season).\nOnce you have the data collected, we can run code that looks quite similar to our code above that explored 2021’s air yards per attempt leaders using load_player_stats(). In this case, however, we are including an additional filter() to gather those passing attempts that resulted only in complete passes:\n\naverage_airyards &lt;- ay_five_years %&gt;%\n  group_by(passer_id) %&gt;%\n  filter(season_type == \"REG\" & complete_pass == 1) %&gt;%\n  summarize(player = first(passer_player_name),\n            completions = sum(complete_pass),\n            air.yards = sum(air_yards),\n            average = air.yards / completions) %&gt;%\n  filter(completions &gt;= 1000) %&gt;%\n  arrange(-average)\n\naverage_airyards\n\n# A tibble: 28 x 5\n   passer_id  player      completions air.yards average\n   &lt;chr&gt;      &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 00-0031503 J.Winston          1081      8830    8.17\n 2 00-0033537 D.Watson           1285      9019    7.02\n 3 00-0034857 J.Allen            1604     10912    6.80\n 4 00-0029263 R.Wilson           1895     12742    6.72\n 5 00-0034796 L.Jackson          1055      7046    6.68\n 6 00-0026143 M.Ryan             2263     14694    6.49\n 7 00-0033077 D.Prescott         1874     12105    6.46\n 8 00-0034855 B.Mayfield         1386      8889    6.41\n 9 00-0029701 R.Tannehill        1261      8043    6.38\n10 00-0026498 M.Stafford         1874     11763    6.28\n# i 18 more rows\n\n\nOf those QBs with at least 1,000 complete passes since the 2017 season, Jameis Winston has the highest average air yards per complete pass at 8.17."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#working-with-the-various-nflreadr-functions",
    "href": "03-nfl-analytics-functions.html#working-with-the-various-nflreadr-functions",
    "title": "\n3  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n3.5 Working with the Various nflreadr Functions",
    "text": "3.5 Working with the Various nflreadr Functions\nThe nflreadr package comes with a multitude of “under the hood” functions designed to provide you with supplemental data, items for data visualization, and utilities for efficiently collecting and storing the data on your system. You can view the entire list of these options using the ls to output all the objects in the package.\n\nls(\"package:nflreadr\")\n\n [1] \"clean_homeaway\"               \"clean_player_names\"          \n [3] \"clean_team_abbrs\"             \"clear_cache\"                 \n [5] \"csv_from_url\"                 \"dictionary_combine\"          \n [7] \"dictionary_contracts\"         \"dictionary_depth_charts\"     \n [9] \"dictionary_draft_picks\"       \"dictionary_espn_qbr\"         \n[11] \"dictionary_ff_opportunity\"    \"dictionary_ff_playerids\"     \n[13] \"dictionary_ff_rankings\"       \"dictionary_injuries\"         \n[15] \"dictionary_nextgen_stats\"     \"dictionary_participation\"    \n[17] \"dictionary_pbp\"               \"dictionary_pfr_passing\"      \n[19] \"dictionary_player_stats\"      \"dictionary_rosters\"          \n[21] \"dictionary_schedules\"         \"dictionary_snap_counts\"      \n[23] \"dictionary_trades\"            \"ffverse_sitrep\"              \n[25] \"get_current_season\"           \"get_current_week\"            \n[27] \"get_latest_season\"            \"join_coalesce\"               \n[29] \"load_combine\"                 \"load_contracts\"              \n[31] \"load_depth_charts\"            \"load_draft_picks\"            \n[33] \"load_espn_qbr\"                \"load_ff_opportunity\"         \n[35] \"load_ff_playerids\"            \"load_ff_rankings\"            \n[37] \"load_from_url\"                \"load_injuries\"               \n[39] \"load_nextgen_stats\"           \"load_officials\"              \n[41] \"load_participation\"           \"load_pbp\"                    \n[43] \"load_pfr_advstats\"            \"load_pfr_passing\"            \n[45] \"load_player_stats\"            \"load_players\"                \n[47] \"load_rosters\"                 \"load_rosters_weekly\"         \n[49] \"load_schedules\"               \"load_snap_counts\"            \n[51] \"load_teams\"                   \"load_trades\"                 \n[53] \"most_recent_season\"           \"nflverse_download\"           \n[55] \"nflverse_game_id\"             \"nflverse_releases\"           \n[57] \"nflverse_sitrep\"              \"parquet_from_url\"            \n[59] \"player_name_mapping\"          \"progressively\"               \n[61] \"qs_from_url\"                  \"raw_from_url\"                \n[63] \"rbindlist_with_attrs\"         \"rds_from_url\"                \n[65] \"team_abbr_mapping\"            \"team_abbr_mapping_norelocate\"\n\n\nGoing forward in this chapter, we will be exploring specific use cases for the functions provided by nflreadr - but not all of them. For example, the dictionary_ functions can more easily be used directly on the nflreadr website where the package’s maintainers keep a copy of each. Many, like the dictionary for play-by-play data, includes a search feature to allow you to quickly find how the variables you are looking for is provided in the column name. Others, like the clear_cache() function is used only when you want to wipe any memoized data stored by nflreadr - often needed if you are troubleshooting a pesky error message - or join_coalesce() which is an experimental function that is only used internally to help build player IDs into the data. The load_pbp() and load_player_stats() function will also not be covered here, as the first portion of this chapter explored the use of both in great detail. We will briefly discuss the use of load_players() and load_rosters() but a more detailed discussion of each is provided in Chapter 4: Data Visualization with NFL Analytics. The remaining functions will be presented in the order provided on the nflreadr function reference website.\n\n3.5.1 The load_participation() Function\nThe load_participation() function allows us to create a data frame of player participation data dating back to 2016 with an option to infuse that information into the existing nflreadr play-by-play data. The resulting data frame, when not including play-by-play data, includes information pertaining to: the individual game ID, the play ID, the possession team, what formation the offense was in, the layout of the offensive personnel, how many defenders were in the box, the defensive personnel, the number of rushers on pass players, and the unique ID number for each player on the field for that specific play.\nDespite the load_participation() function being one of the newest editions to the nflreadr package, it is already being used to create contextual analysis regarding a team’s use of its players out out formations. For example, Joseph Hefner uses the data to create tables (built with the gt and gtExtras packages) that calculates not only each player’s rate of usage out of different personnel packages, but how the team’s EPA per play and pass rate fluctuate with each. In the spirit of R’s open-source nature, he also created a Team Formation ShinyApp that allows anybody to explore the data and output the results in .png format.\n\n\nTo build a data frame of 2022 participation data, that includes complete play-by-play, you must pass both the season and include_pbp argument with the load_participation() function.\n\nparticipation &lt;-\n  nflreadr::load_participation(season = 2022, include_pbp = TRUE)\n\nparticipation\n\n# A tibble: 49,969 x 383\n   nflverse_game_id play_id possession_team offense_formation\n   &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;            \n 1 2022_01_BAL_NYJ        1 \"\"              &lt;NA&gt;             \n 2 2022_01_BAL_NYJ       43 \"BAL\"           &lt;NA&gt;             \n 3 2022_01_BAL_NYJ       68 \"NYJ\"           SINGLEBACK       \n 4 2022_01_BAL_NYJ       89 \"NYJ\"           SHOTGUN          \n 5 2022_01_BAL_NYJ      115 \"NYJ\"           SINGLEBACK       \n 6 2022_01_BAL_NYJ      136 \"NYJ\"           SHOTGUN          \n 7 2022_01_BAL_NYJ      172 \"NYJ\"           &lt;NA&gt;             \n 8 2022_01_BAL_NYJ      202 \"BAL\"           SINGLEBACK       \n 9 2022_01_BAL_NYJ      230 \"BAL\"           SHOTGUN          \n10 2022_01_BAL_NYJ      254 \"BAL\"           EMPTY            \n# i 49,959 more rows\n# i 379 more variables: offense_personnel &lt;chr&gt;, ...\n\n\nRather than exploring the data at the league-level, let’s take a micro-approach to the participation data and examine the the 2022 Pittsburgh Steelers. It is important to note that the participation data, as gathered using the load_participation() function, is not fully prepped for immediate analysis as the players_on_play, offense_players, and defense_players information (which contains the unique player identification numbers) are separated by a delimiter (in this case, a semicolon) in what are called “concatenated strings” or “delimited strings.” While this is a compact way to store the values in the core data frame, it does require us to clean the information through the “splitting” or “tokenizing” process. As you will see in the below code, we are going to place the unique identifiers into separate rows, based on formation, by utilizing the separate_rows() function in tidyr.\n\n\n\n\n\n\nTip\n\n\n\nIf you use load_participation() but set include_pbp = FALSE it is important to remember that the posteam variable that is part of the play-by-play data will not exist. Instead, the offensive team is indicated by the possession_team column.\nIf you gather the participation data with play-by-play information, you can use either possession_team or posteam for any filtering.\nThe same is not true when looking at defensive participation data. A defensive team variable is not provided without including the play-by-play in the data. Once set to TRUE, the defensive team is housed under the typical defteam column.\n\n\n\nparticipation_split &lt;- participation %&gt;%\n  filter(!is.na(offense_formation)) %&gt;%\n  filter(posteam == \"PIT\") %&gt;%\n  tidyr::separate_rows(offense_players, sep = \";\") %&gt;%\n  group_by(offense_personnel, offense_players) %&gt;%\n  summarize(total = n()) %&gt;%\n  ungroup()\n\nparticipation_split\n\n# A tibble: 157 x 3\n   offense_personnel offense_players total\n   &lt;chr&gt;             &lt;chr&gt;           &lt;int&gt;\n 1 1 RB, 1 TE, 3 WR  00-0032897          3\n 2 1 RB, 1 TE, 3 WR  00-0033869        262\n 3 1 RB, 1 TE, 3 WR  00-0034142         37\n 4 1 RB, 1 TE, 3 WR  00-0034347        781\n 5 1 RB, 1 TE, 3 WR  00-0034768        781\n 6 1 RB, 1 TE, 3 WR  00-0034785        744\n 7 1 RB, 1 TE, 3 WR  00-0034928        235\n 8 1 RB, 1 TE, 3 WR  00-0035216        745\n 9 1 RB, 1 TE, 3 WR  00-0035217         25\n10 1 RB, 1 TE, 3 WR  00-0035222        258\n# i 147 more rows\n\n\nWe are doing quite a few things in the above example:\n\n\nThose rows where the is an ‘NA’ value for offense_formation are removed. The participation data includes information for kickoffs, extra points, field goals, punts, QB kneels, no plays, and more. It is possible to remove items using play_type == \"run\" | play_type == \"pass\" but such an approach, if done with the Steelers, results in one fake punt being included in the data (as the play_type was listed as “run”). If you wish to include plays such as fake punts, you do so by filtering for only specific play types.\n\nOnly those rows that includes ‘PIT’ as the posteam are included.\n\n\nThe separate_rows() function from tidyr is used to conduct the splitting of the concatenated players identifiers in the offense_players column. The separate_rows() function needs just two arguments to complete the process - the column name and the delimiter (provided in the argument as sep =).\n\nThe newly split data is then grouped by offense_personnel and offense_players in order to calculate the number of times each specific ID was associated with personnel package. The resulting data frame lists the offense formation type for each player that participated in it, along with the player’s respective participation count.\n\nAs is, the load_participation() data does not include any way to further identify players outside of the unique players IDs. Because of this, it is necessary to use the load_rosters() function to include this information (this process is included below in the load_rosters() section).\n\n3.5.2 The load_rosters() and load_rosters_weekly() Functions\nThe load_rosters() function houses a multitude of useful tools for use in the nflverse. With player information dating back to 1920, load_rosters() will provide you with basic demographic information about each player in the NFL such as their name, birth date, height, weight, the college and high school they attended, and items regarding primary position and depth chart position.\nImportantly, load_rosters() also provides the unique identifier for each player over nine different sources: the gsis_id (which is the core ID used in nflverse data), sleeper_id, espn_id, yahoo_id, rotowire_id, pff_id, fantasy_data_id, sportradar_id, and pfr_id. The different IDs become extremely useful when using data collected from outside the nflverse, or when combining data from two outside sources, as matching the information can be done by matching the various IDs as needed. As well, as outlined in Chapter 4: Data Visualization with NFL Analytics, the URLs for player headshots are included in load_rosters() data.\nLet’s return to our participation_split data frame that we just built while working with the load_participation() function. While the data is formatted and prepared for analysis, there is no information that allows for easy identification of each player (aside from unique IDs, which is not helpful). To correct this, we can bring in the 2022 Pittsburgh Steelers roster information.\n\nrosters &lt;- nflreadr::load_rosters(2022)\n\nThe only argument you must provide the load_rosters() function is the years in which you want to collect roster information (in this specific case, 2022). While we know we only want the Steelers’ roster, it is not suggested to use filter() to gather any specific team at this point. Case in point: doing so with this example will result in the gsis_id 00-0036326 having an “NA” value for a name, despite having over 380 snaps in the 1 RB, 1TE, 3 WR offensive formation. The player associated with that gsis_id is Chase Claypool, who the Steelers traded to the Bears on November 1 of 2022. Because of this, no player name will be associated with that specific gsis_id on the Steelers’ roster (with the end result being a missing value).\n\n\n\n\n\n\nNote\n\n\n\nYou may notice that the resulting data frame, titled pit_roster, has 80 players on the roster despite NFL teams only being permitted to have 53 active players at a time.\nThis is because the information collected with load_rosters() includes not only active players, but those listed on the practice squad and injured reserve. Additionally, you may notice players listed as R/Retired (such as Stephon Tuitt in our data frame, who retired at the end of the 2022 season). In these situations, this status indicates that the player is listed as ‘reserved-retired’ and that the team continues to hold the player’s rights until the official expiration of their contract. In this case, Tuitt could not come out of retirement prior to his contract ending and play with another team unless first released or traded by Pittsburgh.\n\n\nDespite the wealth of information in the roster information, only the gsis_id and the full_name is required to complete our participation_split data frame. Rather than bringing unnecessary information over during the merge, we can select just the two columns we need and then use left_join() to merge each player’s name on the matching ID in the offense_players column and gsis_id from the roster information.\n\nrosters &lt;- nflreadr::load_rosters(2022)\n\nrosters &lt;- rosters %&gt;%\n  select(gsis_id, full_name)\n\nparticipation_split &lt;- participation_split %&gt;%\n  left_join(rosters, by = c(\"offense_players\" = \"gsis_id\"))\n\nparticipation_split\n\n# A tibble: 157 x 4\n   offense_personnel offense_players total full_name        \n   &lt;chr&gt;             &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;            \n 1 1 RB, 1 TE, 3 WR  00-0032897          3 Derek Watt       \n 2 1 RB, 1 TE, 3 WR  00-0033869        262 Mitchell Trubisky\n 3 1 RB, 1 TE, 3 WR  00-0034142         37 J.C. Hassenauer  \n 4 1 RB, 1 TE, 3 WR  00-0034347        781 James Daniels    \n 5 1 RB, 1 TE, 3 WR  00-0034768        781 Chukwuma Okorafor\n 6 1 RB, 1 TE, 3 WR  00-0034785        744 Mason Cole       \n 7 1 RB, 1 TE, 3 WR  00-0034928        235 Steven Sims      \n 8 1 RB, 1 TE, 3 WR  00-0035216        745 Diontae Johnson  \n 9 1 RB, 1 TE, 3 WR  00-0035217         25 Benny Snell      \n10 1 RB, 1 TE, 3 WR  00-0035222        258 Zach Gentry      \n# i 147 more rows\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe load_rosters() data is capable of providing data for multiple seasons at once. Through each season, a player’s gsis_id will remain static. Despite this, when merging multiple years of participation data with multiple years of roster information, the data must be matched on the season variable was well. This process involves including season in both the participation data group_by() and the roster information, as seen below.\n\nparticipation_multi_years &lt;-\n  load_participation(season = 2018:2022, include_pbp = TRUE)\n\nparticipation_2018_2022 &lt;- participation_multi_years %&gt;%\n  filter(!is.na(offense_formation)) %&gt;%\n  filter(posteam == \"PIT\") %&gt;%\n  tidyr::separate_rows(offense_players, sep = \";\") %&gt;%\n  group_by(season, offense_personnel, offense_players) %&gt;%\n  summarize(total = n())\n\nrosters_2018_2022 &lt;- nflreadr::load_rosters(2018:2022) %&gt;%\n  select(season, gsis_id, full_name)\n\nparticipation_2018_2022 &lt;- participation_2018_2022 %&gt;%\n  left_join(rosters_2018_2022,\n            by = c(\"season\", \"offense_players\" = \"gsis_id\"))\n\n\n\nIf your analysis requires providing great granularity, the load_rosters_weekly() function provides the same information as load_rosters() but structures it in weekly format by season (or over multiple seasons, if needed).\n\n3.5.3 The load_teams() Function\nA tool heavily used in the data visualization process, the load_teams() function provides information pertaining to each NFL team’s colors and logos, as well as providing a way to merge data frames that have differing values for teams.\nMore often than not, we will be using left_join to bring in team information (colors, logos, etc.) into a another data frame. We will utilize this function heavily in the example below.\nHowever, it is important to note the one argument that load_teams() accepts: current. Depending on the time span of your data, it is possible that you have teams included prior to relation (like the St. Louis Rams). If you are sure that you do not have such a scenario, you can use load_teams(current = TRUE) which will bring in just the current 32 NFL teams.\nHowever, if you need to include teams before expansion, you can use load_teams(current = FALSE) which will result in a data frame with 36 NFL teams.\n\n3.5.4 The load_officials() Function\nThe load_officials() data will return data, from 2015 to present, outlining which officials were assigned to which game. The data also includes information regarding each referee’s position, jersey number, and their official NFL ID number. Importantly, the data is structured to also include both a game_id and game_key that are sorted by season and week, allowing you to merge the information other data frames.\nWith the data, we can - for example - examine which NFL officiating crews called the most penalties during the 2022 NFL season. Doing so requires a bit of work in order to assign a unique crew_id to each stable of officials “working” under a lead referee.\nAs an example, we can create data frame called nfl_officials that contains the official and referee information for each game during the 2022 season. After, in order to get the referee for each crew, use filter() to select those crew members with the “Referee” position and then create a new column titled crew_id that takes the numeric official ID for each distinct Referee.\n\nnfl_officials &lt;- nflreadr::load_officials(seasons = 2022)\n\nreferees &lt;- nfl_officials %&gt;%\n  filter(position == \"Referee\") %&gt;%\n  mutate(crew_id = match(official_id, unique(official_id)))\n\nWith a unique crew_id associated with each Referee, we can now use left_join() to bring that information back into our nfl_officials data by merging the game_id and crew_id to the corresponding game_id.\n\nnfl_officials &lt;- nfl_officials %&gt;%\n  left_join(referees %&gt;% select(game_id, crew_id), by = \"game_id\")\n\nThe resulting nfl_officials data frame now includes all the original information and now a consistent crew_id for each team that the referee works with.\n\nnfl_officials\n\n# A tibble: 2,065 x 10\n   game_id   game_key official_name position jersey_number official_id\n   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;      \n 1 20220908~ 58838    Nathan Jones  Field J~            42 174        \n 2 20220908~ 58838    Matt Edwards  Back Ju~            96 178        \n 3 20220908~ 58838    Mike Carr     Down Ju~            63 168        \n 4 20220908~ 58838    Eugene Hall   Side Ju~           103 108        \n 5 20220908~ 58838    Jeff Seeman   Line Ju~            45 23         \n 6 20220908~ 58838    Carl Cheffers Referee             51 3          \n 7 20220908~ 58838    Brandon Cruse Umpire               0 201        \n 8 20220911~ 58839    Mike Morton   Umpire               0 206        \n 9 20220911~ 58839    John Jenkins  Field J~           117 86         \n10 20220911~ 58839    Danny Short   Down Ju~           113 172        \n# i 2,055 more rows\n# i 4 more variables: season &lt;int&gt;, season_type &lt;chr&gt;, ...\n\n\nTo calculate the total number of penalties flagged by each crew, we must first bring in the play-by-play from the 2022 season. After, we will use filter() to gather only those plays where a flag was thrown, and then select() the relevant columns, and finally summarize() by the old_game_id to get the total penalties called and the total penalty yards as a result of those penalties.\nAfter, we conduct another left_join() to bring in the specific penalty information into our nfl_officials data frame.\n\npenalty_pbp &lt;- nflreadr::load_pbp(seasons = 2022)\n\npenalties &lt;- penalty_pbp %&gt;%\n  filter(penalty == 1) %&gt;%\n  select(game_id, old_game_id, season, play_id,\n         desc, home_team, away_team,\n         posteam, defteam, week, penalty_team,\n         penalty_type, penalty, penalty_player_id,\n         penalty_player_name, penalty_yards) %&gt;%\n  group_by(old_game_id) %&gt;%\n  summarize(total_called = sum(penalty == 1, na.rm = TRUE),\n            total_yards = sum(penalty_yards, na.rm = TRUE))\n\nnfl_officials &lt;- nfl_officials %&gt;%\n  left_join(penalties, by = c(\"game_id\" = \"old_game_id\")) %&gt;%\n  select(game_id, official_name, position,\n         crew_id, total_called, total_yards)\n\nWith the data now combined, we can group_by() each game’s unique game_id and then use summarize() to sum the penalties called and the penalty yardage. After arranging the results in descending order by the total number of penalties, we can see that Referee Carl Cheffers, and his crew, called the most penalties during the 2022 NFL season with 223 and, unsurprisingly, also had the highest amount of penalty yards with 1,916.\n\nnfl_officials %&gt;%\n  group_by(game_id) %&gt;%\n  filter(position == \"Referee\") %&gt;%\n  ungroup() %&gt;%\n  group_by(crew_id) %&gt;%\n  summarize(referee = unique(official_name),\n            all_pen = sum(total_called, na.rm = TRUE),\n            all_yards = sum(total_yards, na.rm = TRUE)) %&gt;%\n  arrange(desc(all_pen)) %&gt;%\n  slice(1:10)\n\n# A tibble: 10 x 4\n   crew_id referee        all_pen all_yards\n     &lt;int&gt; &lt;chr&gt;            &lt;int&gt;     &lt;dbl&gt;\n 1       1 Carl Cheffers      223      1916\n 2      11 Scott Novak        204      1584\n 3       6 Brad Allen         203      1511\n 4      16 Clete Blakeman     203      1562\n 5       5 Shawn Hochuli      199      1780\n 6       4 Clay Martin        198      1684\n 7      17 Adrian Hill        193      1564\n 8       2 Alex Kemp          192      1602\n 9      10 Tra Blake          189      1545\n10      15 Ronald Torbert     185      1549\n\n\n\n3.5.5 The load_trades() Function\nThe load_trades() function returns a data frames that includes all trades in the NFL on a season-by-season basis with information pertaining to: trade_id, season, trade_date, gave, received, pick_season, pick_round, pick_number, conditional, pfr_id, pfr_name.\nFor example, we can gather every trade involving the New England Patriots with the following code:\n\nne_trades &lt;- nflreadr::load_trades(seasons = 2000:2022) %&gt;%\n  filter(gave == \"NE\" | received == \"NE\")\n\nne_trades\n\n# A tibble: 506 x 11\n   trade_id season trade_date gave  received pick_season pick_round\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1      704   2002 2002-03-11 GB    NE              2002          4\n 2      704   2002 2002-03-11 NE    GB                NA         NA\n 3      716   2002 2002-04-20 NE    WAS             2002          1\n 4      716   2002 2002-04-20 NE    WAS             2002          3\n 5      716   2002 2002-04-20 NE    WAS             2002          7\n 6      716   2002 2002-04-20 WAS   NE              2002          1\n 7      725   2002 2002-04-21 DEN   NE              2002          4\n 8      725   2002 2002-04-21 NE    DEN             2002          4\n 9      725   2002 2002-04-21 NE    DEN             2002          5\n10      727   2002 2002-04-21 DAL   NE              2002          7\n# i 496 more rows\n# i 4 more variables: pick_number &lt;dbl&gt;, conditional &lt;dbl&gt;, ...\n\n\nIf you want to view a trade that involves a specific player, you can do the same as above but filter() for a specific player. As an example, we do search for the trade that resulted in the New England Patriots sending Drew Bledsoe to the Buffalo Bills.\n\nbledsoe_trade &lt;- nflreadr::load_trades() %&gt;%\n  filter(trade_id == trade_id[pfr_name %in% c(\"Drew Bledsoe\")])\n\nbledsoe_trade\n\n# A tibble: 2 x 11\n  trade_id season trade_date gave  received pick_season pick_round\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1      728   2002 2002-04-22 BUF   NE              2003          1\n2      728   2002 2002-04-22 NE    BUF               NA         NA\n# i 4 more variables: pick_number &lt;dbl&gt;, conditional &lt;dbl&gt;,\n#   pfr_id &lt;chr&gt;, pfr_name &lt;chr&gt;\n\n\nSince the load_trades() function also includes NFL Draft round and pick numbers (if included in a trade), we can also - for example - determine all trades that involved a top ten pick switching hands.\n\ntop_ten_picks &lt;- nflreadr::load_trades() %&gt;%\n  filter(pick_round == 1 & pick_number &lt;= 10)\n\ntop_ten_picks\n\n# A tibble: 57 x 11\n   trade_id season trade_date gave  received pick_season pick_round\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n 1      711   2002 2002-04-20 DAL   KC              2002          1\n 2      711   2002 2002-04-20 KC    DAL             2002          1\n 3      661   2003 2003-04-26 CHI   NYJ             2003          1\n 4      662   2003 2003-04-26 ARI   NO              2003          1\n 5       16   2004 2004-04-24 CLE   DET             2004          1\n 6       16   2004 2004-04-24 DET   CLE             2004          1\n 7       64   2005 2005-03-03 OAK   MIN             2005          1\n 8      189   2007 2007-03-22 ATL   HOU             2007          1\n 9      189   2007 2007-03-22 HOU   ATL             2007          1\n10      197   2007 2007-04-28 SF    NE              2008          1\n# i 47 more rows\n# i 4 more variables: pick_number &lt;dbl&gt;, conditional &lt;dbl&gt;, ...\n\n\n\n3.5.6 The load_draft_picks() Function\nThe load_draft_picks() function will load information pertaining to every draft pick dating back to 1980. Aside from the information you would expect (the player’s name, the team that draft, round, pick number, position, etc.), the load_draft_picks() function also includes data regarding how many seasons the player played, the amount of times they were named to Pro Bowls, and top-level statistics regarding rushing, passing, receiving, and defensive metrics.\nThe load_draft_picks() function can be used to explore multiple different facets of the NFL Draft. For example, there is a belief in the analytics community to never draft a running back in the first round. Without getting into the reasoning behind that belief, we can quickly create a visualization of career rushing yards per running back compared to their draft position.\n\ndraft_picks &lt;- nflreadr::load_draft_picks()\nteams &lt;- nflreadr::load_teams()\n\nrb_picks &lt;- draft_picks %&gt;%\n  filter(position == \"RB\") %&gt;%\n  select(pick, team, rush_atts, rush_yards) %&gt;%\n  filter(pick &lt;= 100)\n\nrb_picks &lt;- rb_picks %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\n\nggplot(data = rb_picks, aes(x = rush_yards, y = pick)) +\n  geom_point(color = rb_picks$team_color, size = rb_picks$rush_atts / 500) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  geom_smooth(se = FALSE) +\n  xlab(\"Career Rushing Yards\") +\n  ylab(\"Pick Number\") +\n  nfl_analytics_theme() +\n  labs(title = \"**Career Rushing Yards vs. Pick Number**\",\n       subtitle = \"1980 to 2022\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nCareer Rushing Yards vs. Draft Pick\n\n\n\nAccording to the plot, there has been a limited number of running backs draft in the top 20 to go over 10,000 career yards. However, there are more running backs - some still active - drafted in the mid-range of the draft that are approaching, or have eclipsed, the 10,000 yard mark.\n\n3.5.7 The load_combine() Function\nThe load_combine() function provides NFL Combine data dating back to 2000. Aside from biographical information for each player (including eventual draft position), the data include the player’s scores in the 40-yard dash, the bench press, the vertical, the broad jump, the cone drill, and the shuttle drill.\nWe can join information from load_combine() with outside information to determine if there is any correlation between a running back’s 40-yard dash time in the combine and the total number of rushing yard accumulated during his career.\n\ncombine_data &lt;- nflreadr::load_combine() %&gt;%\n  select(pfr_id, forty) %&gt;%\n  filter(!is.na(pfr_id) & !is.na(forty))\n\nrosters &lt;- nflreadr::load_rosters(2000:2022) %&gt;%\n  select(gsis_id, pfr_id) %&gt;%\n  distinct(gsis_id, .keep_all = TRUE)\n\nplayer_stats &lt;- nflreadr::load_player_stats(seasons = TRUE,\n                                            stat_type = \"offense\") %&gt;%\n  filter(position == \"RB\" & !is.na(player_name) &\n           season_type == \"REG\") %&gt;%\n  group_by(player_name, player_id) %&gt;%\n  summarize(total_yards = sum(rushing_yards, na.rm = TRUE),\n            team = last(recent_team))\n\nplayer_stats &lt;- player_stats %&gt;%\n  left_join(rosters, by = c(\"player_id\" = \"gsis_id\"))\n\nplayer_stats &lt;- player_stats %&gt;%\n  filter(!is.na(pfr_id))\n\nplayer_stats &lt;- player_stats %&gt;%\n  left_join(combine_data, by = c(\"pfr_id\" = \"pfr_id\")) %&gt;%\n  filter(!is.na(forty))\n\nplayer_stats &lt;- player_stats %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\n\nggplot(data = player_stats, aes(x = forty, y = total_yards)) +\n  geom_point(color = player_stats$team_color, size = 3.5) +\n  geom_smooth(method = lm, se = FALSE,\n              color = \"black\",\n              linetype = \"dashed\",\n              size = 0.8) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  nfl_analytics_theme() +\n  xlab(\"Forty-Yard Dash Time\") +\n  ylab(\"Career Rushing Yards\") +\n  labs(title = \"**Forty-Yard Dash Time vs. Career Rushing Yards**\",\n       subtitle = \"2000 to 2022\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nComparing RBs Forty-Yard Dash Time vs. Career Total Yards\n\n\n\nThe visualization indicates that there is little, if any, direct correlation between a running back’s 40-yard dash time and his career total rushing yards. There are likely other factors that contribute to total rushing yards, such as a running back’s agility and vision and - perhaps most important - the quality of the offensive line over the duration of the running back’s career.\n\n3.5.8 The load_nextgen_stats() Function\nThe load_nextgen_stats() function retrieves player-level weekly statistics as provided by NFL Next Gen Stats dating back to the 2016 season. While three different stat types are provided (passing, receiving, and rushing), it is important to note that the data will only contain those players above a minimum number of attempts as determined by the NFL Next Gen Stats team.\nTo illustrate what can be done with the load_nextgen_stats() function, we will gather information to create two different plots. First, we can plot each quarterback’s average time to throw against their average completed air yards. Second, we will construct a graph to highlight which running backs had more actual rushing yards than the NGS “expected rushing yards” model.\n\nngs_data_passing &lt;- nflreadr::load_nextgen_stats(seasons = 2022,\n                                                 stat_type = \"passing\") %&gt;%\n  filter(week == 0) %&gt;%\n  select(player_display_name, team_abbr,\n         avg_time_to_throw, avg_completed_air_yards)\n\nngs_data_passing &lt;- ngs_data_passing %&gt;%\n  left_join(teams, b = c(\"team_abbr\" = \"team_abbr\"))\n\n\nggplot(data = ngs_data_passing, aes(x = avg_time_to_throw,\n                                    y = avg_completed_air_yards)) +\n  geom_hline(yintercept = mean(ngs_data_passing$avg_completed_air_yards),\n             color = \"black\", size = 0.8, linetype = \"dashed\") +\n  geom_vline(xintercept = mean(ngs_data_passing$avg_time_to_throw),\n             color = \"black\", size = 0.8, linetype = \"dashed\") +\n  geom_point(size = 3.5, color = ngs_data_passing$team_color) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  geom_text_repel(aes(label = player_display_name),\n                  family = \"Roboto\", fontface = \"bold\", size = 3.5) +\n  nfl_analytics_theme() +\n  xlab(\"Average Time to Throw\") +\n  ylab(\"Average Completed Air Yards\") +\n  labs(title = \"**Avgerage Time to Throw vs. Average Air Yards**\",\n       subtitle = \"2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nComparing a QB’s average time to throw to their average completed air yards\n\n\n\nThe resulting plot shows that Dak Prescott, Jalen Hurts, and Ryan Tannehill had an average completed air yard above the NFL average despite getting rid of the ball quicker than most other quarterbacks in the league. Conversely, Patrick Mahomes, Daniel Jones, and Justin Herbert has under average completed air yards despite holding on to the ball longer than the rest of the league (on average).\n\nngs_data_rushing &lt;- nflreadr::load_nextgen_stats(seasons = 2022,\n                                                 stat_type = \"rushing\") %&gt;%\n  filter(week == 0) %&gt;%\n  select(player_display_name, team_abbr, expected_rush_yards,\n         rush_yards_over_expected) %&gt;%\n  mutate(actual_rush_yards = expected_rush_yards + rush_yards_over_expected)\n\nngs_data_rushing &lt;- ngs_data_rushing %&gt;%\n  left_join(teams, by = c(\"team_abbr\" = \"team_abbr\"))\n\n\nggplot(data = ngs_data_rushing, aes(x = expected_rush_yards,\n                                    y = actual_rush_yards)) +\n  geom_smooth(method = lm, se = FALSE,\n              color = \"black\",\n              size = 0.8,\n              linetype = \"dashed\") +\n  geom_point(size = 3.5, color = ngs_data_rushing$team_color) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  geom_text_repel(aes(label = player_display_name),\n                  family = \"Roboto\",\n                  fontface = \"bold\", size = 3.5) +\n  nfl_analytics_theme() +\n  xlab(\"Expected Rush Yards\") +\n  ylab(\"Actual Rush Yards\") +\n  labs(title = \"Expected Rush Yards vs. Actual Rush Yards\",\n       subtitle = \"2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nExpected Rushing Yards vs. Actual Rushing Yards using Next Gen Stats\n\n\n\nAfter plotting the rushing data from Next Gen Stats, we can see that Nick Chubb was well above the expected yardage metric while Ezekiel Elliott, Alvin Kamara, Najee Harris, and Dalvin Cook all had actual rushing yards that fell below what was expected by the model.\n\n3.5.9 The load_espn_qbr() Function\nWith information dating back to the 2006 season, the load_espn_qbr() function provides a multitude of data points, including:\n\n\nqbr_total - the adjusted total QBR which is calculated on a scale of 0-100 based on the strength of the defenses played.\n\npts_added - the number of total points added by a quarterback adjusted above the average level of all other QBs.\n\nepa_total - the total number of expected points added for each quarterback as calculated by ESPN’s win probability model.\n\npass - the total of expected points added for just pass plays.\n\nrun - the total of expected points added for just run plays.\n\nqbr_raw - the QBR without adjustment for strength of defense.\n\nsack - the adjustment for expected points added for sacks.\n\nWhile the QBR metric has fallen out of grace among the analytics community in recent years, we can still compare the difference between the adjusted and unadjusted QBR scores to visualize the impact of defensive strength on the total.\n\nespn_qbr &lt;- nflreadr::load_espn_qbr(seasons = 2022) %&gt;%\n  select(name_short, team_abb, qbr_total, qbr_raw)\n\nespn_qbr &lt;- espn_qbr %&gt;%\n  left_join(teams, by = c(\"team_abb\" = \"team_abbr\"))\n\n\nggplot(data = espn_qbr, aes(x = qbr_total, y = qbr_raw)) +\n  geom_smooth(method = lm, se = FALSE,\n              color = \"black\",\n              linetype = \"dashed\",\n              size = 0.8) +\n  geom_point(color = espn_qbr$team_color, size = 3.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  xlab(\"QBR - Adjusted\") +\n  ylab(\"QBR - Unadjusted\") +\n  labs(title = \"QBR: Adjusted vs. Adjusted Scores\",\n       subtitle = \"Based on ESPN's Model: 2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nComparing Adjusted and Adjusted Quarterback Ratings\n\n\n\n\n3.5.10 The load_pfr_advstats() Function\nThe load_pfr_advstats() function provides statistics from Pro Football Reference starting with the 2018 season for passing, rushing, receiving, and defense. The function allows for collecting the data at either the weekly or season level with the summary_level argument.\nTo begin exploring the data, let’s examine the relationship between the number of dropped passes each quarterback endured during the 2022 regular season compared to the total number of bad_throws charted by Pro Football Reference. An argument can be made that a dropped pass without the pass being considered “bad” is the fault of the wide receiver, while a dropped pass that is charted as “bad” falls on the quarterback.\n\n\n\n\n\n\nImportant\n\n\n\nWhen working with data from load_pfr_advstats(), it is important to remember that you must use the clean_team_abbrs() function from nflreadr as there is a difference between the abbreviations used by PFR and the abbreviations used within the nflverse.\n\n\n\npfr_stats_pass &lt;- nflreadr::load_pfr_advstats(seasons = 2022,\n                                              stat_type = \"pass\",\n                                              summary_level = \"season\") %&gt;%\n  select(player, pass_attempts, team, drops, bad_throws) %&gt;%\n  filter(pass_attempts &gt;= 400)\n\npfr_stats_pass$team &lt;- clean_team_abbrs(pfr_stats_pass$team)\n\npfr_stats_pass &lt;- pfr_stats_pass %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\n\nggplot(data = pfr_stats_pass, aes(x = bad_throws, y = drops)) +\n  geom_hline(yintercept = mean(pfr_stats_pass$drops),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_vline(xintercept = mean(pfr_stats_pass$bad_throws),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_point(color = pfr_stats_pass$team_color, size = 3.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  geom_text_repel(aes(label = player),\n                  family = \"Roboto\", fontface = \"bold\", size = 3.5) +\n  xlab(\"Bad Throws\") +\n  ylab(\"Drops\") +\n  labs(title = \"**QB Bad Throws vs. WR Drops**\",\n       subtitle = \"2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nExample RStudio Console Output\n\n\n\nQuarterbacks such as Patrick Mahomes, Josh Allen, and Tom Brady had some of the highest drop numbers in the league but were also charted as having the highest number of bad throws, as well. This indicates that - without additional context - the wide receivers may not be as much at fault for the drops compared to the wide receivers that dropped passes from Daniel Jones and Russell Wilson, both of whom had a lower number of bad throws.\nBy using the rushing data provided by load_pfr_advstats(), we can examine how “hard” it is to tackle a running back by examining the relationship between their yards before contact and yards after contact.\n\n\n\n\n\n\nImportant\n\n\n\nBecause the list of running backs with more than 200 attempts includes Christian McCaffrey, it is necessary to use the case_when() function to switch his tm from 2TMS to the team that he finished the season with (SF). Otherwise, using left_join() for team colors would not work for McCafffrey since 2TMS is not a recognized team abbreviation.\n\n\n\npfr_stats_rush &lt;- nflreadr::load_pfr_advstats(seasons = 2022,\n                                              stat_type = \"rush\",\n                                              summary_level = \"season\") %&gt;%\n  select(player, tm, att, ybc, yac) %&gt;%\n  filter(att &gt;= 200) %&gt;%\n  mutate(tm = case_when(\n    player == \"Christian McCaffrey\" ~ \"SF\",\n    TRUE ~ tm))\n\npfr_stats_rush$tm &lt;- clean_team_abbrs(pfr_stats_rush$tm)\n\npfr_stats_rush &lt;- pfr_stats_rush %&gt;%\n  left_join(teams, by = c(\"tm\" = \"team_abbr\"))\n\n\nggplot(data = pfr_stats_rush, aes(x = ybc, y = yac)) +\n  geom_hline(yintercept = mean(pfr_stats_rush$yac),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_vline(xintercept = mean(pfr_stats_rush$ybc),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_point(color = pfr_stats_rush$team_color, size = 3.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  geom_text_repel(aes(label = player),\n                  family = \"Roboto\", fontface = \"bold\", size = 3.5) +\n  xlab(\"Yards Before Contact\") +\n  ylab(\"Yards After Contact\") +\n  labs(title = \"**Running Backs: Yards Before and After Contact**\",\n       subtitle = \"2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nExploring which running backs get more yards before and after contact\n\n\n\nIn the same vein as the running back statistics, we can use load_pfr_advstats() to explore which wide receivers gained the most yardage after catching the ball.\n\npfr_stats_rec &lt;- nflreadr::load_pfr_advstats(seasons = 2022, stat_type = \"rec\",\n                                             summary_level = \"season\") %&gt;%\n  filter(rec &gt;= 80 & pos == \"WR\") %&gt;%\n  select(player, tm, rec, ybc, yac)\n\npfr_stats_rec &lt;- pfr_stats_rec %&gt;%\n  left_join(teams, by = c(\"tm\" = \"team_abbr\"))\n\n\nggplot(data = pfr_stats_rec, aes(x = ybc, y = yac)) +\n  geom_hline(yintercept = mean(pfr_stats_rec$yac),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_vline(xintercept = mean(pfr_stats_rec$ybc),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_point(color = pfr_stats_rec$team_color, size = 3.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  geom_text_repel(aes(label = player),\n                  family = \"Roboto\", fontface = \"bold\", size = 3.5) +\n  xlab(\"Yards At Point of Catch\") +\n  ylab(\"Yards After Catch\") +\n  labs(title = \"**Yards at Point of Catch vs. Yards After Catch**\",\n       subtitle = \"2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nWhich wide receivers gain the most yardage after catching the ball?\n\n\n\nGiven the outstanding season that Justin Jefferson had in 2022, it is unsurprising to see him in the upper-right quadrant of the plot. Of his 1,809 receiving yards in the season, just under 1,200 of them came through air yards while he gained just over an additional 600 yards on the ground after the catch. On the other hand, D.K. Metcalf and Diontae Johnson were more often than not tackled almost immediately after catching the football.\nGiven the structure of the data collected from load_pfr_advstats(), the information can also be aggregated to define team total. For example, we can use the defensive statistics for blitzes and sacks from individual players to calculate the relationship between a team’s total number of blitzes against the number of sacks those blitzes produce.\n\npfr_stats_def &lt;- nflreadr::load_pfr_advstats(seasons = 2022,\n                                             stat_type = \"def\",\n                                             summary_level = \"season\") %&gt;%\n  filter(!tm %in% c(\"2TM\", \"3TM\")) %&gt;%\n  select(tm, bltz, sk) %&gt;%\n  group_by(tm) %&gt;%\n  summarize(total_blitz = sum(bltz, na.rm = TRUE),\n            total_sack = sum(sk, na.rm = TRUE))\n\npfr_stats_def &lt;- pfr_stats_def %&gt;%\n  left_join(teams, by = c(\"tm\" = \"team_abbr\"))\n\n\nggplot(data = pfr_stats_def, aes(x = total_blitz, y = total_sack)) +\n  geom_hline(yintercept = mean(pfr_stats_def$total_sack),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_vline(xintercept = mean(pfr_stats_def$total_blitz),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_image(aes(image = team_logo_wikipedia), asp = 16/9) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  xlab(\"Total Blitzes\") +\n  ylab(\"Total Sacks\") +\n  labs(title = \"**Total Blitzes vs. Total Sacks**\",\n       subtitle = \"2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nWhich defensive units are most efficient at creating sacks from blitzes?\n\n\n\nThe Philadelphia Eagles were absolutely dominate during the 2022 regular season when it came to turning blitzes into sacks. On the other hand, the Arizona Cardinals, New York Giants, and Pittsburgh Steelers produced high amounts of pressure with blitzes but were not able to convert them into sacks as a consistent basis.\n\n3.5.11 The load_snap_counts() Function\nThe load_snap_counts() function returns week-level snap counts for all players from Pro Football Reference dating back to the 2012 season.\n\n3.5.12 The load_contracts() Function\nThe load_contracts() function brings in data from OverTheCap.com. It is important to remember that much of the information is “nested” and, if you wish to see the yearly information, you must use the unnest() function from tidyr. To highlight this, we can run the following code to bring in the data as it is stored in the nflverse.\n\nnfl_contracts &lt;- nflreadr::load_contracts()\n\ncolnames(nfl_contracts)\n\n [1] \"player\"              \"position\"            \"team\"               \n [4] \"is_active\"           \"year_signed\"         \"years\"              \n [7] \"value\"               \"apy\"                 \"guaranteed\"         \n[10] \"apy_cap_pct\"         \"inflated_value\"      \"inflated_apy\"       \n[13] \"inflated_guaranteed\" \"player_page\"         \"otc_id\"             \n[16] \"date_of_birth\"       \"height\"              \"weight\"             \n[19] \"college\"             \"draft_year\"          \"draft_round\"        \n[22] \"draft_overall\"       \"draft_team\"          \"cols\"               \n\n\nThe returned data includes the contract information for the year in which it was signed, but does not include a year-by-year breakdown of money paid and other relevant information. This data is stored in the cols information and needs to be “opened” up in order to view, like below.\n\nnfl_contracts_unnested &lt;- nfl_contracts %&gt;%\n  tidyr::unnest(cols, name_sep = \"_\") %&gt;%\n  select(player, year, cash_paid) %&gt;%\n  filter(year != \"Total\") %&gt;%\n  mutate(cash_paid = as.numeric(as.character(cash_paid)))\n\nAfter using the unnest function, we use select() to gather just the player’s name, the year, and the cash paid for each year. After, we use filter() to remove the column that tallies the total of the player’s contact and then mutate() the cash_paid column in order to turn it into a number rather than a character.\nWe can now bring in other information to compare a player’s cash paid to their performance on the field. I am going to use an example from the final research project from one of my former Sport Analytics students, Matt Dougherty.1 We will compare a player’s yearly pay against their DAKOTA score (which is the adjusted EPA + CPOE composite based on the coefficients which best predict adjusted EPA/play). In order to do so, we must merge each quarterback’s DAKOTA composite based on the year.\n\nnfl_contracts_unnested &lt;- nfl_contracts %&gt;%\n  tidyr::unnest(cols, name_sep = \"_\") %&gt;%\n  select(player, year, cash_paid) %&gt;%\n  filter(year != \"Total\") %&gt;%\n  mutate(cash_paid = as.numeric(as.character(cash_paid)),\n         year = as.numeric(as.character(year))) %&gt;%\n  filter(year == 2022)\n\ndakota_composite &lt;- nflreadr::load_player_stats(2022) %&gt;%\n  filter(position == \"QB\") %&gt;%\n  group_by(player_display_name, season) %&gt;%\n  summarize(attempts = sum(attempts, na.rm = TRUE),\n            mean_dakota = mean(dakota, na.rm = TRUE)) %&gt;%\n  filter(attempts &gt;= 200)\n\nteams &lt;- nflreadr::load_teams(current = TRUE)\n\nnfl_contracts_unnested &lt;- nfl_contracts_unnested %&gt;%\n  left_join(dakota_composite, by = c(\"player\" = \"player_display_name\"))\n\nnfl_contracts_unnested &lt;- na.omit(nfl_contracts_unnested)\n\nnfl_contracts_unnested &lt;- nfl_contracts_unnested %&gt;%\n  distinct(player, year, .keep_all = TRUE)\n\n\nggplot(data = nfl_contracts_unnested,\n       aes(x = cash_paid, y = mean_dakota)) +\n  geom_hline(yintercept = mean(nfl_contracts_unnested$mean_dakota),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_vline(xintercept = mean(nfl_contracts_unnested$cash_paid),\n             color = \"black\", linetype = \"dashed\", size = 0.8) +\n  geom_point(size = 3.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::dollar_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  xlab(\"Cash Paid (in millions)\") +\n  ylab(\"Mean DAKOTA Composite\") +\n  geom_text_repel(aes(label = player),\n                  family = \"Roboto\", fontface = \"bold\", size = 3.5) +\n  nfl_analytics_theme() +\n  labs(title = \"Cash Paid vs. Mean DAKOTA Composite\",\n       subtitle = \"2022 Regular Season\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nThe relationship between a QB’s average DAKOTA and annual cash paid\n\n\n\nThose players in the upper-right quadrant (Mahomes, Allen, Cousins, etc.) are among the highest paid quarterbacks in the league, but also are the highest performing players based on the DAKOTA composite. On the other hand, those QBs in the lower-right quadrant are - based on the DAKOTA composite - overpaid relevant to their performance on the field. The lower-left QBs are not highly paid, but also do not perform well. Those players in the upper-left performed extremely well (some better than those in the upper-right) but come with a team-friendly contract (in terms of pure cash paid).\n\n\n\n\n\n\nTip\n\n\n\nAs mentioned in the book’s Preface, you can feel free to reach out to me with questions regarding the R programming language and NFL analytics.\nThat said, there is a fantastic nflverse Discord channel that was created and currently maintained by the creators of the packages (Ben Baldwin, Sebastian Carl, Tan Ho, Thomas Mock, etc.).\nYou can become a member here: http://nfl-book.bradcongelio.com/discord-invite\nHowever, before creating a thread and seeking assistance, be sure to visit the “#How To” section in the General Help area to learn to create a reprex (reproducible example). Providing a reprex allows other users to copy and paste your code in order to recreate the issue you are experiencing.\n\n\n\n3.5.13 The load_ftn_charting() Function\nAs one of the newest additions to the nflverse, users can use the load_ftn_charting() function to pull data compiled by the FTN Network.\n\n\n\n\n\n\nNote\n\n\n\nFTN charting data is only available in the nflverse starting with the 2023 season.\n\n\nThe data is manually charted and includes the following variables:\n\nftn_game_id\nnflverse_game_id\nseason\nweek\nftn_play_id\nnflverse_play_id\nstarting_hash\nqb_location\nn_offense_backfield\nis_no_huddle\nis_motion\nis_play_action\nis_screen_pass\nis_rpo\nis_trick_play\nis_qb_out_of_pocket\nis_interception_worthy\nis_throw_away\nread_thrown\nis_catchable_ball\nis_contested_ball\nis_created_reception\nis_drop\nis_qb_sneak\nn_blitzers\nn_pass_rushers\nis_qb_fault_sack\n\nYou may notice that the charting data provided by FTN does not include player information. Because of this, it is important to take the data as provided from FTN and then merge it into the regular nflverse play-by-play data. Fortunately, because the FTN data includes both the nflverse_game_id and nflverse_play_id variables, the merging process is simple.\nTo begin, we can collect the FTN charting data from the 2023 season. Since the data includes only the 2023 season, we can go ahead and drop both the week and season variables so that we do not created duplicate versions during the merge.\n\nftn_data &lt;- nflreadr::load_ftn_charting(2023) %&gt;%\n  select(-week, -season)\n\nIn order to merge with the play-by-play data from nflverse, we must bring in the equivalent season of data.\n\npbp &lt;- nflreadr::load_pbp(2023)\n\nNow we can merge the two sets of data using the unique game and play identifiers provided in each wherein the game_id in pbp matches with nflverse_game_id from ftn_data and play_id from pbp matches with nflverse_play_id from ftn_data.\n\npbp &lt;- pbp %&gt;%\n  left_join(ftn_data, by = c(\"game_id\" = \"nflverse_game_id\",\n                             \"play_id\" = \"nflverse_play_id\"))\n\nWe have now successfully merged the FTN charting data into the nflverse play-by-play data. In order to conduct an exploratory analysis, let’s examine the is_catchable_ball variable from FTN to determine a QB’s accuracy based on air yards (specifically looking at Brock Purdy, Josh Allen, Patrick Mahomes, and Tua Tagovailoa).\n\nairyards_accu &lt;- pbp %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  filter(complete_pass == 1 | incomplete_pass == 1, !is.na(down)) %&gt;%\n  group_by(passer) %&gt;%\n  mutate(total_passes = n()) %&gt;%\n  filter(total_passes &gt;= 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(passer, posteam, air_yards) %&gt;%\n  summarize(\n    total_att = n(),\n    total_catchable = sum(is_catchable_ball == \"TRUE\", na.rm = TRUE),\n    total_pct = total_catchable / total_att) %&gt;%\n  filter(air_yards &gt;= 0 & air_yards &lt;= 25)\n\nWe first collect the data for the entire NFL using complete_pass, incomplete_pass, and !is.na(down) to filter down to just pure passing attempts. After, we use group_by() to gather each QB with both their respective teams and then each individual air yard distance (before using a filter() at the very end to limit those attempts that had between 0 and 25 air yards).\nNext, we use the load_teams function to gather the team colors for each player, select just the four quarterbacks we want to visualize, and then use ggplot to create the plot.\n\nteams &lt;- nflreadr::load_teams(current = TRUE)\n\nairyards_accu &lt;- airyards_accu %&gt;%\n  left_join(teams, by = c(\"posteam\" = \"team_abbr\"))\n\nairyards_accu &lt;- airyards_accu %&gt;%\n  filter(passer %in% c(\"B.Purdy\", \"J.Allen\", \"T.Tagovailoa\", \"P.Mahomes\"))\n\n\nggplot(data = airyards_accu, aes(x = air_yards, y = total_pct)) +\n  geom_smooth(se = FALSE, aes(color = posteam)) +\n  nflplotR::scale_color_nfl(type = \"primary\") +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::label_percent()) +\n  facet_wrap(vars(passer), scales = \"free_y\") +\n  xlab(\"Air Yards\") +\n  ylab(\"Total % of Catchable Attempts\") +\n  labs(title = \"**Total % of Catchable Attempts Based on Air Yards**\",\n       subtitle = \"*Minimum 100 Attempts  |  Through Week 5 of 2023 Season*\",\n       caption = \"*Introduction to NFL Analytics With R*&lt;br&gt;**Bradley J. Congelio**\") +\n  nfl_analytics_theme()\n\n\n\nUsing FTN Data To Examine QB Accuracy Based on Air Yards Distance"
  },
  {
    "objectID": "03-nfl-analytics-functions.html#exercises",
    "href": "03-nfl-analytics-functions.html#exercises",
    "title": "\n3  NFL Analytics with the nflverse Family of Packages\n",
    "section": "\n3.6 Exercises",
    "text": "3.6 Exercises\nThe answers for the following answers can be found here: http://nfl-book.bradcongelio.com/ch3-answers.\n\n3.6.1 Exercise 1\n\n\nLoad data from the 2010 to 2022 regular seasons into a data frame titled pbp.\nIn a data frame titled rushing_success, determine how many rushing attempts each offensive team had on 1st down.\nDetermine how many of those attempts resulted in a positive EPA (success).\nCalculate the percentage of the results into a column titled success_pct.\nArrange the results in descending order by success_pct.\n\n3.6.2 Exercise 2\nLoad data from the 2022 regular season into a data frame titled pbp_2022.\n\nIn a data frame titled qb_short_third, determine how many 3rd down passing attempts each QB had.\nDetermine the number of times the QB’s air yards was less than the required yards to go.\nIn a column titled ay_percent, calculate the percentage of these results.\nFilter the results to those QBs with 100 or more attempts.\nArrange the results in descending order by ay_percent.\n\n3.6.3 Exercise 3\nTom Brady had a long and storied career, serving as a full-time started in the NFL from 2001 to 2022. The qb_epa metric gives a quarterback credit for EPA up to the point where a receivers lost a fumble after a completed catch. For this question, create data frame titled tom_brady and find Brady’s average qb_epa per season, from 2001 to 2022. After, arrange in descending order by his average QB EPA.\n\n3.6.4 Exercise 4\nCreate a data frame titled made_field_goals and find, between the 2000 and 2022 season, the number of field goal attempts and percentage made on all kicks greater than 40-yards in distance.\n\n3.6.5 Exercise 5\nOn December 11 of the 2005 season, Pittsburgh’s Jerome Bettis trucked Bears linebacker Brian Urlacher. You can view the play here: Bettis Trucks Urlacher. Using the load_pbp function, find this specific play (using the video for contextual clues). After, determine how much win probability this individual play added (that is: the differnece between home_wp and home_wp_post."
  },
  {
    "objectID": "03-nfl-analytics-functions.html#footnotes",
    "href": "03-nfl-analytics-functions.html#footnotes",
    "title": "\n3  NFL Analytics with the nflverse Family of Packages\n",
    "section": "",
    "text": "Matt was in my SPT 313 class during the Spring 2023 semester and asked more questions and showed more willingness to learn coding and analytics than all of my prior classes combined. If he is not yet hired by an NFL team, it is a complete injustice.↩︎"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#data-viz-must-understand-the-audience",
    "href": "04-nfl-analytics-visualization.html#data-viz-must-understand-the-audience",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.1 Data Viz Must Understand the Audience",
    "text": "4.1 Data Viz Must Understand the Audience\nAs explained by Stikeleather, the core purpose of a data visualization is to take “great quantities of information” and then convey that information in such a way that it is “easily assimilated by the consumers of the information.” In other words, the process of data visualization should allow for a great quantity of data to be distilled into an easily consumable (and understandable!) format.\nSpeaking specifically to NFL analytics, when doing visualizations we must be conscious about whether or not the intended audience will understand the terminology and concepts we use in the plot. For example, most all NFL fans understand the “non-advanced” statistics in the sport. But, when plots start using metrics such as EPA or completion percentage over expected, for example, the audience looking at the plot may very well have little understanding of what is being conveyed.\nBecause of this, most of my data visualizations include “directables” within the plot. These “directables” may be arrows that indicate which trends on the plot are “good” or they may be text within a scatterplot that explains what each quadrant means. Or, for example, I sometimes include a textual explanation of the “equation” used to develop a metric as seen below:\n\n\n\n\nJoe Burrow was the most aggressive QB in the league when throwing on 3rd down\n\n\n\nThe above plot explores which QBs, from the 2020 season, were most aggressive on 3rd down with between 5 to 10 yards to go. Since “aggressiveness” is not a typical, day-to-day metric discussed by NFL fans, I included a “directable” within the subtitle of the plot that explained that the plot, first, was examining just 3rd down pass attempts within a specific yard range. And, second, I made the decision to include how “aggressiveness” was calculated by including the simple equation within the subtitle as well. Doing so allows even the most casual of NFL fans to easily understand what the plot is showing - in this case, that Joe Burrow’s 3rd down pass attempts with between 5 to 10 yards to go made it to the line of gain, or more, on 68% of his attempts. On the other hand, Drew Lock and Drew Brees were the least aggressive QBs in the line based on the same metric."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#setting-up-for-data-viz",
    "href": "04-nfl-analytics-visualization.html#setting-up-for-data-viz",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.2 Setting Up for Data Viz",
    "text": "4.2 Setting Up for Data Viz\nWhile most of your journey through NFL analytics in this book required you to use the tidyverse and a handful of other packages, the process of creating compelling and meaningful data visualizations will require you to utilize multitudes of other packages. Of course, the most important is ggplot2 which is already installed via the tidyverse. However, in order to recreate the visualizations included in this chapter, it is required that you install other R packages. To install the necessary packages, you can run the following code in RStudio:\n\ninstall.packages(c(\"extrafont\",\n                   \"ggrepel\",\n                   \"ggimage\",\n                   \"ggridges\",\n                   \"ggtext\",\n                   \"ggfx\",\n                   \"geomtextpath\",\n                   \"cropcircles\",\n                   \"magick\",\n                   \"glue\",\n                   \"gt\",\n                   \"gtExtras\"))"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#the-basics-of-using-ggplot2",
    "href": "04-nfl-analytics-visualization.html#the-basics-of-using-ggplot2",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.3 The Basics of Using ggplot2\n",
    "text": "4.3 The Basics of Using ggplot2\n\nThe basics of any ggplot visualization involves three basic calls to information in a data set as well as stipulation which type of geom you would like to use:\n\nthe data set to be used in the visualization\nan aesthetic call for the x-axis\nan aesthetic call for the y-axis\nyour desired geom type\n\n\nggplot(data = 'dataset_name', aes(x = 'x_axsis', y = 'y_axis')) +\n  geom_type()\n\nTo showcase this, let’s use data from Sports Info Solutions regarding quarterback statistics when using play action versus when not using play action. To start, collect the data using the vroom function.\n\nplay_action_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/pa-data\")\n\nTo provide an easy-to-understand example of building a visualization with ggplot, let’s use each QB’s total yardage when using play action and when not. In this case, our two variable names are yds and pa_yds with the yds variable being placed on the x-axis and the pa_yds variable being placed on the y-axis.\n\n\n\n\n\n\nTip\n\n\n\nIt is important to remember which axis is which as you begin to learn using ggplot:\nThe x-axis is the horizontal axis that runs left-to-right.\nThe y-axis is the vertical axis that runs top-to-bottom.\n\n\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point()\n\n\n\nA simplistic plot using geom_point()"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#building-a-data-viz-a-step-by-step-process",
    "href": "04-nfl-analytics-visualization.html#building-a-data-viz-a-step-by-step-process",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.4 Building A Data Viz: A Step-by-Step Process",
    "text": "4.4 Building A Data Viz: A Step-by-Step Process\nOur newly created scatter plot is an excellent starting point for a more finely detailed visualization. While we are able to see the relationship between non-play action passing yards and those attempts that included play action, we are unable to discern which specific point is which quarterback - among other issues. To provide more detail and to “prettify” the plot, let’s discuss doing the following:\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both the x-axis and y-axis\nChange theme elements to make data viz more appealing.\n\n\n\n\n4.4.1 Adding Team Colors to Each Point\n\n\n\n\n\n\nNote\n\n\n\nMuch like anything in the R language, there are multiple ways to go about adding team colors (and logos, player headshots, etc.) to visualizations.\nFirst, we can merge team color information into our play_action_data and then manually set the colors in our geom_point call.\nSecond, we can use the nflplotR package (which is part of the nflverse) to bring the colors in.\nBoth examples will be included in the below example.\n\n\nTo start, we will load team information using the load_teams function within nflreadr. In this case, we are requesting that the package provide only the 32 current NFL teams by including the current = TRUE argument. Conversely, setting the argument to current = FALSE will result in historical NFL teams being included in the data (the Oakland Raiders and St. Louis Rams, for example). We will also use the select() verb from dplyr to gather just the variables we know we will need (team_abbr, team_nick, team_color, and team_color2.\n\n\n\n\n\n\nImportant\n\n\n\nWe are only including the team_abbr variable in this example because we are going to create the plot both with and without the use of nflplotR. As of the writing of this book, the newest development version of the package is 1.1.0.9004 and does not yet (if ever) provide support to use team nicknames. Because of this, we must include team_abbr in our merge since it is the team name version that is standardized for use in nflplotR.\n\n\nAfter collecting the team information needed, we can conduct a left_join() to match the information by team in play_action_data and team_nick in the team information from load_teams() and then confirm the merge was successful by viewing the columns names in play_action_data with colnames().\n\nteams &lt;- nflreadr::load_teams(current = TRUE) %&gt;%\n  select(team_abbr, team_nick, team_color, team_color2)\n\nplay_action_data &lt;- play_action_data %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_nick\"))\n\ncolnames(play_action_data)\n\nWith the team color information now built into our play_action_data, we can include the correct team color for each point by including the color argument within our geom_point.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(color = play_action_data$team_color)\n\n\n\nAdding team colors to the plot\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou may notice that we used the the $ special operator to extract the team_color information within our play_action_data data frame. This is an extremely important distinction, as using the aes() argument from ggplot and not using the $ operator will result in a custom scale color being applied to each team, without the colors being correctly associated to a team.\nTo see this for yourself, you can run the example following code. Remember, this is an incorrect approach and serves to only highlight why the $ special operator was used in the above code.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(aes(color = team_color))\n\n\n\nIn order to bring team colors into the plot, we must use the $ special operator and not the aes() argument\n\n\n\n\n\nAs mentioned, the same result can be achieved using the nflplotR package. The following code will do so:\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(aes(color = team_abbr)) +\n  nflplotR::scale_color_nfl(type = \"primary\")\n\n\n\nAdding team colors to the plot with the nflplotR package\n\n\n\nIn the above example, you will notice that we are including the team color information in an aes() call within the geom_point() function. This is because we ultimately control the specifics of the custom scale through the use of scale_color_nfl in the nflplotR package, which also allows us to select whether we want to display the primary or secondary team color.\nGiven the two examples above, a couple items regarding the use of nflplotR should become apparent.\n\nIf your data already includes team names in team_abbr format (that is: BAL, CIN, DET, DAL, etc.), then using nflplotR is likely a more efficient option as you do not need to merge in team color information. In other words, our play_action_data information could contain just the variables for player, team_abbr, yds, and pa_yds and nflplotR would still work as the package, “behind the scenes”, automatically correlates the team_abbr with the correct color for each team.\nHowever, if your data does not include teams in team_abbr format and you must merge in information manually, it is likely more efficient to use the $ special operator to bring the team colors in without using the aes() call within geom_point().\n\nFinally, because we have both team_color and team_color2 - the primary and secondary colors for each team - in the data, we can get fancy and create points that are filled with the primary team color and outlined by the secondary team color. Doing so simply requires changing the type of our geom_point. In the below example, we are specifying that we want a specific type of geom_point by using shape = 21 and then providing the fill color and the outline color with color. In each case, we are again using the $ special operator to select the primary and secondary color associated with each team.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2)\n\n\n\nUsing a different geom_poitn() shape in order to utilize both team colors\n\n\n\nWith team colors correctly associated with each point, we can turn back to our “to do list” to see what part of the job is next.\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both the x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n4.4.2 Increasing The Size of Each Point\nDetermining when and how to resize the individual points in a scatterplot is a multifaceted decision. To that end, there is no hard and fast rule for doing so as it depends on both the specific goals and context of the visualization. There are, however, some general guidelines to keep in mind:\n\n\nData density: if you have a lot of data points within your plot, making the points smaller may help to reduce issues of overlapping/overplotting. Not only is this more aesthetically pleasing, but it can also help in making it easier to see patterns.\n\nImportance of individual points: if certain points within the scatterplot are important, we may want to increase the size of those specific points to make them standout.\n\nVisual aesthetics: the size of the points can be adjusted simply for visual appeal.\n\nContextual factors: can the size of the points be used to highlight even more uniqueness in the data? For example, given the right data structure, we can size individual dots to show the spread in total attempts across the quarterbacks.\n\nGiven the above guidelines, the resizing of the points in our play_action_data data frame is going to be a strictly aesthetic decision. We cannot, as mentioned above, alter the size of each specific points based on each quarterback’s number of attempts as the data provides attempts for both play action and non-play action passes. Moreover, we could create a new column that add both attempt numbers to get a QB’s cumulative total but that does not have a distinct correlation to the data on either axis.\n\n\n\n\n\n\nCaution\n\n\n\nFor the sake of educational purposes, we can alter the size of each specific point to correlate to the total number of play action attempts for each quarterback (and then divide this by 25 in order to decrease the size of the points to fit them all onto the plot).\nAgain: it is important to point out that this not a good approach to data visualization, as the size of the points correlate to just one of the variables being explored in the plot.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = play_action_data$pa_att / 25)\n\n\n\nAn incorrect approach to sizing the points, as the size correltes to just one of the variables\n\n\n\n\n\nIn order to maintain correct visualization standards, we can resize the points for nothing more than aesthetic purposes (that is: making them bigger so they are easier to see). To do so, we still add the size argument to our geom_point but providing a numeric value to apply uniformly across all the points. To process of selecting the numeric value is a case of trial and error - inputting and running, changing and running, and changing and running again until you find the size that provides easier to see points without adding overlap into the visualization.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5)\n\n\n\nResizing the points for aesthetic purposes\n\n\n\nWith the size of each point adequately adjusted, we can move on to the next part of our data visualization “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both the x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n4.4.3 Adding Player Names to Each Point\nWhile the our current plot includes team-specific colors for the points, we are still not able to discern - for the most part - which player belongs to which point. To rectify this, we will turn to using the ggrepel package, which is designed to improve the readability of text labels on plots by automatically repelling overlapping labels, if any. ggrepel operates with the use of two main functions: geom_text_repel and geom_label_repel. Both provide the same end result, with the core difference being geom_label_repel adding a customized label under each player’s name.\nWe can do a bare minimum addition of the player names by adding one additional line of code using geom_text_repel, wrapping it in an aes() call, and specifying which variable in the play_action_data is the label we would like to display.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player))\n\n\n\nAdding player names to the plot\n\n\n\nWhile it is a good first attempt at adding the names, many of them are awkwardly close to the respective point. Luckily, the ggrepel package provides plenty of built-in customization options:\n\n\n\nggrepel Option1\n\nDescription of Option\n\n\n\nseed\na random numeric seed number for the purposes of recreating the same layout\n\n\nforce\nforce of repulsion between overlapping text labels\n\n\nforce_pull\nforce of attraction between each text label and its data point\n\n\ndirection\nmove the text labels in either “x” or “y” directions\n\n\nnudge_x\nadjust the starting x-axis starting position of the label\n\n\nnudge_y\nadjust the starting y-axis starting position of the label\n\n\nbox.padding\npadding around the text label\n\n\npoint.padding\npadding around the labeled data point\n\n\narrow\nrenders the line segment as an arrow\n\n\n\nOf the above options, the our current issue with name and point spacing can be resolved by including a numeric value to the box.padding. Moreover, we can control the look and style of the text (such as size, font family, font face, etc.) in much the same way. To make these changes, we can set the box.padding to 0.45, set the size of the text to three using size as well as switch the font to ‘Roboto’ using family, and - finally - make it bold using fontface.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\")\n\n\n\nAdding options from ggrepel to the player names\n\n\n\nThe plot, as is, is understandable in that we are able to associate each point with a specific quarterback to examine how a quarterback’s total passing yardage is split between play action and non-play action passes. While the graph could hypothetically “standalone” as is - minus a need for a title - we can still do work on it to make it more presentable. Let’s return to our data visualization “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n4.4.4 Editing Axis Ticks and Values\nBecause steps four and five from the above “to do list” can be accomplished with the same package, we will lump both together and complete them at once.\nLet’s first examine the idea of increasing the number of ticks on each axis. The “axis ticks” refer to the specific spots on each axis wherein a numeric data point resides. With our current visualization, we currently have “1000, 2000, 3000” on the x-axis and “400, 800, 1200, 1600” on the y-axis.\nWe may want to increase the number of axis ticks in this visualization as it can provide a more detailed view of the data being presented. Typically, increasing the number of tickets will show more granularity in the data and make it easier to interpret the values represented by each point. In this specific case, we can look at the cluster of points represented by Andy Dalton, Mac Jones, and Matt Ryan. Given the current structure of the axis ticks, we can guess that Matt Ryan has roughly 2,500 yards on non-play action passing attempts. Given we know Matt Ryan’s amount, we can make guesses that Andy Dalton may be around 2,300 and Mac Jones somewhere between the two.\nBy increasing the number of values on each axis, we have the ability to see more specific results. Conversely, we must be careful to not add too many so that the data becomes overwhelming to interpret. Much like the size of geom_point was a case of trial and error, so is selecting an appropriate amount of ticks.\nHowever, before implementing these changes, we need to segue into a discussion on continuous and discrete data.\n\n\n\n\n\n\nImportant\n\n\n\nWhen implementing changes to either the x- or y-axis in ggplot, you will be working with either continuous or discrete data, and using the scale_x_continuous or scale_x_discrete functions (replacing x with y when working with the opposite axis). In either case, both functions allow you to customize the axis of a plot but are used for different types of data.\nscale_x_continuous is used for continuous (or numeric) data, where the axis is represented by a continuous range of numeric values. The values within a continuous axis can take on any number within the given range.\nscale_x_discrete is used for discrete data (or often character-based data). You will see this function used when working with variables such as player names, teams, college names, etc. In any case, discrete data is limited to a specific set of categories.\nPlease know that ggplot will throw an error if you try to apply a continuous scale to discrete data, or the opposite, that reads: Error: Discrete value supplied to continuous scale.\n\n\nIn the case of our current plot, we now know we will be using the scales_x_continuous and scale_y_continuous functions as both contain continuous (numeric) data. To make the changes, we can turn to the scales package and its pretty_breaks function to change the number of “breaks” (or ticks) on each axis. By placing n = 6 within the pretty_breaks argument, we are requesting a total of six axis ticks on both the x- and y-axis.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6))\n\n\n\nChanging numbers of breaks on each axis with pretty_breaks()\n\n\n\nDespite our request to build the plot with six ticks on each axis, you will see the generated visualization includes seven on the x-axis and eight on the y-axis. This does not mean your code with pretty_breaks did not work. Instead, the pretty_breaks function is designed to internally determine the best axis tick optimization based on your requested number. To that end, the function determined that seven and eight ticks, respectively, was the most optimized way to display the data given our desire to have at least six on each.\nWith the number of axis ticks corrected, we can turn our attention to getting the labels of the axis ticks into correct numeric format. Within the same scale_x_continuous or scale_y_continuous arguments, we will use the labels function, combined with another tool from the scales package to make the adjustments.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma())\n\n\n\nAdding commas to the axis values using label_comma()\n\n\n\nBy adding a labels option to both scale_x_continuous and scale_y_continuous, we can use the label_comma() option from within the scales package to easily add a comma into numbers that are in the thousands.\nWith much of the heavy lifting for our visualization now complete, we can move on to the final steps in our “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n4.4.5 Changing Axis Titles and Adding Title, Subtitle, and Caption\nMuch like our last section, we can work on changing the title of each axis and adding a title, subtitle, and caption for the plot within one section, as all this is added and/or changed by using labs().\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\",\n       caption = \"*Introduction to NFL Analytics with R*&lt;br&gt;**Bradley J. Congelio**\")\n\n\n\nAdding the plot’s title and subtitle\n\n\n\nWithin the new labs(), we are placing a total of five items: x (allowing us to name the x-axis outside the confines of what it is called in the beginning aes() call, y (allowing us to name the y-axis), title (allowing us to add a title to the top of the plot), subtitle (allowing us to add a subtitle below the title and provide more contextual information), and caption (allowing us to provide information about where the graph come from and who designed it). We will explore ways to change the font, size, color, and more of these items when we move on to the last item of our “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n4.4.6 Adding Mean Lines to Both x-axis and y-axis\nAdding mean (or average) lines to both the x-axis and y-axis allows us to visualize where each quarterback falls within one of four sections (according to the amount of passing yards in both situations). Adding the lines is done with the inclusion of two additional geoms to the existing plot (in this case geom_hline and geom_vline).\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n   geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\") +\n  geom_hline(yintercept = mean(play_action_data$pa_yds),\n             linewidth = .8, color = \"black\",\n             linetype = \"dashed\") +\n  geom_vline(xintercept = mean(play_action_data$yds),\n             linewidth = .8, color = \"black\",\n             linetype = \"dashed\")\n\n\n\nAdding mean lines incorrectly, as they are top of every layer\n\n\n\nIn the above output, we used geom_hline and geom_vline to draw a dashed line at the average for both pa_yds at the yintercept and yds at the xintercept. Because of this, we can see that - for example - Marcus Mariota is above average for play action yards, but below average for non-play action yards. Additionally, Matthew Stafford, Baker Mayfield, Kyler Murray, and many others are all below average in both metrics while Jared Goff, Justin Herbert, Patrick Mahomes, and others are well above average in both.\nHowever, adding the geom_hline and geom_vline at the very end of the ggplot code creates an issues (and one I’ve intentionally created for the purposes of education). As you look at the plot, you will see that the dashed line runs on top of the names and dots in the plot. This is because ggplot follows a very specific ordering of layering.\n\n\n\n\n\n\nImportant\n\n\n\nIn ggplot, it is important to remember that items in a plot are layered in the order in which they are added to the plot. This process of layering is important because it ultimately determines which items end up on top of others, which can have significant implications on the visual appearance of the plot.\nAs we’ve seen so far in the process, each layer of a plot is added by including a geom_. The first layer added will always be at the very bottom of the plot, with each additional layer building on top of the previous layers.\n\n\nBecause of the important layering issue highlighted above, it is visually necessary for us to move the geom_hline and geom_vline to the beginning of the ggplot code so both are layered underneath everything else in the plot (geom_point and geom_text_repel in this case). As well, we can apply the alpha option to each to slightly decrease each line’s transparency.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_hline(yintercept = mean(play_action_data$pa_yds), \n             linewidth = .8, \n             color = \"black\", \n             linetype = \"dashed\",\n             alpha = 0.5) +\n  geom_vline(xintercept = mean(play_action_data$yds), \n             linewidth = .8, \n             color = \"black\", \n             linetype = \"dashed\",\n             alpha = 0.5) +\n   geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\")\n\n\n\nAdding mean lines at beginning of code so they are a bottom layer of the plot\n\n\n\nBy moving the average lines to the top of our ggplot code, both are now layered under the other two geom_ and are not visually impacting the final plot.\n\n4.4.6.1 Adding Mean Lines with nflplotR\n\n\n\n\n\n\n\nTip\n\n\n\nEven though we were not able to use nflplotR to handle the colors in this plot because the data lacked a corresponding team_abbr variable, we can still use nflplotR to add our mean lines - and I actually recommend doing so, as it requires less lines of code (thus less typing). See below for an example.\n\n\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(x0 = yds, y0 = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\")\n\n\n\nAdding mean lines with nflplotR\n\n\n\nBy using the geom_mean_lines() function within nflplotR, we can construct both of the lines together rather than needing to provide a geom_hline() and a geom_vline() argument. Because of this, we can also provide the size, width, and type of our line just once (rather than repeating it again like we had to in the former method).\nWe can now move onto the final item on our data visualization “to do list.”\n\n\n\n\n\n\nNote\n\n\n\nOur data visualization “to do list”:\n\nAdding team colors to each point.\nIncreasing the size of each point.\nAdding player names to each point.\nIncreasing the number of ticks on each axis.\nProvide the numeric values in correct format (that is, including a , to correctly show thousands).\nRename the title of each axis.\nProvide a title, subtitle, and caption for the plot.\nAdd mean lines to both x-axis and y-axis.\nChange theme elements to make data viz more appealing.\n\n\n\n\n4.4.7 Making Changes to Theme Elements\nThere is a laundry list of options to be explored when it comes to editing your plot’s theme elements to make it look exactly as you want. Currently, according to the ggplot2 website, the following is a comprehensive list of elements that you can tinker with.\n\n  line,\n  rect,\n  text,\n  title,\n  aspect.ratio,\n  axis.title,\n  axis.title.x,\n  axis.title.x.top,\n  axis.title.x.bottom,\n  axis.title.y,\n  axis.title.y.left,\n  axis.title.y.right,\n  axis.text,\n  axis.text.x,\n  axis.text.x.top,\n  axis.text.x.bottom,\n  axis.text.y,\n  axis.text.y.left,\n  axis.text.y.right,\n  axis.ticks,\n  axis.ticks.x,\n  axis.ticks.x.top,\n  axis.ticks.x.bottom,\n  axis.ticks.y,\n  axis.ticks.y.left,\n  axis.ticks.y.right,\n  axis.ticks.length,\n  axis.ticks.length.x,\n  axis.ticks.length.x.top,\n  axis.ticks.length.x.bottom,\n  axis.ticks.length.y,\n  axis.ticks.length.y.left,\n  axis.ticks.length.y.right,\n  axis.line,\n  axis.line.x,\n  axis.line.x.top,\n  axis.line.x.bottom,\n  axis.line.y,\n  axis.line.y.left,\n  axis.line.y.right,\n  legend.background,\n  legend.margin,\n  legend.spacing,\n  legend.spacing.x,\n  legend.spacing.y,\n  legend.key,\n  legend.key.size,\n  legend.key.height,\n  legend.key.width,\n  legend.text,\n  legend.text.align,\n  legend.title,\n  legend.title.align,\n  legend.position,\n  legend.direction,\n  legend.justification,\n  legend.box,\n  legend.box.just,\n  legend.box.margin,\n  legend.box.background,\n  legend.box.spacing,\n  panel.background,\n  panel.border,\n  panel.spacing,\n  panel.spacing.x,\n  panel.spacing.y,\n  panel.grid,\n  panel.grid.major,\n  panel.grid.minor,\n  panel.grid.major.x,\n  panel.grid.major.y,\n  panel.grid.minor.x,\n  panel.grid.minor.y,\n  panel.ontop,\n  plot.background,\n  plot.title,\n  plot.title.position,\n  plot.subtitle,\n  plot.caption,\n  plot.caption.position,\n  plot.tag,\n  plot.tag.position,\n  plot.margin,\n  strip.background,\n  strip.background.x,\n  strip.background.y,\n  strip.clip,\n  strip.placement,\n  strip.text,\n  strip.text.x,\n  strip.text.x.bottom,\n  strip.text.x.top,\n  strip.text.y,\n  strip.text.y.left,\n  strip.text.y.right,\n  strip.switch.pad.grid,\n  strip.switch.pad.wrap\n\nIt’s not likely that we will encounter all these theme elements in this book. But, the ones we do use, we will use heavily. For example, I prefer to design all of my data visualizations without the “axis ticks” (those small lines sticking out from the plot just above, or beside, each yardage number).\n\n\n\n\n\n\nTip\n\n\n\nPlease note the keywords in the above paragraph: “I prefer.”\nNearly all the work you conduct within the theme() of your data visualizations are just that - your preference. I very much have a “personal preference” that unites all the data viz work that I do and share for public consumption.\nYou can feel free to follow along with my preferences, including the use of the upcoming nfl_analytics_theme() I will provide, or to make slight (or major!) adjustments to everything we cover in the coming section to make it fit your artistic vision.\nBe creative and do not be afraid to experiment with all the options available to you in theme().\n\n\nLet’s start by removing the ticks on both the x- and y-axis. To do so, we will add the theme() argument at the end of your prior ggplot code and then start building out each and every change we want to make from the above list of options.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\") +\n  theme(\n    axis.ticks = element_blank())\n\n\n\nEditing theme to remove axis ticks\n\n\n\nWithin the theme() argument, we take the specific element we wish to change (from the above list of possibilities from the ggplot2 website) and then provide the instruction on what to do. In this case, since we wish to completely remove the axis.ticks from the entire plot, we can provide element_blank() which removes them. We can continue making changes to the plot’s elements by adding all of these preferences to out theme().\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"Cumulative Passing Yards\",\n       subtitle = \"Non-Play Action vs. Play Action\") +\n  theme(\n    axis.ticks = element_blank(),\n    axis.title = element_text(family = \"Roboto\",\n                              size = 10, \n                              color = \"black\"),\n    axis.text = element_text(family = \"Roboto\",\n                             face = \"bold\",\n                             size = 10,\n                             color = \"black\"),\n    plot.title.position = \"plot\",\n    plot.title = element_text(family = \"Roboto\",\n                              size = 16,\n                              face = \"bold\",\n                              color = \"#E31837\",\n                              vjust = .02,\n                              hjust = 0.5),\n    plot.subtitle = element_text(family = \"Roboto\",\n                                 size = 12,\n                                 color = \"black\",\n                                 hjust = 0.5),\n    plot.caption = element_text(family = \"Roboto\",\n                                size = 8,\n                                face = \"italic\",\n                                color = \"black\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major =  element_line(color = \"#d0d0d0\"),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    plot.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_blank())\n\n\n\nAdding a multitude of theme elements to the plot\n\n\n\nThere is a lot going on now in our theme() argument. Importantly, you may notice that we used element_blank() to remove the axis tick marks, but then switched to using element_text() and element_rect() for the reminder of the edits with our theme(). Both are one of four theme elements that can be modified in the above fashion.\n\n\n\n\n\n\nTip\n\n\n\nThe Four Theme Elements You Can Edit\nWhen working on editing your plot to your liking, you can make changes using one of four theme elements:\n\n\nelement_blank() - this used to entirely remove an element from the plot like we did with axis.ticks.\n\nelement_rect() - this is used to make changes to the borders and backgrounds of a plot.\n\nelement_lines() - this is used to make changes to any element in the plot that is a line.\n\nelement_text() - this is used to make change to any element in the plot that is text.\n\n\n\nWe first made changes to the text of the title associated with the x- and y-axis and made edits to the text of the numeric values for each yardage distance. This process is started by using axis.title and axis.text in conjunction with element_text(), since that is the specific element type we are wishing to edit. Within our element_text(), we provided the numerous argument on how we wished to edit the text by providing the family (or the font), the face, the size, and the color.\nAfter, we got a little fancy in our edits to our plots title and subtitle. I knew that I wanted to center both directly in the middle of the plot. Rather than figuring out the specific horizontal adjustment needed, I used the plot.title.position() argument and set it to \"plot\", which used the entire width of our plot as the reference point for where to center the plot title and subtitle.\nTo take advantage of this, we followed by using the plot.title() argument to set the title’s horizontal adjustment to 0.5 (hjust = 0.5). As you may guess, the inclusion of 0.5 instructs the output to center the title (and the subtitle in the ensuing edit) directly over the middle of the plot (as calculated through our prior use of plot.title.position().\nOur next significant change occurred by changing the aesthetics of the plot’s grid lines, background, and border. Because we are working with line and background elements, we switch from element_text() and begin to use either element_line() or element_rect() (as well as again using element_blank() to completely remove the panel’s minor grid lines).\n\n\n\n\n\n\nTip\n\n\n\nIn a plot, which are the minor grid lines and which are the major?\nIn a ggplot2 plot, minor grid lines are those lines that hit either the x- or y-axis between the continuous or discrete values. Conversely, major grid lines are the lines that hit the axis at the same spot as the data values.\nIn the case of the current plot, our major grid lines are those that hit the x-axis at 500, 1,000, 1,500, and so on and hit the y-axis at 400, 600, 800, etc. The minor grid lines met the axis between the major grid lines.\n\n\nAfter removing the panel’s minor grid lines (again, a personal preference of mine), we also change the color of the panel’s major grid lines, then change the color of the plot’s background (both using element_rect()). The end result is a aesthetically pleasing data visualization."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-your-own-ggplot2-theme",
    "href": "04-nfl-analytics-visualization.html#creating-your-own-ggplot2-theme",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.5 Creating Your Own ggplot2 Theme",
    "text": "4.5 Creating Your Own ggplot2 Theme\nAs mentioned, I have a distinctive “brand and look” for the data visualizations I create that make use of the same design elements and choices. Rather than copy and paste those into each and every ggplot piece of code I write, I’ve opted to consolidate all the theme() element changes into my own nfl_analytics_theme() function. In much the same way, I’ve created a “quick and easy” theme for use in this book. To get started, running the following chunk of code will create a function titled nfl_analytics_theme and place it into your RStudio environment.\n\nnfl_analytics_theme &lt;- function(..., base_size = 12) {\n  \n    theme(\n      text = element_text(family = \"Roboto\", size = base_size),\n      axis.ticks = element_blank(),\n      axis.title = element_text(color = \"black\",\n                                face = \"bold\"),\n      axis.text = element_text(color = \"black\",\n                               face = \"bold\"),\n      plot.title.position = \"plot\",\n      plot.title = element_text(size = 16,\n                                face = \"bold\",\n                                color = \"black\",\n                                vjust = .02,\n                                hjust = 0.5),\n      plot.subtitle = element_text(color = \"black\",\n                                   hjust = 0.5),\n      plot.caption = element_text(size = 8,\n                                  face = \"italic\",\n                                  color = \"black\"),\n      panel.grid.minor = element_blank(),\n      panel.grid.major =  element_line(color = \"#d0d0d0\"),\n      panel.background = element_rect(fill = \"#f7f7f7\"),\n      plot.background = element_rect(fill = \"#f7f7f7\"),\n      panel.border = element_blank())\n}\n\nYou may notice that the elements in the above theme creation are quite similar to the ones we passed into our previous plot. And that is true, and the results will be nearly 99.9% identical. After wrapping our theme() inside a function, indicated by the opening and closing curly brackets { }, we can provide our desired theme element appearances as we would within a regular ggplot2 code block.\nHowever, in the above theme function, we have streamlined the basis a bit by indicating a base_size of all the text, when means all text output will be in size 12 font unless specifically indicated in the element (for example, we have the plot.title set to have a size of 16). As well, the same process was done for font (Roboto) so there was not a need to type it repeatedly into every element.\nBased on the above example, you are free to create as detailed a theme function as you desire. The beauty of creating your own theme like above is you will no longer need to edit each portion of the every plot element.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"**Cumulative Passing Yards**\",\n       subtitle = \"*Non-Play Action vs. Play Action*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  nfl_analytics_theme()\n\n\n\nConsolidating theme element changes into nfl_analytics_theme()\n\n\n\nAs you can see, we just consolidated 28 lines of code into a single line of code by wrapping all our theme elements into an easy to construct function.\nUnfortunately, you will notice that the resulting plot does not output with the title (“Cumulative Passing Yards”) in Kansas City red like in our original. This is because, in our nfl_analytics_theme() function, the color for axis.title() is set to \"black\". Thankfully, we can make this small edit within our ggplot code to switch the title back to Kansas City red, highlighting the idea that - despite the theme being built into a function - we still have the ability to make necessary edits on the fly without including all 28 lines of code. With the nfl_analytics_theme() active, we can still add additional theme elements as needed to make modifications, as seen below.\n\nggplot(data = play_action_data, aes(x = yds, y = pa_yds)) +\n  geom_mean_lines(aes(v_var = yds, h_var = pa_yds), \n                  size = 0.8,\n                  color = \"black\", \n                  linetype = \"dashed\",\n                  alpha = 0.5) +\n  geom_point(shape = 21,\n             fill = play_action_data$team_color,\n             color = play_action_data$team_color2,\n             size = 4.5) +\n  geom_text_repel(aes(label = player),\n                  box.padding = 0.45,\n                  size = 3,\n                  family = \"Roboto\",\n                  fontface = \"bold\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(x = \"Non-Play Action Yards\",\n       y = \"Play Action Yards\",\n       title = \"**Cumulative Passing Yards**\",\n       subtitle = \"*Non-Play Action vs. Play Action*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  nfl_analytics_theme() +\n  theme(plot.title = element_markdown(color = \"#E31837\"))\n\n\n\nEditing the color of the plot’s title after including nfl_analytics_theme()"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#using-team-logos-in-plots",
    "href": "04-nfl-analytics-visualization.html#using-team-logos-in-plots",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.6 Using Team Logos in Plots",
    "text": "4.6 Using Team Logos in Plots\nTo explore the process of placing team logos into a plot, let’s stick with our previous example of working with play action passing data but explore it at the team level rather than individual quarterbacks. To gather the data, we can use vroom to read it in from the book’s GitHub.\n\nteam_playaction &lt;- vroom(\"http://nfl-book.bradcongelio.com/team-pa\")\n\nThe resulting team_playaction data contains nearly identical information to the previous QB-level data. However, there is a slight change in how Sports Info Solutions charts passing yards on the team level. You will notice a column for both net_yds and gross_yds. When charting passing attempts in football, a player’s individual passing yards are aggregated under gross yards with all lost yardage resulting from a sack being included. On the other hand, team passing yards are always presented in net yards and any lost yardage from sacks are not included. Case in point, the gross_yds number in our team_playaction data is greater than the net_yds for all 32 NFL teams. In any case, we will build our data visualization using net_yds.\nIn order to include team logos in the plot, we must first merge in team logo information (again with the understanding that we could use nflplotR if team abbreviations were included in the data). We can collect the team logo information using the load_teams() function within nflreadR. There are three different variations of each team’s logo available in the resulting data: (1.) the logo from ESPN, (2.) the logo from Wikipedia, (3.) a pre-edited version of the logo that is cropped into a square.\n\n\n\n\n\n\nNote\n\n\n\nThere are slight differences in disk space, pixels, and utility in the provided ESPN, Wikipedia, and squared versions of the logos.\nThe Wikipedia versions are, generally, smaller in size. The Arizona Cardinals logo, for example, is just 9.11 KB in size from Wikipedia while the ESPN version of the logo is 20.6 KB. This difference in disk size is the result of each image’s dimensions and pixels. The ESPN version, with the larger disk size, is also a higher quality image that is scaled in 500x500 dimensions (and 500 pixels). The Wikipedia version is scaled in 179x161 dimensions at 179 pixels and 161 pixels, respectively. The squared version of the logo is a 200x200 image at 200 pixels with a background matching the team’s primary color.\nWhat does this mean? The ESPN version of the logo is better for those applications where the logo will be large and you do not want any loss of quality. The Wikipedia version, conversely, is better suited for applications like ours: for use in a small-scale data visualization. We do not plan on “blowing up” the image, thus losing quality and the smaller disk space size of the images allows for a slightly quicker rendering time when we use the ggimage package. While sparingly used, the squared version of the logo can be used in certain data visualization applications that requires the team logo to quickly and easily “merge” into the background of the plot (more on this later in this chapter, though).\n\n\nBecause of the above explanation, we can collect just the team nicknames and the Wikipedia version of the logo to merge into our existing team_playaction data.\n\nteams &lt;- nflreadr::load_teams(current = TRUE) %&gt;%\n  select(team_nick, team_logo_wikipedia)\n\nteam_playaction &lt;- team_playaction %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_nick\"))\n\nteam_playaction\n\nWith the data now containing the correct information, we can build a basic version of our data visualization using the same geom_point as above, and continue to configure the information on both the x- and y-axis, to verify that everything is working correctly before switching out geom_point for geom_image in order to bring the team logos into the plot.\n\nggplot(data = team_playaction, aes(x = net_yds, y = pa_net_yds)) +\n  geom_hline(yintercept = mean(team_playaction$pa_net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_vline(xintercept = mean(team_playaction$net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_point() +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(title = \"**Net Passing Yards Without Play Action vs. With Play Action**\",\n       subtitle = \"*2022 NFL Regular Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\",\n       x = \"Net Yards Without Play Action\",\n       y = \"Net Yards With Play Action\") +\n  nfl_analytics_theme()\n\n\n\nBuilding the plot before adding team logos instead of points\n\n\n\nThe resulting plot, constructed in nearly an identical manner to our above example, looks correct and we are ready to swap out geom_point for team logos.\n\n\n\n\n\n\nImportant\n\n\n\nBefore proceeding with this next step, be sure that you have the ggimage package installed and loaded.\n\n\n\nggplot(data = team_playaction, aes(x = net_yds, y = pa_net_yds)) +\n  geom_hline(yintercept = mean(team_playaction$pa_net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_vline(xintercept = mean(team_playaction$net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_image(aes(image = team_logo_wikipedia)) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(title = \"**Net Passing Yards Without Play Action vs. With Play Action**\",\n       subtitle = \"*2022 NFL Regular Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\",\n       x = \"Net Yards Without Play Action\",\n       y = \"Net Yards With Play Action\") +\n  nfl_analytics_theme()\n\n\n\nUsing team logos without including an aspect ratio\n\n\n\nBy using the geom_image function, we are able to wrap the image argument within an aesthetics call (that is, aes()) and then stipulate that the team_logo_wikipedia variable is to serve as the image associated with each data point.\nBut, wait: the image looks horrible, right? Indeed, the logos are pixelated, are skewed in shape, and are just generally unpleasant to look at.\nThat is because we failed to provide an aspect ratio for the team logos. In this case, we need to add asp = 16/9.\n\n\n\n\n\n\nNote\n\n\n\nWhy are we including a specific aspect ratio of 16/9 for each team logo? Good question.\nAn aspect ratio of 16/9 refers to the proportional relationship between the width and height of a rectangular display or image. In this case, the width of the image is 16 units, and the height is 9 units. Importantly, this aspect ratio is commonly used for widescreen displays, including most (if not all) modern televisions, computer monitors, and smartphones.\nAs well, the 16/9 aspect ratio is sometimes referred to as 1.78:1, which means that the width is 1.78 times the height of the image. This aspect ratio is wider than the traditional 4:3 aspect ratio that was common used in older television and CRT-based computer monitors.\n\n\nWe can make the quick adjustment in our prior code to provide the correct aspect ratio for each of our team logos:\n\nggplot(data = team_playaction, aes(x = net_yds, y = pa_net_yds)) +\n  geom_hline(yintercept = mean(team_playaction$pa_net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_vline(xintercept = mean(team_playaction$net_yds), \n             linewidth = 0.8, \n             color = \"black\", \n             linetype = \"dashed\") +\n  geom_image(aes(image = team_logo_wikipedia), asp = 16/9) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 6),\n                     labels = scales::label_comma()) +\n  labs(title = \"**Net Passing Yards Without Play Action vs. With Play Action**\",\n       subtitle = \"*2022 NFL Regular Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\",\n       x = \"Net Yards Without Play Action\",\n       y = \"Net Yards With Play Action\") +\n  nfl_analytics_theme()\n\n\n\nUsing team logos with the correct 16/9 aspect ratio"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-geom_line-plot",
    "href": "04-nfl-analytics-visualization.html#creating-a-geom_line-plot",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.7 Creating a geom_line Plot",
    "text": "4.7 Creating a geom_line Plot\nA geom_line() plot is useful when you want to display the trends and/or relationships in data over a continuous variable (such as seasons). In that context a geom_line() plot can be used to explore player statistics over time (such as passing yards and rushing yards), or to do the same but at the team level, or even comparing one team against another by including two (or more!) lines on one plot.\nLike all other geom_ types, the basic foundation of a line graph can be created by, first, calling the ggplot() function and then adding geom_line() after. To begin building our first line graph, let’s read in the below data that contains information regarding the fourth-down attempt percentages by the Philadelphia Eagles into a data frame titled eagles_fourth_downs.\n\neagles_fourth_downs &lt;- vroom(\"http://nfl-book.bradcongelio.com/phi-4th-downs\")\n\nThe data frames includes information regarding the season, the total fourth downs in each season, the number of fourth-down attempts in total_go, and the conversion percentage in go_pct. As well, the data has already been joined with information from nflreadr::load_teams() to include colors and logos.\nLet’s build the foundation of the plot by using ggplot() and geom_line() and placing the season on the x-axis and the go_pct on the y-axis.\n\nggplot(eagles_fourth_downs, aes(x = season, y = go_pct)) +\n  geom_line()\n\n\n\nBuilding the foundation of a line plot\n\n\n\nThe above output is a very basic line graph. Based on this output, we can start making modifications to multiple items to reach the end result in a step-by-step fashion. First, let’s explore making changes to scale of both the x and y-axis. Given that this is an examination of a yearly statistic, the x-axis should include every season in the data frame. The y-axis can be changed to be whole numbers, and to include the % after each number.\n\nggplot(eagles_fourth_downs, aes(x = season, y = go_pct)) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(2012, 2022, 1)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format())\n\n\n\nEditing axis details\n\n\n\nPlease note that difference in how the breaks on each scale was handled. Because we want each year on the x-axis, we manually controlled the output by using the seq() function to start the x-axis at 2012 and to increase by 1 until the final number of 2022 is reached. On the other hand, we used the pretty_breaks() function from scales to format the number of breaks on the y-axis and then used percent_format() to edit the labels to be whole numbers with the percentage sign included.\nNext, we can add more geom_ types to highlight the general percentage for each season on the x-axis. We will add one geom_point() to to indicate the percentage for each season. And then, for aesthetic purposes, we will add a second geom_point() that is larger than the original, with a color that matches the eventual background, to give the visual impression that the line doesn’t quite “reach” the point. Lastly, because we are dealing with aesthetic issues like size and color, we will increase the size of the geom_line() and also add the Eagles’ secondary team color to it.\n\nggplot(eagles_fourth_downs, aes(x = season, y = go_pct)) +\n  geom_line(size = 2, color = eagles_fourth_downs$team_color2) +\n  geom_point(size = 5, color = \"#f7f7f7\") +\n  geom_point(size = 3, color = eagles_fourth_downs$team_color) +\n  scale_x_continuous(breaks = seq(2012, 2022, 1)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format())\n\n\n\nAdding points to the line\n\n\n\nThe order in which we apply all three geom_ types is important because, as previously discussed, a ggplot() is layered with the first items being placed under the items that come next. In this case, geom_line() is under the first geom_point() which is then placed under the second geom_point().\nTo complete the visualization, we can attached our custom nfl_analytics_theme() and then use xlab, ylab, and labs to edit the titles of the each axis and to include a title, subtitle, and caption.\n\nggplot(eagles_fourth_downs, aes(x = season, y = go_pct)) +\n  geom_line(size = 2, color = eagles_fourth_downs$team_color2) +\n  geom_point(size = 5, color = \"#f7f7f7\") +\n  geom_point(size = 3, color = eagles_fourth_downs$team_color) +\n  scale_x_continuous(breaks = seq(2012, 2022, 1)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format()) +\n  nfl_analytics_theme() +\n  xlab(\"Season\") +\n  ylab(\"Go For It Percentage\") +\n  labs(title = \"**Philadelphia Eagles: Fourth-Down Attempt Percentage**\",\n       subtitle = \"*2012 - 2022: Regular Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nAdding the finishing touches to the `geom_line() plot\n\n\n\nThe resulting visualization shows cases how often the Philadelphia Eagles went for it on fourth down between 2012 and 2022. The upward trend , especially starting in 2019, generally follows an established trend among NFL head coaches. Given that, how does the Eagles’ trend compare to the rest of the NFL?\nWe can visualize this by adding a second geom_line that shows the averaged go_pct for the other 31 NFL teams.\n\n4.7.1 Adding Secondary geom_line For NFL Averages\nThe combined_fourth_data data frame created by running the below code includes the same information as before for the Philadelphia Eagles, but also includes the NFL as a posteam for the 2012-2022 seasons along with the total fourth down attempt and conversion rate.\n\ncombined_fourth_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/4th-data\")\n\nThe addition of a second geom_line() to the plot - one that belongs to the Eagles and one that belongs to averaged number for all other NFL teams - requires a few new additions to our previous code. First, in the ggplot() call, we’ve added group = posteam. Because the data is structured so that PHI and NFL are both observations in the posteam column, the use of group = posteam instructs ggplot() to “split” that information into two and to create a geom_line() for each. Second, we’ve added scale_color_manual to the code and manually assigned the appropriate hex code colors to each like (#004C54 for the Eagles and #013369 for the rest of the NFL).\n\nggplot(combined_fourth_data, aes(x = season,\n                                 y = go_pct,\n                                 group = posteam)) +\n  geom_line(size = 2, aes(color = posteam)) +\n  scale_color_manual(values = c(\"#013369\", \"#004C54\")) +\n  geom_point(size = 5, color = \"#f7f7f7\") +\n  geom_point(size = 3, aes(color = posteam)) +\n  nfl_analytics_theme() +\n  scale_x_continuous(breaks = seq(2012, 2022, 1)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format()) +\n  nfl_analytics_theme() +\n  theme(legend.position = \"none\") +\n  xlab(\"Season\") +\n  ylab(\"Go For It Percentage\") +\n  labs(title = \"**Philadelphia Eagles vs. NFL: Fourth-Down Attempt Percentage**\",\n       subtitle = \"*2012 - 2022: Regular Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nAdding a secondary line to compare to the NFL average\n\n\n\nThe resulting visualization does a very good job at showing the upward trend in head coaches going for it on fourth down and that the Eagles are consistently more aggressive in such situations than the rest of the NFL."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-geom_area-plot",
    "href": "04-nfl-analytics-visualization.html#creating-a-geom_area-plot",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.8 Creating a geom_area Plot",
    "text": "4.8 Creating a geom_area Plot\nA geom_area() plot is similar to a geom_line() plot in that it is also often used to display quantitative data over a continuous variable. The core difference, however, is that the geom_area() plot provides the ability to fill in the surface area between the x-axis and the line. To showcase this, the mahomes_epa data frame created by running the below code contains Patrick Mahomes’ average EPA for each week of the 2022 regular season, as well as the opponent and the opponent’s team logo.\n\nmahomes_epa &lt;- vroom(\"http://nfl-book.bradcongelio.com/mahomes_epa\")\n\nThe basics of the completed plot can be output by placing week on the x-axis and mean_qb_epa on the y-axis and then using geom_area(). Also included in the geom_area() are two arguments (fill and alpha). The hex color for the Kansas City Chiefs’ secondary color is provided to “fill” in the area and then the alpha is set to 0.4 to make it slightly transparent.\n\nggplot(data = mahomes_epa, aes(x = week, y = mean_qb_epa)) +\n  geom_area(fill = \"#FFB612\", alpha = 0.4)\n\n\n\nBuilding the foundation of a geom_area() plot\n\n\n\nTo help the geom_area() “pop” a bit more off the background, we will now included a geom_line() that “rides” across the top of the geom_area() and also add geom_smooth() to show the week-by-week trend of Mahomes’ average EPA.\n\nggplot(data = mahomes_epa, aes(x = week, y = mean_qb_epa)) +\n  geom_smooth(se = FALSE, color = \"black\", linetype = \"dashed\") +\n  geom_area(fill = \"#FFB612\", alpha = 0.4) +\n  geom_line(color = \"#E31837\", size = 1.5)\n\n\n\nAdding geom_smooth() and `geom_line() with team color\n\n\n\nWhile the plot does provide information regarding Mahomes’ average EPA, we still do not know who he was playing on any given week. To add this information, we can use the geom_image() function from ggimage to add the logo of each opponent to the points of the geom_area().\n\nggplot(data = mahomes_epa, aes(x = week, y = mean_qb_epa)) +\n  geom_smooth(se = FALSE, color = \"black\", linetype = \"dashed\") +\n  geom_area(fill = \"#FFB612\", alpha = 0.4) +\n  geom_line(color = \"#E31837\", size = 1.5) +\n  geom_image(aes(image = team_logo_wikipedia), size = 0.045, asp = 16/9)\n\n\n\nAdding logos of the opponent to the line\n\n\n\nWith the logos of each opponent added, we can turn to the finishing touches including adding our custom theme, changing the title of each axis, and adding a title. We also need to make edits to the scale of each axis, again using seq() to include all 18 weeks on the x-axis.\n\nggplot(data = mahomes_epa, aes(x = week, y = mean_qb_epa)) +\n  geom_smooth(se = FALSE, color = \"black\", linetype = \"dashed\") +\n  geom_area(fill = \"#FFB612\", alpha = 0.4) +\n  geom_line(color = \"#E31837\", size = 1.5) +\n  geom_image(aes(image = team_logo_wikipedia), size = 0.045, asp = 16/9) +\n  scale_x_continuous(breaks = seq(1,18,1)) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  xlab(\"Week\") +\n  ylab(\"Average QB EPA\") +\n  labs(title = \"**Patrick Mahomes: Mean QB EPA per Week**\",\n       subtitle = \"*2022 Regular Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nAdding finishing touches to the geom_area() plot\n\n\n\nThe finished visualization indicates that Mahomes’ had his highest average EPA in week 1 against the Arizona Cardinals (at roughly 0.8) before dropping considerably for week two through six. However, after a spike in week 7 against the 49ers, the geom_smooth() shows a continued downward trend for the remainder of the season, including four poor performances (by Mahomes’ standard, anyways) against the Broncos, Seahawks, and Raiders to end the season."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-geom_col-plot",
    "href": "04-nfl-analytics-visualization.html#creating-a-geom_col-plot",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.9 Creating a geom_col Plot",
    "text": "4.9 Creating a geom_col Plot\nTo begin creating our geom_col plot, we will use vroom to gather the necessary data into a data frame titled qb_thirddown_data. The resulting data frame includes the top ten quarterback in “third down aggressiveness.” The metric is calculated by first gathering the total number of 3rd down passing attempts by each quarterback with 10 or less yards to go. A pass attempt is considered “aggressive” if the total air yards is equal to - or greater than - the needed yards to go. Based on this calculation, Tua Tagovailoa was the most aggressive quarterback on 3rd downs during the 2022 regular season.\nBecause this is not a metric that is constructed against a continuous variable (such as season in the geom_line() examples), we turn to using geom_col().\n\nqb_thirddown_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/qb-3rd-data\")\n\nTo start, we will place passer_id on the x-axis and qb_agg_pct on the y-axis. However, to make sure the columns are plotted in descending order, we use the reorder() function to arrange passer_id in descending order by the qb_agg_pct variable.\n\nggplot(data = qb_thirddown_data, aes(x = reorder(passer_id, -qb_agg_pct), y = qb_agg_pct)) +\n  geom_col()\n\n\n\nReordering the quarterbacks in descending order by aggressiveness percentage\n\n\n\nThe resulting plot is a solid foundation and just needs aesthetic adjustments and additions to bring it to the final version. We will first use the fill argument to add the respective team colors to each column and then the color argument to add the secondary team color as an outline to each column.\n\nggplot(data = qb_thirddown_data, aes(x = reorder(passer_id, -qb_agg_pct),\n                                     y = qb_agg_pct)) +\n  geom_col(fill = qb_thirddown_data$team_color,\n           color = qb_thirddown_data$team_color2)\n\n\n\nAdd team colors to each column\n\n\n\nNext we can make adjustments to each axis. Using scale_y_continuous, we will set the number of breaks with pretty_breaks() and then use percent_format() to adjust the scale into whole numbers with the accompanying percentage sign. The expand() argument is used is set to c(0,0) to get rid of the “dead space” between the bottom of each column and the y-axis.\nWe can also provide the xlab(), ylab(), and include our custom nfl_analytics_theme() to the plot.\n\nggplot(data = qb_thirddown_data, aes(x = reorder(passer_id, -qb_agg_pct),\n                                     y = qb_agg_pct)) +\n  geom_col(fill = qb_thirddown_data$team_color,\n           color = qb_thirddown_data$team_color2) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format(),\n                     expand = c(0,0)) +\n  xlab(\"\") +\n  ylab(\"QB Aggressiveness on 3rd Down\") +\n  nfl_analytics_theme()\n\n\n\nUse the scales package to edit each axis\n\n\n\nWe will provide two ways to identify which column belong to which quarterback: by adding the player name to the column and by including each player’s headshot at the bottom of each.\nTo place the player name on each column, we will use geom_text() wrapped inside the with_outer_glow() function (which is part of the ggfx package). The with_outer_glow() function allows us to apply a drop shadow effect to the text, making sure that it is easy to read on the colored background of each column. The geom_text function accepts the arguments relating to the angle, horizontal adjustment hjust, color, font family, font face, and size of the text. The with_outer_glow function is used to apply the sigma (how dark or light to make the shadow/glow), expand (how far to spread out the glow/shadow), and color.\n\nggplot(data = qb_thirddown_data, aes(x = reorder(passer_id, -qb_agg_pct),\n                                     y = qb_agg_pct)) +\n  geom_col(fill = qb_thirddown_data$team_color,\n           color = qb_thirddown_data$team_color2) +\n  with_outer_glow(geom_text(aes(label = passer),\n                            position = position_stack(vjust = .98),\n                            angle = 90, hjust = .98, color = \"white\",\n                            family = \"Roboto\",\n                            fontface = \"bold\", size = 8),\n                  sigma = 6, expand = 1, color = \"black\") +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format(),\n                     expand = c(0,0)) +\n  xlab(\"\") +\n  ylab(\"QB Aggressiveness on 3rd Down\") +\n  nfl_analytics_theme()\n\n\n\nAdding player names with text effects using the ggfx package\n\n\n\nTo add player headshot to the bottom of each column, we will use the element_nfl_headshot function from the nflplotR package. Inside the theme() option, we use element_nfl_headshot() to replace the axis_text.x(). It is important to remember to use passer_id on the x-axis (rather than passer_name or something similar). The nflplotR package uses the passer_id to automatically pull the correct headshot for each player. Without including passer_id in the x-axis, the resulting headshots will be the “blank” NFL picture.\n\nggplot(data = qb_thirddown_data, aes(x = reorder(passer_id, -qb_agg_pct),\n                                     y = qb_agg_pct)) +\n  geom_col(fill = qb_thirddown_data$team_color,\n           color = qb_thirddown_data$team_color2) +\n  with_outer_glow(geom_text(aes(label = passer),\n                            position = position_stack(vjust = .98),\n                            angle = 90,\n                            hjust = .98,\n                            color = \"white\",\n                            family = \"Roboto\",\n                            fontface = \"bold\",\n                            size = 8),\n                  sigma = 6, expand = 1, color = \"black\") +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format(),\n                     expand = c(0,0)) +\n  theme(axis.text.x = nflplotR::element_nfl_headshot(size = 1)) +\n  xlab(\"\") +\n  ylab(\"QB Aggressiveness on 3rd Down\") +\n  nfl_analytics_theme()\n\n\n\nUsing nflplotR to include headshots with the element_nfl_headshots() function\n\n\n\nTo complete the plot, we can add the title, subtitle, and caption using the labs function.\n\nggplot(data = qb_thirddown_data, aes(x = reorder(passer_id, -qb_agg_pct),\n                                     y = qb_agg_pct)) +\n  geom_col(fill = qb_thirddown_data$team_color,\n           color = qb_thirddown_data$team_color2) +\n  with_outer_glow(geom_text(aes(label = passer),\n                            position = position_stack(vjust = .98),\n                            angle = 90,\n                            hjust = .98,\n                            color = \"white\",\n                            family = \"Roboto\",\n                            fontface = \"bold\",\n                            size = 8),\n                  sigma = 6, expand = 1, color = \"black\") +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format(),\n                     expand = c(0,0)) +\n  theme(axis.text.x = nflplotR::element_nfl_headshot(size = 1)) +\n  xlab(\"\") +\n  ylab(\"QB Aggressiveness on 3rd Down\") +\n  nfl_analytics_theme() +\n  labs(title = \"**QB Aggressiveness on Third Down**\",\n       subtitle = \"*Numbers of Times Air Yards &gt;= Yards To Go*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nAdding fishing touches to the geom_col() plot\n\n\n\nAs highlighted in the resulting plot, Tagovailoa was the only QB in the league to go above 60% in aggressiveness (Derek Carr was an even 60%). In this case, our domain knowledge of the NFL is important as we will recall that Tua was in concussion protocol for portions of the 2022 season. His aggressive percentage could be slightly inflated because of the fewer games compared to the other quarterbacks.\n\n4.9.1 An Alternative geom_col Plot\nThe geom_col() plot is incredibly versatile, allowing you to create multiple types of visualizations. For example, we can use the data frame below, epa_per_play, to graph each teams average EPA per play during the 2022 regular season. However, rather than plot it in a standard column format like above, we will plot it in a “left-to-right” design with each team diverging from the 0.00 EPA per Play.\n\nepa_per_play &lt;- vroom(\"http://nfl-book.bradcongelio.com/epa-per-play\")\n\nThe ggplot() code required to output the visualization is quite similar to our above geom_col() plot for QB aggressiveness. There are several distinct differences, however.\n\nWe’ve included a fill = if_else() argument in geom_col(). If a team has a positive EPA per play, the column for that team will be blue. If the team has a negative EPA per play, the column will be red.\nThe use of scale_fill_identity() is used because the data has already been scaled (by EPA per play) and already represents aesthetic values that are native to ggplot2().\n\nscale_y_discrete is used because the y-axis is team names rather than a continuous variables. Moreover,expand = c(.05, 0)) is included to make sure that the Chiefs and Texans logos are not clipped during the processing of the plot.\n\n\nggplot(data = epa_per_play, aes(x = epa_per_play,\n                                y = reorder(posteam, epa_per_play))) +\n  geom_col(aes(fill = if_else(epa_per_play &gt;= 0,\n                              \"#2c7bb6\", \"#d7181c\")),\n           width = 0.2) +\n  geom_vline(xintercept = 0, color = \"black\") +\n  scale_fill_identity() +\n  geom_image(aes(image = team_logo_wikipedia),\n             asp = 16/9, size = .035) +\n  xlab(\"EPA Per Play\") +\n  ylab(\"\") +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 6)) +\n  scale_y_discrete(expand = c(.05, 0)) +\n  nfl_analytics_theme() +\n  theme(axis.text.y = element_blank()) +\n  labs(title = \"**Expected Points Added per Play**\",\n       subtitle = \"*2022 Regular Season*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nAn alternate way to design a geom_col() plot\n\n\n\nIt should not be a surprise that the Chiefs outperform - by a large margin - the rest of the NFL. The Texans and Colts, on the other hand, averaged -0.15 expected points added per play during the 2022 regular season."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-geom_box-plot",
    "href": "04-nfl-analytics-visualization.html#creating-a-geom_box-plot",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.10 Creating a geom_box Plot",
    "text": "4.10 Creating a geom_box Plot\nWe can create a geom_box() plot to examine the distribution of air yards for each playoff team from the 2022 season. A box plot is a suitable way to view distribution but to also view the “skewness” of a metric by also including the interquartile ranges, averages, and any outliers in the data.\n\nplayoff_airyards_plot &lt;- vroom(\"http://nfl-book.bradcongelio.com/playoff-airyards\")\n\nThe geom_boxplot() function includes one more argument - color. In this case, we have set the argument to black. We will also include the geom_jitter() function, which will plot each pass underneath the box plot. This is not a requirement for a box plot, but does add an additional level of aesthetics and detail to the final plot. Last, we use the element_nfl_logo function to place the respective team logo under each box plot.\n\nggplot(data = playoff_airyards_plot, aes(x = posteam,\n                                         y = air_yards,\n                                         fill = posteam)) +\n  geom_jitter(color = playoff_airyards_plot$team_color, alpha = 0.09) +\n  geom_boxplot(color = \"black\") +\n  nflplotR::scale_fill_nfl() +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  ylab(\"Air Yards\") +\n  xlab(\"\") +\n  labs(title = \"**Distribution of Regular Season Air Yards**\",\n       subtitle = \"*2022 NFL Playoff Teams*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  theme(axis.text.x = element_nfl_logo(size = 1.25))\n\n\n\nThe finished geom_boxplot plot\n\n\n\nIn the visualization, each box plot represent the interquartile range (IQR) of air yards distribution for each team. The black line represents the median average, while the area below the black line is the lower quartile (Q1), or the 25th percentile, while the area above is the upper quartile (Q3), or the 75th percentile. The “whisker” coming from both ends of the box plot represent the minimum and maximum amounts (where the minimum is Q1 - 1.5 * IQR and the maximum is Q3 + 1.5 * IQR). The black dots above the whiskers indicate outliers.\nBased on the results, the Miami Dolphins had the highest median air yards of the 2022 playoff teams while the New York Giants had the smallest spread (indicated by the 25th and 75th percentiles being closest to the median)."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-geom_ridges-plot",
    "href": "04-nfl-analytics-visualization.html#creating-a-geom_ridges-plot",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.11 Creating a geom_ridges Plot",
    "text": "4.11 Creating a geom_ridges Plot\nWe can take the same idea used in the geom_boxplot() graph to reformat it to be used in a geom_ridges() plot, which is another option to represent the distribution of a continuous variable.\n\nplayoff_airyards_ridges_plot &lt;- vroom(\"http://nfl-book.bradcongelio.com/ay-ridges\")\n\nBefore creating the plot, we will create a function titled meansd that will assist in including lines within the geom_ridges() to show the mean and standard deviation for each team.\nThe meansd function is included in the geom_density_ridges() arguments. We want to show quantile_lines, so we provide meansd to the quantile_fun argument and then set the color of the quantile lines.\n\n### creating function to add SD, mean, and mean + sd\nmeansd &lt;- function(x, ...) {\n  mean &lt;- mean(x)\n  sd &lt;- sd(x)\n  c(mean - sd, mean, mean + sd)\n}\n\n### plotting the data\nggplot(data = playoff_airyards_ridges_plot, aes(x = air_yards,\n                                                y = reorder(posteam,\n                                                            -air_yards),\n                                                fill = posteam)) +\n  geom_density_ridges(quantile_lines = T,\n                      quantile_fun = meansd,\n                      color = \"#d0d0d0\") +\n  nflplotR::scale_fill_nfl() +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  nfl_analytics_theme() +\n  xlab(\"Air Yards\") +\n  ylab(\"\") +\n  labs(title = \"**Distribution of Regular Season Air Yards**\",\n       subtitle = \"*2022 NFL Playoff Teams*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  theme(axis.text.y = element_nfl_logo(size = 1.25))\n\n\n\nThe finished geom_ridges() plot"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-geom_violin-plot",
    "href": "04-nfl-analytics-visualization.html#creating-a-geom_violin-plot",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.12 Creating a geom_violin Plot",
    "text": "4.12 Creating a geom_violin Plot\n\n\nyards_gained_compared &lt;- vroom(\"http://nfl-book.bradcongelio.com/yards_gained\")\n\n\ncustom_labeller &lt;- function(variable, value){\n  new_labels &lt;- c(\"1st Down\", \"2nd Down\", \"3rd Down\")\n  return(new_labels[value])\n}\n\nggplot(data = yards_gained_compared, aes(x = posteam,\n                                         y = yards_gained,\n                                         group = posteam,\n                                         fill = posteam)) +\n  geom_violin() +\n  stat_summary(fun.y = mean, fun.ymin = mean, fun.ymax = mean,\n               geom = \"crossbar\",\n               width = 0.50,\n               position = position_dodge(width = .25)) +\n  facet_wrap(~down, labeller = labeller(down = custom_labeller)) +\n  scale_fill_manual(values = c(\"#E31837\", \"#013369\")) +\n  ylab(\"Average Yards Gained\") +\n  xlab(\"\") +\n  labs(title = \"*Averaged Yards Gained per Down*\",\n       subtitle = \"**KC Chiefs vs. NFL: 2022 Regular Season**\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  theme(axis.text.x = nflplotR::element_nfl_logo(size = 1.5),\n        legend.position = \"none\",\n        strip.text = element_text(family = \"Roboto\", face = \"bold\"),\n        strip.background = element_rect(fill = \"#f7f7f7\")) +\n  nfl_analytics_theme()\n\n\n\nThe finished geom_violin() plot"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-geom_histogram-plot",
    "href": "04-nfl-analytics-visualization.html#creating-a-geom_histogram-plot",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.13 Creating a geom_histogram Plot",
    "text": "4.13 Creating a geom_histogram Plot\n\n\ndef_yards_allowed &lt;- vroom(\"http://nfl-book.bradcongelio.com/def-yards\")\n\n\nggplot(data = def_yards_allowed, aes(x = yards_allowed, fill = defteam)) +\n  geom_histogram(binwidth = 50) +\n  nflplotR::scale_fill_nfl(type = \"primary\") +\n  scale_x_continuous(breaks = seq(200, 500, 100)) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  facet_wrap(vars(defteam), scales = \"free_x\") +\n  nfl_analytics_theme() +\n  theme(strip.text = element_nfl_wordmark(size = .5),\n        strip.background = element_rect(fill = \"#F7F7F7\")) +\n  xlab(\"Defensive Yards Allowed\") +\n  ylab(\"Total Count\") +\n  labs(title = \"*Histogram of Defensive Yards Allowed*\",\n       subtitle = \"**2022 NFL Regular Season**\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nThe finished geom_histgram() plot"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#saving-your-ggplot2-creations",
    "href": "04-nfl-analytics-visualization.html#saving-your-ggplot2-creations",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.14 Saving Your ggplot2 Creations",
    "text": "4.14 Saving Your ggplot2 Creations\nThe most convenient way to save your finished plot is with the ggsave() function, which will save the last plot created. The function accepts a multitude of different arguments, including filename, device, path, scale, width, height, and dpi.\nIn most cases, you will be able to save your plot using just the filename and dpi arguments.\n\nggsave(\"this-is-your-filename.png\", dpi = 400)\n\nIn the above example, the plot will be outputted in .png format into your current working directory (you can see where the plot will be saved by using getwd()). The image dpi - or “dots per inch” - ultimately controls the plot’s resolution. A higher number will allow you to increase the size the image, but will also increase the size of the file itself. I find that using a dpi between 300 and 400 is suitable for nearly all plots."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#advanced-headshots-method",
    "href": "04-nfl-analytics-visualization.html#advanced-headshots-method",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.15 Advanced Headshots Method",
    "text": "4.15 Advanced Headshots Method\nThe ability to include player headshots onto plots is an outstanding addition to the NFL analytics universe. However, the headshots themselves quite often have “harsh” aesthetics in that the image is cut off at the shoulders and, as a result, can result in plots where the shoulders just seem to “fall off” into the abyss of the plot itself.\nTo correct this, we can use several different package to create circular croppings of the of the individual headshot images and then wrap the player’s name around the image itself.\nTo showcase the process, let’s create a data frame titled complete_wr_airyards with the below code:\n\ncomplete_wr_airyards &lt;- vroom(\"http://nfl-book.bradcongelio.com/complete_ay\")\n\nTo begin the process, we create a data frame called crop_circles_data that includes just one observation for each wide receive that has the receiver_id and headshot_url variables. After, we create a new column in crop_circles_data called circle and then use the circle_crop function from the cropcircles package to automatically crop each player’s headshot into a uniform circle. The resulting image is saved locally on your device. Last, since we no longer need the headshot_url information, we drop that from the crop_circles_data data frame.\n\ncrop_circles_data &lt;- complete_wr_airyards %&gt;%\n  select(receiver_id, headshot_url) %&gt;%\n  distinct(receiver_id, .keep_all = TRUE)\n\ncrop_circles_data &lt;- crop_circles_data %&gt;%\n  mutate(circle = cropcircles::circle_crop(headshot_url))\n\ncrop_circles_data &lt;- crop_circles_data %&gt;%\n  select(receiver_id, circle)\n\nThe next step, which is a modified version of an approach used by Tanya Shapiro, is multifaceted. We first use the magick package to create a composite of the image and place a white background behind the circular cropped headshot.\nWe then create a function called plot_image_label that will wrap each player’s name around the top of the circular headshot and then proceed with producing the images with the names wrapped using ggplot().\n\nborder &lt;- function(im) {\n  ii &lt;- magick::image_info(im)\n  ii_min &lt;- min(ii$width, ii$height)\n  \n  img &lt;- image_blank(width = ii_min, height = ii_min, color = \"none\")\n  drawing &lt;- image_draw(img)\n  symbols(ii_min/2, ii_min/2, circles = ii_min/2.2,\n          bg = \"white\", inches = FALSE, add = TRUE)\n  dev.off()\n  \n  x = image_composite(image_scale(drawing, \"x430\"),\n                      image_scale(im, \"x400\"), offset = \"+15+15\")\n  \n  x\n}\n\n### now let's add the player's name and wrap it around the circle\nplot_image_label&lt;-function(image,\n                           label,\n                           font_color=\"black\", \n                           top_bottom=\"top\",\n                           hjust = 0.5){\n  \n  t = seq(0, 1, length.out = 100) * pi\n  \n  ### set up data\n  if(top_bottom==\"top\"){data = data.frame(x = cos(t),y = sin(t))}\n  else if(top_bottom==\"bottom\"){data=data.frame(x = cos(t),y = sin(t)*-1)}\n  \n  ### set up data   ### the `vjust` option here changes how close name is\n  if(top_bottom==\"top\"){vjust=0.5}\n  else if(top_bottom==\"bottom\"){vjust=-0.1}\n  \n  ### set up data\n  if(top_bottom==\"top\"){ymax=1.2}\n  else if(top_bottom==\"bottom\"){ymax=0.9}\n  \n  ### set up data\n  if(top_bottom==\"top\"){ymin=-0.9}\n  else if(top_bottom==\"bottom\"){ymin=-1.2}\n  \n  ### now taking the text an adding it into a ggplot with the circle\n  ggplot() +\n    geom_image(aes(x=0, y=0, image = image), asp=2.4/2.1,\n               size=.7, image_fun=border) +\n    scale_x_continuous(limits = c(-1.2, 1.2))+\n    scale_y_continuous(limits=c(ymin, ymax))+\n    geom_textpath(data = data, aes(x,y,label = toupper(label)),\n                  linecolor=NA, color=font_color,\n                  size = 16,  fontface=\"bold\",\n                  vjust = vjust, hjust=hjust, family = \"Roboto\")+\n    coord_equal()+\n    theme_void()\n}\n\nAfter the process finishes running (it may take a while depending on your computing power), we create a new column titled new_image_path and use paste0 to copy the receiver_id for each player and add “.png” on the end.\n\ncrop_circles_data &lt;- crop_circles_data %&gt;%\n  mutate(new_image_path = paste0(receiver_id, \".png\"))\n\nWe then loop our crop_cirlces_data function back over the images and save them into a new images folder on our computer.\n\nfor(i in 1:nrow(crop_circles_data)) {\n  \n  pos = \"top\"\n  hjust = 0.5\n  path = crop_circles_data$new_image_path[i]\n  plot = plot_image_label(image = crop_circles_data$circle[i],\n                          label = crop_circles_data$receiver[i],\n                          font_color = \"black\",\n                          top_bottom = \"top\",\n                          hjust = hjust)\n  \n  ggsave(filename = glue(\"./images/circle_headshots/{path}\"),\n         plot=plot, height=3.95, width=4.5)\n}\n\n\n\n\n\n\n\nImportant\n\n\n\nIf RStudio happens to stop outputting ggplot() visualizations at this point, please run dev.off().\n\n\n\ncrop_circles_data &lt;- crop_circles_data %&gt;%\n  mutate(image = glue(\"./images/circle_headshots/{new_image_path}\"))\n\nWe can now take our new cropped headshots and bring them into a visualization created with ggplot().\n\nggplot(complete_wr_ay, aes(x = air_yards, fill = posteam)) +\n  geom_density(alpha = .4) +\n  nflplotR::scale_fill_nfl() +\n  geom_image(data = complete_wr_ay, aes(x = 40, y = 0.04,\n                                        image = image),\n             size = 0.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  facet_wrap(~ receiver_id) +\n  nfl_analytics_theme() +\n  theme(strip.background = element_blank(),\n        strip.text= element_blank()) +\n  xlab(\"Air Yards\") +\n  ylab(\"Density\") +\n  labs(title = \"**Top Ten WRs in Total Air Yards**\",\n       subtitle = \"*Density Plot of How They Were Earned*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nUsing magick to create circular headshots for plots"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-tables-with-gt-and-gtextras",
    "href": "04-nfl-analytics-visualization.html#creating-tables-with-gt-and-gtextras",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.16 Creating Tables with gt and gtExtras\n",
    "text": "4.16 Creating Tables with gt and gtExtras\n\nIn some instances, you may find that a plot produced by ggplot2 is not the best medium in which to display your data. In that case, you may want to turn to use the gt package (and the accompanying add-on from Thomas Mock, gtExtras). Much like ggplot2 stands for the “grammar of graphics”, the gt package is short for the “grammar of tables.” Taking a data frame into a gt table is as simple as piping into the gt() function, but the end result will likely not be visually pleasing. To make the necessary adjustments, you will need to begin making changes to the output by altering the core parts of a gt table: the table header, the stub and stub head, the column labels, the table body, and the table footer.\nTo provide a walk through of this process, let’s read in prepared data that examines the top ten average completion percentage over expected for those attempts in a pure passing situation (that is, plays that have a predictive probability of 70% or greater of being a passing attempt).\n\npure_cpoe &lt;- vroom(\"http://nfl-book.bradcongelio.com/pure-cpoe\")\n\nAs mentioned, we can quickly take the data and turn it into a gt table by piping it directly into the package’s core function. Please note that we are using the cols_hide() function to remove both the headshot and team_logo_wikipedia variables at first, as including them at this point in the design process results in long strings of URL data that make it difficult to grasp what the current table looks like.\n\npure_cpoe %&gt;%\n  gt() %&gt;%\n  cols_hide(c(headshot, team_logo_wikipedia))\n\n\n\n\n\n\nseason\npasser\ntotal_attempts\nmean_cpoe\n\n\n\n2013\nP.Rivers\n291\n10.893818\n\n\n2018\nP.Mahomes\n261\n9.293220\n\n\n2020\nA.Rodgers\n242\n8.573572\n\n\n2013\nR.Wilson\n231\n8.473162\n\n\n2018\nR.Wilson\n277\n8.448647\n\n\n2011\nD.Brees\n311\n8.285040\n\n\n2018\nM.Ryan\n315\n8.233131\n\n\n2012\nM.Ryan\n303\n8.108120\n\n\n2015\nR.Wilson\n279\n7.515386\n\n\n2021\nJ.Burrow\n343\n7.117319\n\n\n\n\nUsing the dplyr to pipe the data frame directly into the gt package\n\n\nThe output is not terrible, but it does not include the headshot and team logo information yet. The output is realistically just the data frame outputted into a different format. Let’s begin making incremental changes to the table by first adding a title and subtitle, using Markdown (md()) to provide bold and italicized styling to each.\n\npure_cpoe %&gt;%\n  gt() %&gt;%\n  cols_hide(c(headshot, team_logo_wikipedia)) %&gt;%\n  tab_header(\n    title = md(\"**Average CPOE in Pure Passing Situations**\"),\n    subtitle = md(\"*2010-2022  |  Pure Passing is &gt;= 70% Chance of Pass Attempt*\"))\n\n\n\n\n\n\n\nAverage CPOE in Pure Passing Situations\n    \n\n2010-2022  |  Pure Passing is &gt;= 70% Chance of Pass Attempt\n    \n\n\nseason\n      passer\n      total_attempts\n      mean_cpoe\n    \n\n\n2013\nP.Rivers\n291\n10.893818\n\n\n2018\nP.Mahomes\n261\n9.293220\n\n\n2020\nA.Rodgers\n242\n8.573572\n\n\n2013\nR.Wilson\n231\n8.473162\n\n\n2018\nR.Wilson\n277\n8.448647\n\n\n2011\nD.Brees\n311\n8.285040\n\n\n2018\nM.Ryan\n315\n8.233131\n\n\n2012\nM.Ryan\n303\n8.108120\n\n\n2015\nR.Wilson\n279\n7.515386\n\n\n2021\nJ.Burrow\n343\n7.117319\n\n\n\n\nAdding a title and subtitle to the Pure CPOE table\n\n\nAfter adding the title and subtitle to the top of the table, we can continue to remove the team_logo_wikipedia information using the cols_hide() function and then place the player pictures into the table using the gt_img_rows() function from the gtExtras package.\n\npure_cpoe %&gt;%\n  gt() %&gt;%\n  cols_hide(team_logo_wikipedia) %&gt;%\n  tab_header(\n    title = md(\"**Average CPOE in Pure Passing Situations**\"),\n    subtitle = md(\"*2010-2022  |  Pure Passing is &gt;= 70%\n                  Chance of Pass Attempt*\")) %&gt;%\n  gt_img_rows(headshot, height = 25)\n\n\n\n\n\nAdding player headshots to the table using gt_img_rows from gtExtras\n\n\n\nThat is considerably better. But it can be even better. For example, the column labels are still in the format from the pure_cpoe data frame with underscores and issues with capitalized letters. Let’s make these grammatical corrections using cols_label() . As well, let’s move the passer column to the far left and use the tab_stubhead() function, allowing us to position the names to the left of the column labels and with a vertical line separating them from the other data being presented. This also requires placing rowname_col = \"passer\" into the call to the gt() package.\n\npure_cpoe %&gt;%\n  gt(rowname_col = \"passer\") %&gt;%\n  cols_hide(team_logo_wikipedia) %&gt;%\n  tab_header(\n    title = md(\"**Average CPOE in Pure Passing Situations**\"),\n    subtitle = md(\"*2010-2022  |  Pure Passing is &gt;= 70%\n                  Chance of Pass Attempt*\")) %&gt;%\n  tab_stubhead(label = \"Quarterback\") %&gt;%\n  gt_img_rows(headshot, height = 25) %&gt;%\n  cols_label(\n    passer = \"Quarterback\",\n    headshot = \"\",\n    season = \"Season\",\n    total_attempts = \"Attempts\",\n    mean_cpoe = \"Mean CPOE\")\n\n\n\n\n\n\nWe are almost there. But, it looks strange to have the player’s picture so far removed from their name. Let’s place the headshot column as the first column and, while we are at it, use the cols_align() function to center align the passer and total_attempts columns.\n\npure_cpoe %&gt;%\n  gt(rowname_col = \"passer\") %&gt;%\n  cols_hide(team_logo_wikipedia) %&gt;%\n  tab_header(\n    title = md(\"**Average CPOE in Pure Passing Situations**\"),\n    subtitle = md(\"*2010-2022  |  Pure Passing is &gt;= 70%\n                  Chance of Pass Attempt*\")) %&gt;%\n  tab_stubhead(label = \"Quarterback\") %&gt;%\n  gtExtras::gt_img_rows(headshot, height = 25) %&gt;%\n  cols_label(\n    passer = \"Quarterback\",\n    headshot = \"\",\n    season = \"Season\",\n    total_attempts = \"Attempts\",\n    mean_cpoe = \"Mean CPOE\") %&gt;%\n  cols_move_to_start(\n    columns = headshot) %&gt;%\n  cols_align(align = \"center\", columns = c(\"passer\", \"total_attempts\"))\n\n\n\n\n\n\nAs one last visual enhancement, we will use the gt_color_box() function from gtExtras to replace the values in mean_cpoe with colorboxes that are similar to those used in data visualizations on Pro Football Focus.\n\npure_cpoe %&gt;%\n  gt(rowname_col = \"passer\") %&gt;%\n  cols_hide(team_logo_wikipedia) %&gt;%\n  tab_header(\n    title = md(\"**Average CPOE in Pure Passing Situations**\"),\n    subtitle = md(\"*2010-2022  |  Pure Passing is &gt;= 70%\n                  Chance of Pass Attempt*\")) %&gt;%\n  tab_stubhead(label = \"Quarterback\") %&gt;%\n  gt_img_rows(headshot, height = 25) %&gt;%\n  cols_label(\n    passer = \"Quarterback\",\n    headshot = \"\",\n    season = \"Season\",\n    total_attempts = \"Attempts\",\n    mean_cpoe = \"Mean CPOE\") %&gt;%\n  cols_move_to_start(\n    columns = headshot) %&gt;%\n  cols_align(align = \"center\", columns = c(\"passer\", \"total_attempts\")) %&gt;%\n  gt_color_box(mean_cpoe, domain = 7:11,\n               palette = \"ggsci::blue_material\", accuracy = 0.01)\n\n\n\n\n\n\nIt is important to include the accuracy = 0.01 within the gt_color_box(), otherwise the output of the table includes just whole numbers. At this point, the table is complete all for adding a caption at the bottom and using one of the formatted themes from gtExtras (I am partial to gt_theme_538).\n\npure_cpoe %&gt;%\n  gt(rowname_col = \"passer\") %&gt;%\n  cols_hide(team_logo_wikipedia) %&gt;%\n  tab_header(\n    title = md(\"**Average CPOE in Pure Passing Situations**\"),\n    subtitle = md(\"*2010-2022  |  Pure Passing is &gt;= 70%\n                  Chance of Pass Attempt*\")) %&gt;%\n  tab_stubhead(label = \"Quarterback\") %&gt;%\n  gt_img_rows(headshot, height = 25) %&gt;%\n  cols_label(\n    passer = \"Quarterback\",\n    headshot = \"\",\n    season = \"Season\",\n    total_attempts = \"Attempts\",\n    mean_cpoe = \"Mean CPOE\") %&gt;%\n  cols_move_to_start(\n    columns = headshot) %&gt;%\n  cols_align(align = \"center\", columns = c(\"passer\", \"total_attempts\")) %&gt;%\n  gt_color_box(mean_cpoe, domain = 7:11,\n               palette = \"ggsci::blue_material\", accuracy = 0.01) %&gt;%\n  tab_source_note(\n    source_note = md(\"*An Introduction to NFL Analytics with R*&lt;br&gt;\n                     **Brad J. Congelio**\")) %&gt;%\n  gtExtras::gt_theme_538()"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#creating-a-shiny-app",
    "href": "04-nfl-analytics-visualization.html#creating-a-shiny-app",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.17 Creating a Shiny App",
    "text": "4.17 Creating a Shiny App\nWith a solid understanding of the data visualization process, we can now turn our attention to take a completed data frame and transitioning it into a Shiny App, resulting in an interactive application for users to explore your data and findings.\nTo do this, we will be utilize a data frame that you will see again in Chapter 5 that is created during an XGBoost modeling process. It contains the actual rushing yards and expected rushing yards, based on predictive machine learning, for all running backs from 2016 to 2022. You can read in the data to view it with the following vroom code.\n\nryoe_shiny &lt;- vroom(\"http://nfl-book.bradcongelio.com/ryoe-projs\")\n\nThe resulting ryoe_shiny data frame contains information regarding actual_yards, season, down, ydstogo, red_zone, rusher, rusher_player_id, posteam, defteam, exp_yards. As you will soon see, despite being in the data frame, will will not use all of the variables while also adding some in the backend of the Shiny App. To that end, the Shiny App we will be producing will allow users to select one or more seasons to see each running back’s total rushing yards, total expected yards, and the difference between the two (where a positive number indicates the running back gained more yards than the XGBoost model predicted). Users will be able to select specific downs and yards to go, as well as indicate if they want to see the information pertaining to only those rushing attempts in the red zone. We will also build in the capability for the Shiny App to automatically update and format a ggplot scatterplot to visualize the information.\nYou can see the end result and the functioning Shiny App here: NFL Analytics with R: RYOE Shiny App.\n\n4.17.1 Requirements for a Shiny App\nFirst, it is important to know that a Shiny App cannot be built within a typical R script. Rather, you must select “Shiny Web App” from File -&gt; New File inside RStudio. You will then be prompted to indicate the folder in which you would like to save the files and whether or not you want a combined file or a separate ui.R and server.R. A single file containing both is acceptable for a simple Shiny App but more complex designs will require separate files to maintain organization.\n\n\nUser Interface (ui.R) - the ui.R file controls the layout and appearance of your Shiny App, including controlling all the elements of the user interface such as text, inputs, sliders, checkboxes, tables, and plots. The most basic requirements to construct a user interface are fluidpage(), sidebarLayout(), sidebarPanel(), and mainPanel(). The sidebarPanel() is what contains the options for user input, while mainPanel() is where the results of the input display in the web app.\n\nServer Function (server.R) - the server.R file contains the information needed to build and modify the outputs in response to user input, as well as where you construct the inner-workings of the Shiny App (like data manipulation). Each server.R file will contain a function(input, output) where the input is an object that contains the requests from the user input and output is the required items to be rendered.\n\nBoth the ui.R and server.R are ultimately passed into the shinyApp() function that outputs the final product.\n\n4.17.2 Building our ui.R File\nBoth the ui.R and server.R files will start with similar information: reading in the data, any necessary preparation, and - in our case - including the custom nfl_analytics_theme() to be passed into the ggplot output.\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(vroom)\n\nryoe_shiny &lt;- vroom(\"http://nfl-book.bradcongelio.com/ryoe-projs\")\n\nryoe_shiny &lt;- ryoe_shiny %&gt;%\n  filter(ydstogo &lt;= 10)\n\nteams &lt;- nflreadr::load_teams(current = TRUE) %&gt;%\n  select(team_abbr, team_logo_wikipedia, team_color, team_color2)\n\nryoe_shiny &lt;- ryoe_shiny %&gt;%\n  left_join(teams, by = c(\"posteam\" = \"team_abbr\"))\n\nnfl_analytics_theme &lt;- function(..., base_size = 12) {\n  \n  theme(\n    text = element_text(family = \"Roboto\",\n                        size = base_size,\n                        color = \"black\"),\n    axis.ticks = element_blank(),\n    axis.title = element_text(face = \"bold\"),\n    axis.text = element_text(face = \"bold\"),\n    plot.title.position = \"plot\",\n    plot.title = element_markdown(size = 16,\n                                  vjust = .02,\n                                  hjust = 0.5),\n    plot.subtitle = element_markdown(hjust = 0.5),\n    plot.caption = element_markdown(size = 8),\n    panel.grid.minor = element_blank(),\n    panel.grid.major =  element_line(color = \"#d0d0d0\"),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    plot.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_blank(),\n    legend.background = element_rect(color = \"#F7F7F7\"),\n    legend.key = element_rect(color = \"#F7F7F7\"),\n    legend.title = element_text(face = \"bold\"),\n    legend.title.align = 0.5)\n}\n\nAfter gathering the data into a data frame called ryoe_shiny, we use filter() to limited the rushes plays to just those with 10 or less yards to go, and then use the load_teams() function from nflreadr to merge in team_color, team_color2, and team_logo_wikipedia. After pasting in our existing code for the custom theme, we can construct the user interface.\n\nui &lt;- fluidPage(\n  titlePanel(\"NFL Analytics with R: RYOE Shiny App\"),\n  sidebarLayout(fluid = TRUE,\n    sidebarPanel(\n      selectInput(\"season\", \"Season:\",\n                  choices = unique(ryoe_shiny$season),\n                  selected = NULL, multiple = TRUE),\n      sliderInput(\"down\", \"Down:\",\n                  min = min(ryoe_shiny$down),\n                  max = max(ryoe_shiny$down),\n                  value = range(ryoe_shiny$down)),\n      sliderInput(\"ydstogo\", \"Yards to Go:\",\n                  min = min(ryoe_shiny$ydstogo),\n                  max = 10,\n                  value = range(ryoe_shiny$ydstogo)),\n      checkboxInput(\"red_zone\", \"Red Zone:\", value = FALSE)),\n    mainPanel(\n      tableOutput(\"myTable\"),\n      plotOutput(\"myPlot\", width = \"500px\"),\n      width = 6)))\n\nWe begin constructing the user interface by calling in the fluidPage() function. Each “fluid page” in a Shiny application consists of rows, which are created either either with fluidRow()/column() function or, in our case, by using sidebarLayout(). Within sidebarLayout(), we can provide the arguments for both sidebarPanel(), which is the column that contains the user input controls, and mainPanel() which shows the requested outputs.\nWithin the sidebarPanel(), we begin inputting the options that users will be able to select. There are several different options in which you can allow users manipulate the data, including text inputs, selection inputs, check boxes, radio buttons, date inputs, and even file uploads. For the purposes of our application, we are providing four different options for input using four different options of input type:\n\nusers can select a specific season/seasons using selectInput(). The first season in the input indicates the specific column name from the data to use, while the uppercase Season: is the title of the input that displays on the application. Within choices, we use the unique() function to pass in the individual season values in the data (2016, 2017, 2018, 2019, 2020, 2021, and 2022). While the selected argument permits “pre-filling” the input with a selection, we will leave it at NULL and then set multiple to TRUE to allow for users to select more than one season at once.\nEach sliderInput() allows the user to select from a predetermined set values: either 1-4 for down and 1-10 for yards to go. The min and max arguments are set to 1st and 4th down and 1 yard to go and 10 yards to go, respectively. For value, we are supplying all the numeric values in the data so that the user sees each number on the input bar.\nFinally, the checkboxInput is permits users to either select/deselect the Red Zone argument to view the data when the offense is either in or outside of the opposing red zone.\n\nThe mainPanel() options takes information that is constructed within the server.R file (specifically, in this case, a table with results and the including scatterplot of the data) and displays it beside the user input area. We are also instructing the plot is to have a width of 500px with the totality of the mainPanel() having, itself, a width of 6.\n\n4.17.3 Building our server.R File\nThe server.R file begins with three different arguments.\n\nserver &lt;- function(input, output) {\n  \n  selectedSeasons &lt;- reactive({\n    paste(input$season, collapse = \", \")\n  })\n  \n  filteredData &lt;- reactive({\n    ryoe_shiny %&gt;%\n      filter(\n        season %in% input$season,\n        down &gt;= input$down[1] & down &lt;= input$down[2],\n        ydstogo &gt;= input$ydstogo[1] & ydstogo &lt;= input$ydstogo[2],\n        red_zone == input$red_zone\n      )\n  })\n\nEach server.R file will begin with function(input, output) with the specifics of each application coming after. After, the first object created in the Shiny App is selectedSeasons which stores the season(s) that the user selects so that the values can be placed in the resulting ggplot data visualization. After, we take our ryoe_shiny data frame and wrap it inside the reactive() function and write it into a data frame called filteredData, thus causing filteredData to become what is called a “reactive expression.” Reactive expressions are what give Shiny the ability to update outputs (like tables and plots, in our case) in response to user input.\nThe information after the reactive() function is the data that is to be updated in real time. For our Shiny App, we are using the filter() function to keep only the criteria from ryoe_shiny that the user request. That is, we are filtering for the season in the data frame that includes (%in%) the season(s) selected on the user interface. The filtering of down is done by instructing that the first down must be greater than or equal to the user input while the second down must be less than or equal to. The [1] and [2] including in both down and ydstogo are indices that specify the first and second elements of the value. For example, if the user selects a range of down from first to third, input$down[1] would be 1 and input$down[2] would be 3 (with the equivalent tidyverse version of it being filter(down &gt;= 1 & down &lt;= 3). Last, if the box is checked for red_zone, it will filter the data to include only those plays that include the binary 1 in the red_zone variable.\nThe data, now filtered based on the user input, can be passed into the output table and plot.\n\n  output$myTable &lt;- renderTable({\n    resultTable &lt;- filteredData() %&gt;%\n      group_by(rusher) %&gt;%\n      summarise(\n        total_rushes = as.integer(n()),\n        total_actual_yards = as.integer(sum(actual_yards)),  \n        total_expected_yards = as.integer(sum(exp_yards)),       \n        difference = as.integer(sum(actual_yards) - sum(exp_yards)) \n      ) %&gt;%\n      rename(\n        \"Rusher\" = rusher,\n        \"Total Rushes\" = total_rushes,\n        \"Total Actual Yards\" = total_actual_yards,\n        \"Total Expected Yards\" = total_expected_yards,\n        \"Difference\" = difference\n      ) %&gt;%\n      arrange(desc(Difference)) %&gt;%\n      head(10) %&gt;%\n      as.data.frame()\n    \n        resultTable\n  })\n\nTo create the output table, we use the renderTable() function, another reactive element within the Shiny App. We write the information contained in filteredData() (which was created above during the reactive() filtering process) to a new data frame called resultTable. After, we handle the data just like we would within the tidyverse outside of Shiny, aside from ensuring the information is written out in data frame form by including as.data.frame() at the end. All of this is ultimately written into the output$myTable to be passed along to the user interface. As highlighted in the below code, outputting the ggplot is done in the same fashion, except for the use of renderPlot(). After, we create the data frame called top10 that is also built off of the filtered data contained in filteredData(). After, the process includes the basic tenants of group_by(), summarize(), and building out the visualization in ggplot().\n\n  output$myPlot &lt;- renderPlot({\n    top10 &lt;- filteredData() %&gt;%\n      group_by(rusher) %&gt;%\n      summarize(\n        team_color = last(team_color),\n        total_actual_yards = as.integer(sum(actual_yards)),  \n        total_expected_yards = as.integer(sum(exp_yards))      \n      ) %&gt;%\n      top_n(10, wt = total_actual_yards)\n    \n    ggplot(data = top10, aes(x = total_actual_yards,\n                             y = total_expected_yards)) +\n      stat_poly_line(method = \"lm\", se = FALSE,\n                     linetype = \"dashed\",\n                     color = \"black\") +\n      stat_poly_eq(mapping = use_label(c(\"R2\", \"P\")),\n                   p.digits = 2,\n                   label.x = .20,\n                   label.y = 3) +\n      geom_point(color = top10$team_color, size = 3.5) +\n      geom_text_repel(data = top10, aes(label = rusher)) +\n      labs(\n        x = \"Total Actual Yards\", \n        y = \"Total Expected Yards\", \n        title = paste(\"Actual Rushing Yards vs. Expected for Season(s):\",\n                      selectedSeasons()),\n        subtitle = \"Model: *LightGBM* in **tidymodels**\") +\n      nfl_analytics_theme()\n  }, width = 500)\n}\n\nAs you build the Shiny app, you can check the validity of your coding by clicking the Run App button in the upper-right part of the coding screen. If everything is correct, the application will run within your local system. If there are issues, the information will be provided in the ‘Console’ area of RStudio. Once you are content with functioning and style of the application, you can publish it and host it, for free, on ShinyApps.io."
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#exercises",
    "href": "04-nfl-analytics-visualization.html#exercises",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "\n4.18 Exercises",
    "text": "4.18 Exercises\nThe answers for the following answers can be found here: http://nfl-book.bradcongelio.com/ch4-answers.\n\n4.18.1 Exercise 1\nTo begin, run the following code to create the data frame titled rb_third_down:\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\nrb_third_down &lt;- pbp %&gt;%\n  filter(play_type == \"run\" & !is.na(rusher)) %&gt;%\n  filter(down == 3 & ydstogo &lt;= 10) %&gt;%\n  group_by(rusher, rusher_player_id) %&gt;%\n  summarize(\n    total = n(),\n    total_first = sum(yards_gained &gt;= ydstogo, na.rm = TRUE),\n    total_pct = total_first / total) %&gt;%\n  filter(total &gt;= 20) %&gt;%\n  arrange(-total_pct)\n\nAfter, do the following:\n\nCreate a geom_col plot\nPlace rusher_player_id on the x-axis and total_pct on the y-axis\nReorder rusher in descending order by total_pct\n\nOn the y-axis, set the breaks and labels use the scales:: option (note: the y-axis should be in percent format)\nAgain on the y-axis, use expand = c(0,0) to remove the space of the bottom of the plot\nUse xlab() and ylab() to add a title to only the y-axis\nUse the nflplotR package, add the primary team color as the column fill and secondary as color\n\nUse the nflplotR package, edit the axis.text.x within the theme() option to add the player headshots\nAdd theme_minimal() to the plot\nUse labs() to include a title and subtitle\n\n4.18.2 Exercise 2\nRun the code below to create the offensive_epa data frame, then follow the instructions below to complete the exercise.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\noffensive_epa &lt;- pbp %&gt;%\n  filter(!is.na(posteam) & !is.na(down)) %&gt;%\n  group_by(posteam) %&gt;%\n  summarize(mean_epa = mean(epa, na.rm = TRUE))\n\n\nPlace mean_epa on the x-axis and posteam on the y-axis, using reordering the teams by mean_epa\n\nUse geom_col with the posteam set as both the color and fill and add a width of 0.75\nUse nflplotR to set the color and fill to “secondary” and “primary”\nAdd a geom_vline with an xintercept of 0, add the color as “black” and a linewidth of 1\nUse nflplotR to add team logos to the end of each bar\nUse theme_minimal()\n\nRemove the y-axis text with axis.text.y within theme()\n\nUse xlab, ylab, and labs to add the x-axis label and the title/subtitle\n\n\n\n\n\nStikeleather, J. (2013). The three elements of successful data visualizations. Retrieved from https://hbr.org/2013/04/the-three-elements-of-successf"
  },
  {
    "objectID": "04-nfl-analytics-visualization.html#footnotes",
    "href": "04-nfl-analytics-visualization.html#footnotes",
    "title": "\n4  Data Visualization with NFL Data\n",
    "section": "",
    "text": "As listed on the ggrepel website: https://ggrepel.slowkow.com/articles/examples.html↩︎"
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#introduction-to-statistical-modeling-with-nfl-data",
    "href": "05-nfl-analytics-advanced-methods.html#introduction-to-statistical-modeling-with-nfl-data",
    "title": "\n5  Advanced Model Creation with NFL Data\n",
    "section": "\n5.1 Introduction to Statistical Modeling with NFL Data",
    "text": "5.1 Introduction to Statistical Modeling with NFL Data\nThe process of conducting statistical modeling with NFL data is difficult to cover in a single chapter of a book, as the topic realistically deserves an entire book as the field of modeling and machine learning is vast and complex, and continues to grow as a result of access to new data and the releases of packages (as of November 2020, over 16,000 packages are available in the R ecosystem).(Wikipedia, 2023) The list of options for conducting statistical analysis and machine learning is vast, with the more widely-used ones including caret (combined functions for creating predictive models), mgcv (for generalized additive models), lme4 (for linear mixed effects models), randomForest (for creating random forests models), torch (for image recognition, time series forecasting, and audio processing), and glmnet (for lasso and elastic-net regression methods). A newer addition to the modeling and machine learning ecosystem is tidymodels, which is a collection of various packages that operate cohesively using tidyverse principles.\nWhile it is not possible for this chapter to cover every aspect of modeling and machine learning, its goal is to provide the foundation in both to provide you with the necessary skills, understanding, and knowledge to independently explore and analyze NFL data using different methodologies and tools. The techniques covered in this chapter, from creating a simple linear regression to a predictive XGBoost model, are building blocks to all further analysis and machine learning you will conduct. In doing so, you will learn how to use built-in options such as stats to more technical packages such as tidymodels.\nTo that end, by mastering these introductory concepts and techniques, you not only learn the process and coding needed to create models or do machine learning, but you will begin developing an analytical mindset which will allow you to understand why one statistical approach is not correct for your data and which one is, how to interpret the results of your model, and - when necessary - how to correct issues and problems in the process as they arise. The fundamentals provided in this chapter, I believe, serve as a actionable stepping stone for a further journey into the modeling and machine learning in the R programming language, with or without the use of NFL data."
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#regression-models-with-nfl-data",
    "href": "05-nfl-analytics-advanced-methods.html#regression-models-with-nfl-data",
    "title": "\n5  Advanced Model Creation with NFL Data\n",
    "section": "\n5.2 Regression Models with NFL Data",
    "text": "5.2 Regression Models with NFL Data\nRegression models are the bedrock of many more advanced machine learning techniques. For example, the “decision tree” process that is conducted in random forests models and XGBoost are created based on the split in variable relationships and neural networks are, at their core, just a intricate web of regression-like calculations. Because of this, regression models are an outstanding starting point for not only learning the basics of modeling and machine learning in R, but an important step in comprehending more advanced modeling techniques.\nAs you will see, a regression model is simply a mathematical determination of the relationship between two or more variables (the response and dependent variables). In the context of the NFL, a team’s average points per game would be a response variable while those metrics that cause the average points per game are the predictor variables (average passing and rushing yards, turnovers, defensive yards allowed, etc.). The results of regression models allow us to determine which predictor variables have the largest impact on the response variable. Are passing yards a more predictive variable to average points per game than rushing yards? A regression model is capable of answering that question.\nThis section starts with exploring simple linear regression and then extends this knowledge to creating both multiple linear and logistic regression models which deal, respectively, with multiple predictors and categorical dependent variables. After, we will focus on creating binary and multinomial regression models.\n\n5.2.1 Simple Linear Regression\nA linear regression is a fundamental statistical technique that is used to explore the relationship between two variables - the dependent variable (also called the “response variable”) and the independent variables (also called the “predictor variables”). By using a simple linear regression, we can model the relationship between the two variables as a linear equation that best fits the observed data points.\nA simple linear regression aims to fit a straight line through all the observed data points in such a way that the total squared distance between the actual observations and the values predicted by the model are minimal. This line is often referred to as either the “line of best fit” or the “regression line” and it represents the interaction between the dependent and independent variables. Mathematically, the equation for a simple linear regression is as follows:\n\\[\nY = {\\beta}_0 + {\\beta}_1  X + \\epsilon\n\\]\n\n\n\\(Y\\), in the above equation, is the dependent variable where the \\(X\\) represents the independent variable.\n\n\\({\\beta}_o\\) is the intercept of the regression model.\n\n\\({\\beta}_1\\) is the slope of the model’s “line of best fit.”\n\n\\(\\epsilon\\) represents the error term.\n\nTo better illustrate this, let’s use basic football terms using the above regression equation to compare a team’s offensive points scored in a season based on how many offensive yards it accumulated. The intercept (\\({\\beta}_o\\)) represents the value when a team’s points scored and offensive yards are both zero. The slope (\\({\\beta}_1\\)) represents the rate of change in \\(Y\\) as the unit of \\(X\\) changes. The error term (\\(\\epsilon\\)) is represents the difference between the actual observed values of the regression’s dependent variable and the value as predicted by the model.\nUsing our points scored/total yards example, a team’s total yards gained is the independent variable and total points scored is the dependent variable, as a team’s total yardage is what drives the change in total points (in other words, a team’s total points is dependent on its total yardage). A team will not score points in the NFL if it is not also gaining yardage. We can view this relationship by building a simple linear regression model in R using the lm() function.\n\n\n\n\n\n\nNote\n\n\n\nThe lm() function is a built-in RStudio tool as part of the stats package and stand for “linear model.” It is used, as described above, to fit a linear regression to the data that you provide. The completed regression estimates the coefficients of the data and also includes both the intercept and slope, which are the main factors in explaining the relationship between the response and predictor variables.\nThe lm() function requires just two argument in order to provide results: the formula and the data frame to use:\nmodel_results &lt;- lm(formula, data)\nThe formula argument requires that you specify both the response and predictor variables, as named in your data frame, in the structure of y ~ x wherein y is the response variable and x is the predictor. In the case that you have more than one predictor variable, the + is used to add to the formula:\nmodel_results &lt;- lm(y ~ x1 + x2 + x3, data)\nThe returned coefficients, residuals, and other statistical results of the model are returned into your RStudio environment in a lm data object. There are several ways to access this data and are discussed below in further detail.\n\n\nTo put theory into action, let’s build a simple linear regression model that explores the relationship between the total yardage earned by a team over the course of a season and the number of points scored. To begin, gather the prepared information into a data frame titled simple_regression_data by running the code below.\n\nsimple_regression_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/simple-reg\")\n\nThe data contains the total yardage and points scored for each NFL team between the 2012 and 2022 seasons (not including the playoffs). Before running our first linear regression, let’s first begin by selecting just the 2022 data and create a basic visualization to examine the baseline relationship between the two variables.\n\nregression_2022 &lt;- simple_regression_data %&gt;%\n  filter(season == 2022)\n\nteams &lt;- nflreadr::load_teams(current = TRUE)\n\nregression_2022 &lt;- regression_2022 %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\nggplot(regression_2022, aes(x = total_yards, y = total_points)) +\n  geom_smooth(method = \"lm\", se = FALSE,\n              color = \"black\",\n              linetype = \"dashed\",\n              size = .8) +\n  geom_image(aes(image = team_logo_wikipedia), asp = 16/9) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  labs(title = \"**Line of Best Fit: 2022 Season**\",\n       subtitle = \"*Y = total_yards ~ total_points*\") +\n  xlab(\"Total Yards\") +\n  ylab(\"Total Points\") +\n  nfl_analytics_theme()\n\n\n\nAn example regression between total yards and total points\n\n\n\nThe plot shows that a regression between total_yards and total_points results in several teams - the Titans, Giants, Packers, Raiders, Jaguars, and Chiefs - being fitted nearly perfectly with the line of best fit. These teams scored points based on total yards in a linear fashion. The Cowboys, however, are well above the regression line. This indicates that Dallas scored more total points than what the relationship between total_yards and total_points found as “normal” for a team that earned just a hair over 6,000 total yards. The opposite is true for the Colts, Jets, and Denver. In each case, the total_points scored is below what is expected for teams that gained approximately 5,500 total yards.\nThe line of best fit can explain this relationship in slightly more detail. For example, the total_yards value of 5,500 cross the regression line just below the total_points value of 350. This means that a team that gains a total of 5,500 yards should - based on this fit - score just under 350 points during the season. Viewing it the other way, if you want your team to score 450 points during the upcoming season, you will need the offensive unit to gain roughly 6,500 total yards.\nTo further examine this relationship, we can pass the data into a simple linear regression model to start exploring the summary statistics.\n\nresults_2022 &lt;- lm(total_points ~ total_yards,\n                   data = regression_2022)\n\nUsing the lm() function, the \\(Y\\) variable (the dependent) is total_yards and the \\(X\\) variable (the predictor) is entered as total_yards with the argument that the data is coming from the regression_2022 data frame. We can view the results of the regression model by using the summary() function.\n\nsummary(results_2022)\n\n\nCall:\nlm(formula = total_points ~ total_yards, data = regression_2022)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-71.44 -22.33   1.16  19.15  68.08 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -225.0352    65.4493   -3.44   0.0017 ** \ntotal_yards    0.1034     0.0113    9.14  3.6e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 32 on 30 degrees of freedom\nMultiple R-squared:  0.736, Adjusted R-squared:  0.727 \nF-statistic: 83.5 on 1 and 30 DF,  p-value: 3.6e-10\n\n\n\n\n\n\n\n\nReading & Understanding Regression Results\n\n\n\nYou have now run and output the summary statistics for your first linear regression model that explore the relationship between an NFL team’s total yards and total points over the course of the 2022 season.\nBut what do they mean?\nThe summary() output of any regression models contains two core components: the residuals and the coefficients.\nResiduals\nA model’s residuals are the calculated difference between the actual values of the predictor variables as found in the data and the values predicted by the regression model. In a perfect uniform relationship, all of the values in the data frame would sit perfectly on the line of best fit. Take the below graph, for example.\n\nexample_fit &lt;- tibble(\n  x = 1:10,\n  y = 2 * x + 3)\n\nexample_perfect_fit &lt;- lm(y ~ x, data = example_fit)\n\nggplot(example_perfect_fit, aes(x = x, y = y)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\",\n              size = .8, linetype = \"dashed\") +\n  geom_image(image = \"./images/football-tip.png\", asp = 16/9) +\n  scale_x_continuous(breaks = scales::pretty_breaks()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  labs(title = \"A Perfect Regression Model\",\n       subtitle = \"Every Point Falls on the Line\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"Our Predictor Variable\") +\n  ylab(\"Our Response Variable\") +\n  nfl_analytics_theme()\n\n\n\nAn example of a perfect line of best fit\n\n\n\nIn this example, the regression model was able to successfully “capture” the entirety of the relationship between the predictor variable on the x-axis and the response variable on the y-axis. This means that the model leaves no unexplained or undetermined variance between the variables. As a result, we could provide the model new, unseen data and the result would predict - with 100% accuracy - the resulting values of the response variable.\nHowever, it is rare to have real-world data be perfectly situated on the line of best fit. In fact, it is more often than not a sign of “overfitting,” which occurs when the model successfully discovers the “random noise” in the data. In such cases, a model with a “perfect line of fit” will perform incredibly poorly when introduced to unseen data.\nA regression model that is not overfitted will have data points that do not fall on the line of best fit, but fall over and under it. The results of the regression model uses a simple formula to help us interpret the difference between those actual and estimated values:\nresiduals = observed_value - predicted_value\nInformation about these residual values are included first in our summary(results_2022) output and provide insight into the distribution of the model’s residual errors.\n\n\n\n\n\n\n\nSummary Distribution\nMeaning\n\n\nThe Min Distribution\nThe Min distribution provides the smallest difference between the actual values of the model’s predictor variable (total points) and the predicted. In the example summary, the minimum residual is -71.443 which means that the lm() model predicted that one specific team scored 71.443 more points than it actually did.\n\n\n\nThe 1Q Distribution\nThe 1Q distribution is based on the first quartile of the data (or where the first 25% of the model’s residual fall on the line of best fit). The 1Q residual is -22.334, which means the lm() model predicted that 25% of the teams scored 22.334 more points than the actual values.\n\n\n\nThe Median Distribution\nThe Median distribution is the residuals from the 50th percentile of the data. The Median residual in the above summary is 1.157, which means that the lm() model - for 50% of teams - either overestimated or underestimated a teams total points by less than 1.157 points.\n\n\n\nThe 3Q Distribution\nCovering the third quartile of the residuals, the 3Q Distribution is 19.145 which means that 75% of the NFL teams in the data had a total points prediction either overestimated or underestimated by less than 19.145 points.\n\n\n\nThe Max Distribution\nThe opposite of the Min distribution, the Max distribution is the largest difference between the model’s observed and predicted values for a team’s total points. In this case, for one specific team, the model predicted the team scored 68.080 points less than the actual value.\n\n\n\n\nA model’s residuals allow you to quickly get a broad view of accurately it is predicting the data. Ideally, a well-performing model will return residuals that are small and distributed evenly around zero. In such cases, it is good first sign that the predictions are close to the actual value in the data and the model is not producing over or under estimates.\nBut that is not always the case.\nFor example, we can compare our above example residuals to the residuals produced by manually created data.\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -13.70   -2.18   -0.60    0.00    4.16   10.54 \n\n\nCompared to the residuals from the above NFL data, the residuals from the randomly created data are small in comparison and are more evenly distributed around zero. Given that, it is likely that the linear model is doing a good job at making predictions that are close to the actual value.\nBut that does not mean the residuals from the NFL data indicate a flawed and unreliable model.\nIt needs to be noted that the “goodness” of any specific linear regression model is wholly dependent on both the context and the specific problem that the model is attempting to predict. To that end, it is also a matter of trusting your subject matter expertise on matters regarding the NFL.\nThere could be any number of reasons that can explain why the residuals from the regression model are large and not evenly distributed from zero. For example:\n\nRed-zone efficiency: a team that moves the ball downfield with ease, but then struggles to score points once inside the 20-yardline , will accumulate total_yards but fail to produce total_points in the way the model predicts.\nTurnovers: Similar to above, a team may rack up total_yards but ultimately continue to turn the ball over prior to being able to score.\nDefensive scoring: a score by a team’s defense, in this model, still counts towards total_points but does not count towards total_yards.\nStrength of Opponent: At the end of the 2022 season, the Philadelphia Eagles and the New York Jets both allowed just 4.8 yards per play. The model’s predicted values of the other three teams in each respective division (NFC East and AFC East) could be incorrect because such contextual information is not included in the model.\n\nAll that to say: residuals are a first glance at the results of the data and provide a broad generalization of how the model performed without taking outside contextual factors into consideration.\nCoefficients\nThe coefficients of the model are the weights assigned to each predictor variable and provide insight into the relationship between the various predictor and response variables, with the provided table outlining the statistical metrics.\n\n\nEstimate - representing the actual coefficient of the model, these numbers are the mathematical relationship between the predictor and response variables. In our case, the estimate for total_yards is 0.1034 which means that for each additional one yard gained we can expected a team’s total_points to increase by approximately 0.1034.\n\nStd. Error - the standard error is the numeric value for the level of uncertainty in the model’s estimate. In general, a lower standard error indicates a more reliable estimate. In other words, if we were to resample the data and then run the regression model again, we could expect the total_yards coefficient to vary by approximately 0.0113, on average, for each re-fitting of the model.\n\nt value - The t-statistic value is the measure of how many standard deviations the estimate is from 0. A larger t-value indicates that it is less likely that the model’s coefficient is not equal to 0 by chance.\n\nPr(&gt;|t|) - This value is the p-value associated with the hypothesis test for the coefficient. The level of significance for a statistical model is typically 0.05, meaning a value less than this results in a rejection of the null hypothesis and the conclusion that the predictor does have a statistically significant relationship with the response variable. In the case of our regression model, a p-value of 0.00000000036 indicates a highly significant relationship.\n\n\n\nReturning to the summary statistics of our analysis of the 2022 season, the residuals have a wide spread and an inconsistent deviation from zero. While the median residual value is the closest to zero at 1.157, it is still a bit too high to safely conclude that the model is making predictions that adequately reflect the actual values. Moreover, both tail ends of the residual values (Min and Max) are a large negative and positive number, respectively, which is a possible indication that the regression model is both over- and underestimating a team’s total_points by statistically significant amount.\nHowever, as mentioned, this widespread deviation from zero is likely the result of numerous factors outside the model’s purview that occur in any one NFL game. To get a better idea of what the residual values represent, we can plot the data and include NFL team logos.\n\nregression_2022$residuals &lt;- residuals(results_2022)\n\nggplot(regression_2022, aes(x = total_yards, y = residuals)) +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = .7) +\n  stat_fit_residuals(size = 0.01) +\n  stat_fit_deviations(size = 1.75, color = regression_2022$team_color) +\n  geom_image(aes(image = team_logo_wikipedia), asp = 16/9, size = .0325) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +\n  labs(title = \"**Total Yards & Residual Values**\",\n       subtitle = \"*Y = total_points ~ total_yards*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"Total Yards\") +\n  ylab(\"Residual of Total Points\") +\n  nfl_analytics_theme() +\n  theme(panel.grid.minor.y = element_line(color = \"#d0d0d0\"))\n\n\n\nPlotting a simple linear regression’s residual values\n\n\n\nWith the data visualized, it is clear that the model’s Min distribution of -71.44 is associated with the Tampa Bay Buccaneers, while the Max distribution of 68.08 is the prediction for the total points earned by the Dallas Cowboys. Because a negative residual means that the model’s predicted value is too high, and a positive residual means it was too low, we can conclude that the Buccaneers actually scored 71.4 points less than the the results of the model, while the Cowboys scored 68.08 more than predicted.\n\n`Coefficients:\n             Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) -225.0352    65.4493   -3.44        0.0017 ** \ntotal_yards    0.1034     0.0113    9.14 0.00000000036 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1`˜.â€™ 0.1 â€˜ â€™ 1`\n\nThe (Intercept) of the model, or where the regression line crosses the y-axis, is -225.0350. When working with NFL data, it of course does not make sense that the (Intercept) is negative. Given the model is built on a team’s total yards and total points, it seems intuitive that the regression line would cross the y-axis at the point of (0,0) as an NFL team not gaining any yards is highly unlike to score any points.\nIt is important to remember that the linear model attempts to position the regression line to come as close to all the individual points as possible. Because of this, it is not uncommon for regression line to not cross exactly where the x-axis and y-axis meet. Again, contextual factors of an NFL game are not accounted for in the model’s data: strength of the opponent’s defense, the quality of special teams play, defensive turnovers and/or touchdowns, field position, etc. can all impact a team’s ability to score points without gaining any yardage. The lack of this information in the data ultimately impacts the positioning of the line of best fit.\nThe total_yards coefficient represents the slope of the model’s regression line. It is this slope that represents how a team’s total points are predicted to change with every additional gain of one yard. In this example, the total_yards coefficient is 0.10341 - so for every additional yard gained by a team, it is expected to add 0.10341 points to the team’s cumulative amount.\nThe Std. Error summary statistic provides guidance on the accuracy of the other estimated coefficients. The Std. Error for the model’s (Intercept) is quite large at 65.44927. Given the ability to resample the data from NFL terms numerous times and then allowing the linear model to predict again, this specific Std. Error argues that the regression line will cross the y-axis within 65.44972 of -225.03520 in either direction. Under normal circumstances, a large Std. Error for the (Intercept) would cause concern about the validity of the regression line’s crossing point. However, given the nature of this data - an NFL team cannot score negative points - we should not have any significant concern about the large Std. Error summary statistic for the (Intercept).\nAt 0.01132, the Std. Error for the total_yards coefficient is small and indicates that the Estimate of total_yards - that is, the increase in points per every yard gained - is quite accurate. Given repeated re-estimating of the data, the relationship between total_yards and total_points would vary by just 0.01132, either positively or negatively.\nWith a t-value of 9.135, the total_yards coefficient has a significant relationship with total_points. The value of -3.438 indicates that the (Intercept) is statistically different from 0 but we should still largely ignore this relationship given the nature of the data.\nThe model’s Pr(&gt;|t|) value of highly significant for total_yards and is still quite strong for the (Intercept). The value of 0.00000000036 indicates an incredibly significant relationship between total_yards and total_points.\nThe linear model’s Residual Standard Error is 32.04, which means that the average predicted values of total_points are 32.04 points different from the actual values in the data. The linear model was able to explain 73.56% of the variance between total_yards and total_points based on the multiple R-squared value of 0.7356. Additionally, the Adjusted R-squared value of 0.7268 is nearly identical to the multiple R2, which is a sign that the linear model is not overfitting (in this case because of the simplicity of the data). The model’s F-Statistic of 83.45 indicates a overall significance to the data, which is backed up by an extremely strong p-value.\nBased on the summary statistics, the linear model did an extremely good job at capturing the relationship between a team’s total_yards and total_points. However, with residuals ranging from -71.443 to 68.080, it is likely that the model can be improved upon by adding additional information and statistics. However, before providing additional metrics, we can try to improve the model’s predictions by including all of the data (rather than just the 2022 season). By including 20-seasons worth of total_yards and total_points, we are increasing the sample size which, in theory, allows for a reduced impact of any outliers and an improve generalizability.\n\n\n\n\n\n\nImportant\n\n\n\nWorking with 10+ years of play-by-play data can be problematic in that the model, using just total_yards and total_points, is not aware of changes in the overall style of play NFL. The balance between rushing and passing has shifted, there’s been a philosophical shift in the coaching ranks in “going for it” on 4th down, etc. A simple linear regression cannot account for how these shifts impact the data on a season-by-season basis.\n\n\nThe results from including the total_points and total_yards for each NFL team from 2012-2022 show an improvement of the model, specifically with the residual values.\n\nregression_all_seasons &lt;- simple_regression_data %&gt;%\n  select(-season)\n\nall_season_results &lt;- lm(total_points ~ total_yards,\n                         data = regression_all_seasons)\n\nsummary(all_season_results)\n\nThe residual values after including 20-seasons worth of data are a bit better. The Median is -1.26 which is slightly higher than just one season (M = 1.16). The 1Q and 3Q distributions are both approximately symmetric around the model’s M value compared to just the 2022 season regression that results in a deviation between 1Q and 3Q (-22.33 and 19.15, respectively). The Min and Max values of the new model still indicate longtail cases on both ends of the regression line much like the 2022 model found.\n\n\n\n\n\n\nTip\n\n\n\nTo further examine the residual values, we can use a Shapiro-Wilk Test to test the whether results are normally distributed.\nThe Shapiro-Wilk Test provides two values with the output: the test statistic (provided as a W score) and the model’s p-value. Scores for W can range between 0 and 1, where results closer to 1 mean the residuals are in a normal distribution. The p-value is used make a decision on the null hypothesis (that there is enough evidence to conclude that there is uneven distribution). In most cases, if the p-value is larger than the regression’s level of significance (typically 0.05), than you may reject the null hypothesis.\nWe can run the Shapiro-Wilk Test on our 2012-2022 data using the shapiro.test function that is part of the stats package in R.\n\nresults_2012_2020 &lt;- residuals(all_season_results)\n\nshapiro_test_result &lt;- shapiro.test(results_2012_2020)\n\nshapiro_test_result\n\n\n    Shapiro-Wilk normality test\n\ndata:  results_2012_2020\nW = 1, p-value = 0.8\n\n\nThe W score for the residual is 1, meaning a very strong indication that the data in our model is part of a normal distribution. The p-value is 0.8, which is much larger than the regression’s level of significance (0.05). As a result, we can reject the null hypothesis and again conclude that the data is in a normal distribution.\n\n\n\nteams &lt;- nflreadr::load_teams(current = TRUE)\n\nregression_all_seasons &lt;- left_join(regression_all_seasons,\n                                    teams, by = c(\"team\" = \"team_abbr\"))\n\nregression_all_seasons$residuals &lt;- residuals(all_season_results)\n\nggplot(regression_all_seasons, aes(x = total_yards, y = residuals)) +\n  geom_hline(yintercept = 0,\n             color = \"black\", linewidth = .7) +\n  stat_fit_residuals(size = 2,\n                     color = regression_all_seasons$team_color) +\n  stat_fit_deviations(size = 1,\n                      color = regression_all_seasons$team_color, alpha = 0.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +\n  labs(title = \"**Total Yards & Residual Values: 2012-2022**\",\n       subtitle = \"*Y = total_points ~ total_yards*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"Total Yards\") +\n  ylab(\"Residual of Total Points\") +\n  nfl_analytics_theme() +\n  theme(panel.grid.minor.y = element_line(color = \"#d0d0d0\"))\n\n\n\nResidual values for 20 seasons of data\n\n\n\nWe can also compare the multiple R2 and adjusted R2 score between the two regression models.\n2012 - 2022 Data:\nMultiple R-squared:  0.683\nAdjusted R-squared:  0.682\n\n2022 Data\nMultiple R-squared:  0.736\nAdjusted R-squared:  0.727\nThe regression using just the 2022 data results in a slightly better multiple and adjusted R2 score compared to using data from the last twenty seasons of the NFL. While this does indicate that the model based on the single season is better at defining the relationship between a team’s total_yards and total_points it is essential to remember that there is different underlying patterns in the data as a result of the changing culture in the NFL and, ultimately, the epp and flow of team performance as a result of high levels of parity in the league.\nIn order to account for this “epp and flow” in both team performance and the changing culture/rules of the NFL, we need to turn to a multiple linear regression in order to include these additional factors as it is a model that is capable of better accounting for the nuances of NFL data.\n\n5.2.2 Multiple Linear Regression\nA multiple linear regression is extremely similar to a simple linear regression (both in design and implementation in RStudio).The main difference is that a multiple linear regression allows for us to include additional predictor variables by using the + sign in the model’s formula. The inclusion of these additional predictive variables, in theory, allows the model to compute more complex relationships in NFL data and improve on the model’s final performance.\nWe will again create our first multiple regression linear regression with data from just the 2022 regular season that includes the same predictor (total_yards) and response variable (total_points). For additional predictors, we must consider what circumstances may lead a team to have high total_yards but an amount of total_points that would fall below the model’s predicted value. We will include the following as additional predictors:\n\n\nRedzone Efficiency: provided as a percentage, this is a calculation of how many times a team drove into the redzone and scored. A higher percentage is better.\n\nRedzone Touchdown Efficiency: This is the same as redzone efficiency, but includes only the number of redzone trips divided by the total touchdowns scored from the redzone.\n\nRedzone Field Goal Efficiency: The same as redzone touchdown efficiency, but with field goals.\n\nCumulative Turnovers: The total number of turnovers during the regular season.\n\nDefensive Touchdowns: The number of touchdowns scored by each team’s defensive unit.\n\nSpecial Teams Touchdowns: The number of touchdowns scored by special teams (kick/punt returns).\n\nTo begin building the multiple linear regression model for the 2022 season, we can read in the data below using vroom::vroom().\n\nmultiple_lm_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/multiple-lm\")\n\nmultiple_lm_data &lt;- multiple_lm_data %&gt;%\n  filter(season == 2022)\n\nteams &lt;- multiple_lm_data$team\n\nThe data for our multiple linear regression has the same four columns as the simple linear regression (season, team, total_points, and total_yards) but also includes the new predictor variables (rz_eff, rz_td_eff, rz_fg_eff, def_td, and spec_tds).\n\n\n\n\n\n\nCaution\n\n\n\nPlease note that, of the predictor and response variables, all of the values are in whole number format except for rz_eff, rz_td_eff, and rz_fg_eff. While it is not a problem to include predictors that are on differing scales (in this case, whole numbers and percentages), it may cause difficulty in interpreting the summary statistics. If this is the case, the issue can be resolved by using the scale() function to standardize all the predictors against one another.\n\n\nThe construction of the multiple linear regression model is the same process of the simple regression, with the inclusion of additional predictors to the formula using the + sign. We are also applying a filter() to our multiple_lm_data to retrieve just the 2022 season.\n\nlm_multiple_2022 &lt;- lm(total_points ~ total_yards + rz_eff + rz_td_eff + rz_fg_eff\n                       + total_to + def_td + spec_tds, data = multiple_lm_data)\n\nsummary(lm_multiple_2022)\n\n\nCall:\nlm(formula = total_points ~ total_yards + rz_eff + rz_td_eff + \n    rz_fg_eff + total_to + def_td + spec_tds, data = multiple_lm_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-53.03 -15.58  -0.35  14.60  43.53 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -459.5225   128.5557   -3.57   0.0015 ** \ntotal_yards    0.0906     0.0113    8.02  2.2e-08 ***\nrz_eff       228.8853   113.5625    2.02   0.0547 .  \nrz_td_eff    167.2323    82.7097    2.02   0.0540 .  \nrz_fg_eff          NA         NA      NA       NA    \ntotal_to       0.4058     1.4934    0.27   0.7881    \ndef_td         4.4560     4.0573    1.10   0.2826    \nspec_tds       5.4977     7.8674    0.70   0.4911    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.9 on 25 degrees of freedom\nMultiple R-squared:  0.844, Adjusted R-squared:  0.807 \nF-statistic: 22.6 on 6 and 25 DF,  p-value: 5.81e-09\n\n\nThe summary statistic residuals for the multiple linear regression are more evenly distributed towards the mean than our simple linear regression. Based on the residuals, we can conclude that - for 50% of the teams - the model either over or underestimated their total_points by just -0.35 (as listed in the Median residual). The interquartile range (within the 1Q and 3Q quartiles) are both close to the median and the Min and Max residuals both decreased significantly from our simple linear model, indicating a overall better line of fit.\nWe can confirm that the multiple linear regression resulted in an even distribution of the residuals by again using a Shapiro-Wilk Test.\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  results_lm_2022\nW = 1, p-value = 0.9\n\n\nThe results of the Shapiro-Wilk test (W = 1 and p-value = 0.9) confirm that residuals are indeed evenly distributed. A visualization showcases the model’s even distribution of the residuals.\n\nmlm_2022_fitted &lt;- predict(lm_multiple_2022)\nmlm_2022_residuals &lt;- residuals(lm_multiple_2022)\n\nplot_data_2022 &lt;- data.frame(Fitted = mlm_2022_fitted,\n                             Residuals = mlm_2022_residuals)\n\nplot_data_2022 &lt;- plot_data_2022 %&gt;%\n  cbind(teams)\n\nnfl_teams &lt;- nflreadr::load_teams(current = TRUE)\n\nplot_data_2022 &lt;- plot_data_2022 %&gt;%\n  left_join(nfl_teams, by = c(\"teams\" = \"team_abbr\"))\n\nggplot(plot_data_2022, aes(x = Fitted, y = Residuals)) +\n  geom_hline(yintercept = 0,\n             color = \"black\", linewidth = .7) +\n  stat_fit_deviations(size = 1.75,\n                      color = plot_data_2022$team_color) +\n  geom_image(aes(image = team_logo_espn),\n             asp = 16/9, size = .0325) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks()) +\n  labs(title = \"**Multiple Linear Regression Model: 2022**\") +\n  xlab(\"Fitted Values\") +\n  ylab(\"Residual Values\") +\n  nfl_analytics_theme() +\n  theme(panel.grid.minor.y = element_line(color = \"#d0d0d0\"))\n\n\n\nVisualizing residuals of a multiple linear regression\n\n\n\nJust as the residual values in the summary statistics indicated, plotting the fitted_values against the residual_values shows an acceptable spread in the distribution, especially given the nature of NFL data. Despite positive results in the residual values, the summary statistics of the multiple linear regression indicates a significant issue with the data. Within the Coefficients, it is explained that one of the items is “not defined because of singularities.”\n\n\n\n\n\n\nImportant\n\n\n\n“Singularities” occur in the data as a result of the dreaded multicollinearity between two or more predictors. The involved predictors were found to have a high amount of correlation between one another, meaning that one of the variables can be predicted in a near linear fashion with one or more of the other predictive variables. As a result, it is difficult for the regression model to correctly estimate the contribution of these dependent variables to the response variable.\nThe model’s Coefficients of our multiple linear regression shows NA values for the rz_fg_eff predictor (the percentage of times a team made a field goal in the red zone rather than a touchdown). This is because rz_fg_eff was one of the predictive variables strongly correlated and was the one dropped by the regression model to avoid producing flawed statistics as a result of the multicollinearity.\nIf you are comfortable producing the linear regression with rz_fg_eff being a dropped predictor, there are no issues with that. However, we can create a correlation plot that allows is to determine which predictors have high correlation values with others. Examining the issue allows us to determine if rz_fg_eff is, indeed, the predictive variable we want the regression to drop or if we’d rather, for example, drop rz_eff and keep just the split between touchdowns and field goals.\n\nregression_corr &lt;-\n  cor(multiple_lm_data[, c(\"total_yards\",\n                           \"rz_eff\", \"rz_td_eff\",\n                           \"rz_fg_eff\", \"total_to\",\n                           \"def_td\", \"spec_tds\")])\n\nmelted_regression_corr &lt;- melt(regression_corr)\n\nggplot(data = melted_regression_corr, aes(x = Var1,\n                                          y = Var2,\n                                          fill = value)) +\n  geom_tile() +\n  scale_fill_distiller(palette = \"PuBu\",\n                       direction = -1,\n                       limits = c(-1, +1)) +\n  geom_text(aes(x = Var1, y = Var2, label = round(value, 2)),\n            color = \"black\",\n            fontface = \"bold\",\n            family = \"Roboto\", size = 5) +\n  labs(title = \"Multicollinearity Correlation Matrix\",\n       subtitle = \"Multiple Linear Regression: 2022 Data\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  nfl_analytics_theme() +\n  labs(fill = \"Correlation \\n Measure\", x = \"\", y = \"\") +\n  theme(legend.background = element_rect(fill = \"#F7F7F7\"),\n        legend.key = element_rect(fill = \"#F7F7F7\"))\n\n\n\nUsing a correlation matrix to determine which predictor have high correlation values with others\n\n\n\nUsing a correlation plot allows for easy identification of those predictive variables that have high correlation with one another. The general rule is that two predictors become problematic in the regression model if the coefficient between the two is above 0.7 (or 0.8, given domain knowledge about the context of the data).\nIn our correlation plot, there are two squares (indicated by the darkest blue color) that have a value greater than 0.7 (or -0.7 in this case, as both strong and negative correlations are capable of producing multicollinearity. The two squares happen to relate to the same relationship between the rz_fg_eff and rz_td_eff predictors.\nRecall that the regression model automatically removed the rz_fg_eff from the measured Coefficients. Given the context of the data, I am not sure that is the best decision. Because we are examining the relationship between the predictive variables and total_points, removing the rz_fg_eff variable inherently erases a core source of points in a game of football.\nBecause of this - and since our rz_eff predictor accounts for both touchdowns and field goals - I believe we can move forward on rerunning the regression without bothrz_fg_eff and rz_td_eff.\n\n\nTo run the multiple linear regression again, without the predictors relating to red zone touchdown and field efficiency, we will drop both from our multiple_lm_2022 data frame, rerun the regression model, and then examine the ensuing summary statistics.\n\nlm_multiple_2022_edit &lt;- multiple_lm_data %&gt;%\n  select(-rz_td_eff, -rz_fg_eff)\n\nlm_multiple_2022_edit &lt;- lm(total_points ~ total_yards + rz_eff +\n                         total_to + def_td + spec_tds,\n                         data = lm_multiple_2022_edit)\n\nsummary(lm_multiple_2022_edit)\n\n\nCall:\nlm(formula = total_points ~ total_yards + rz_eff + total_to + \n    def_td + spec_tds, data = lm_multiple_2022_edit)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-60.18 -14.83  -3.79  18.99  55.92 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -472.5877   135.8042   -3.48   0.0018 ** \ntotal_yards    0.0998     0.0109    9.13  1.4e-09 ***\nrz_eff       309.1167   112.5462    2.75   0.0108 *  \ntotal_to      -0.2760     1.5388   -0.18   0.8591    \ndef_td         3.5808     4.2670    0.84   0.4090    \nspec_tds       4.9584     8.3167    0.60   0.5562    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.5 on 26 degrees of freedom\nMultiple R-squared:  0.819, Adjusted R-squared:  0.784 \nF-statistic: 23.5 on 5 and 26 DF,  p-value: 7.03e-09\n\n\nWe have certainly simplified the model by removing both rz_td_eff and rz_fg_eff but the impact of this change is a fair trade off to avoid further issues with multicollinearity. Our new adjusted R is still high (0.784), only dropping a bit from the original model that included both predictors (0.807). Both models did well at explaining the amount of variance between the predictors and the response variable. While the F-statistic and the p-value are strong in both models, it is important to note that the Residual standard error dropped from 27 in the original model to 28 in the more simplified version. Given that this value is the average difference between the actual values and the predicted equivalents in the regression, both would ideally be smaller.\nWith multiple linear regression model producing acceptable results over the course of the 2022 season, we can now see if the results remain stable when produced over the course of the 2012-2022 seasons.\n\nmultiple_lm_data_all &lt;- multiple_lm_data %&gt;%\n  select(-rz_td_eff, -rz_fg_eff, -season)\n\nlm_multiple_all &lt;- lm(total_points ~ total_yards + rz_eff +\n                              total_to + def_td + spec_tds,\n                      data = multiple_lm_data_all)\n\nsummary(lm_multiple_all)\n\n\nCall:\nlm(formula = total_points ~ total_yards + rz_eff + total_to + \n    def_td + spec_tds, data = multiple_lm_data_all)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-60.18 -14.83  -3.79  18.99  55.92 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -472.5877   135.8042   -3.48   0.0018 ** \ntotal_yards    0.0998     0.0109    9.13  1.4e-09 ***\nrz_eff       309.1167   112.5462    2.75   0.0108 *  \ntotal_to      -0.2760     1.5388   -0.18   0.8591    \ndef_td         3.5808     4.2670    0.84   0.4090    \nspec_tds       4.9584     8.3167    0.60   0.5562    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.5 on 26 degrees of freedom\nMultiple R-squared:  0.819, Adjusted R-squared:  0.784 \nF-statistic: 23.5 on 5 and 26 DF,  p-value: 7.03e-09\n\n\nThe results of the multiple linear regression over data from the 2012-2022 indicates a statistically significant relationship between our predictor variables and a team’s total yards. That said, two items are worth further exploration.\n\nThe model’s Residual standard error increased closer to 30, as opposed to the values of 27 and 28 from the models built on a single season of data. This means that the model, on average, is over or underpredicting the actual values by approximately thirty points. To verify that a residual standard error of 30 is not too high, we can evaluate the value against the scale of our data based on the mean and/or median averages of the total_points variable. As seen below, the model’s RSE as a percentage of the mean is 8.1% and its percentage of the median is 8.2%. Given that both values are below 10%, it is reasonable to conclude that the value of the model’s residual standard error is statistically small compared to the scale of the total_points dependent variable.\n\n\n\n\n\n\n\nTip\n\n\n\n\ntotal_mean_points &lt;- mean(multiple_lm_data_all$total_points)\ntotal_points_median &lt;- median(multiple_lm_data_all$total_points)\n\nrse_mean_percentage &lt;- (30 / total_mean_points) * 100\nrse_median_percentage &lt;- (30 / total_points_median) * 100\n\n\n\n\nThe spec_tds predictor, which is the total number of special teams touchdowns scored by a team, has a p-value of 0.61. This high of a p-value indicates that the amount of special teams touchdowns earned by a team is not a dependable predictor of the team’s total points. Given the rarity of kickoff and punt returns, it is not surprising that the predictor returned a high p-value. If we run the regression again, without the spec_tds predictive variable, we get results that are nearly identical to the regression model that includes it as a predictor. The only significant difference is a decrease in the F-statistic from 398 to 317. Given the small decrease, we will keep spec_tds in the model.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe final step of our multiple linear regression model is feeding it new data to make predictions on.\nTo begin, we need to create a new data frame that holds the new predictor variables. For nothing more than fun, let’s grab the highest value from each predictive variable between the 2012-2022 seasons.\n\nnew_observations &lt;- data.frame(\n  total_yards = max(multiple_lm_data$total_yards),\n  rz_eff = max(multiple_lm_data$rz_eff),\n  total_to = max(multiple_lm_data$total_to),\n  def_td = max(multiple_lm_data$def_td),\n  spec_tds = max(multiple_lm_data$spec_tds))\n\nThis hypothetical team gained a total of 7,317 yards in one season and was incredibly efficient in the red zone, scoring 96% of the time. It also scored nine defensive touchdowns and returned a punt or a kickoff to the house four times. Unfortunately, the offense also turned the ball over a whopping total of 41 times.\nWe can now pass this information into our existing model using the predict function and it will output the predicted total_points earned by this hypothetical team based on the multiple linear regression model we built with 20 years of NFL data.\n\nnew_predictions &lt;- predict(lm_multiple_all, newdata = new_observations)\n\nnew_predictions\n\nThe model determined, based on the new predictor variables provided, that this hypothetical team will score a total of 563 points, which is the second-highest cumulative amount scored by a team dating back to the 2012 season (the 2013 Denver Broncos scored 606 total points). In this situation, the hypothetical team has nearly double the turnovers as the 2013 Bronco (41 turnovers to 21). It is reasonable that providing this hypothetical team a lower number of turnovers would result in it becoming the highest scoring team since 2012.\n\n\n\n5.2.3 Logistic Regression\nLogistic regressions are particularity useful when modeling the relationship between a categorical dependent variable and a given set of predictor variables. While linear models, as covered in the last section, handles data with a continuous dependent variable, logistic regressions are used when the response variable is categorical (whether a team successfully converted a third down, whether a pass was completed, whether a running back fumbled on the play, etc.). In each example, there are only two possible outcomes: “yes” or “no”.\nMoreover, a logistic regression model does not model the relationship between the predictors and the response variables in a linear fashion. Instead, logistic regressions use seek to mutate the predictors into a values between 0 and 1. Specifically, the formula for a logistic regression is as follows:\n\\[\nP(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}\n\\]\n\n\n\\(P(Y=1|X)\\) in the equation is the likelihood of the event \\(Y=1\\) taking place given the provided predictor variables (\\(X\\)).\nThe model’s coefficients are represented by \\(\\beta_0 + \\beta_1X_1 + \\beta_2\\).\n\n\\(X1, X2, ..., Xn\\) represent the model’s predictor variables.\nThe \\(1\\) in the formula’s numerator results in the model’s probability values always being between 0 and 1.\nThe \\(1\\) in the denominator is a part of the underlying logistic function and ensures that the value is always greater than or equal to 1.\nFinally, the \\(e\\) is part of the model’s logarithm (or the constant of 2.71828). In short, this function allows the linear combination to be a positive value.\n\nTo better explain the formula, let’s assume we want to predict the probability of any one NFL team winning a game (again, in a binary 1 for winning, or 0 for losing). These binary outcome is represented by the \\(Y\\). The predictor variables, such as the team’s average points scored, the average points allowed by the opposing defense, home-field advantage, and any other statistics likely to be important in making the prediction are represented by \\(X1, X2, ..., Xn\\). The model’s coefficients, or the relationship between the predictors and a team’s chance of winning, are represented by \\(\\beta_0 + \\beta_1X_1 + \\beta_2\\). In the event that a team’s average passing yards is represented by the \\(B1\\) coefficient, a positive number suggests that higher average passing yards results in an increase in the probability of winning. Conversely, a negative coefficient number indicates the statistic has a negative impact on winning probability.\nTo that end, there are three distinct types of logistic regressions: binary, multinomial, and ordinal (we will only be covering binary and multinomial in this chapter, however). All three allow for working with various types of dependent variables.\nBinary regression models are used when the dependent variable (the outcome) has just two outcomes (typically presented in binary format - 1 or 0). Using the logistic function, a binary regression model determines the probability of the dependent event occurring given the predictor variables. As highlighted below, a binary regression model can be used to predict the probability that a QB is named MVP at the conclusion of the season. The response variable is a binary (1 indicating that the player was named MVP and 0 indicating that the player did not win MVP). Various passing statistics can serve as the predictor variables.\nA multinomial regression model is an extension of the binary model in that it allows for the dependent variable to have more than two unordered categories. For example, a multinomial regression model can be used to predict the type of play a team is going to run (run or pass) based on the predictor variables (down and distance, time remaining, score differential, etc.).\nAn ordinal regression model is used when the dependent variables not only have ordered categories, but the structure and/or order of these categories contains meaningful information. Ordinal regression can be used, for example, to attempt to predict the severity of a player’s injury. The ordered and meaningful categories of the dependent variable can coincide with the severity of the injury itself (mild, moderate, severe, season-ending) while the predictor variables include several other types of categories (the type of contact, the playing surface, what position the player played, how late into the season it occurred, whether the team is coming off a bye week, etc.).\n\n5.2.3.1 Logistic Regression 1: Binary Classification\nA binary regression model is used when the dependent variable is in binary format (that is, 1 for “yes” or 0 for “no). This binary represents two - and only two - possible outcomes such as a a team converting on third down or not or if a team won or lost the game. Constructing binary regression models allows use to predict the likelihood of the dependent event occurring based on the provided set of predictor variables.\nWe will build a binary regression model to predict the probability that an NFL QB will win the MVP at the conclusion of the season.\nLet’s first create a data frame, called qb_mvp, that contains the statistics for quarterbacks that we intuitively believe impact the likelihood of a QB being named MVP.\n\nplayer_stats &lt;- nflreadr::load_player_stats(2000:2022) %&gt;%\n  filter(season_type == \"REG\" & position == \"QB\") %&gt;%\n  filter(season != 2000 & season != 2005 & season != 2006 & season != 2012) %&gt;%\n  group_by(season, player_display_name, player_id) %&gt;%\n  summarize(\n    total_cmp = sum(completions, na.rm = TRUE),\n    total_attempts = sum(attempts, na.rm = TRUE),\n    total_yards = sum(passing_yards + rushing_yards, na.rm = TRUE),\n    total_tds = sum(passing_tds + rushing_tds, na.rm = TRUE),\n    total_interceptions = sum(interceptions, na.rm = TRUE),\n    mean_epa = mean(passing_epa, na.rm = TRUE)) %&gt;%\n  filter(total_attempts &gt;= 150) %&gt;%\n  ungroup()\n\nWe are using data from over the course of 22 NFL seasons (2000 to 2022), but then removing the 2000, 2005, 2006, and 2012 seasons as the MVP for each was not a quarterback (Marshall Faulk, Shaun Alexander, LaDainian Tomlison, and Adrian Peterson, respectively). To begin building the model, we collect QB-specific metric from the load_player_stats() function from nflreadr, including: total completions, total attempts, total yards (passing + rushing), total touchdowns (passing + rushing), and the QB’s average EPA (only for pass attempts).\nBecause the style of play in the NFL has changed between the earliest seasons in the data frame and the 2022 season, it may not be fair to compare the specific statistics to each other. Rather, we can rank the quarterbacks for each season, per statistic, in a decreasing fashion as the statistical numbers increase. For example, Patrick Mahomes led the league in passing yards in the 2022 season. As a result, he is ranked as 1 in the forthcoming yds_rank column while the QB with the second-most yards (Justin Herbert, with 4,739) will be ranked as 2 in the yds_rank column. This process allows us to normalize the data and takes into the account the change in play style in the NFL over the time span of the data frame. To add the rankings to our data, we will create a new data frame titled qb_mvp_stats from the existing player_stats and then use the order function from base R to provide the rankings in descending order. After, we use the select() function to gather the season, player_display_name, and player_id as well as the rankings that we created using order().\n\nqb_mvp_stats &lt;- player_stats %&gt;%\n  dplyr::group_by(season) %&gt;%\n  mutate(cmp_rank = order(order(total_cmp, decreasing = TRUE)),\n         att_rank = order(order(total_attempts, decreasing = TRUE)),\n         yds_rank = order(order(total_yards, decreasing = TRUE)),\n         tds_rank = order(order(total_tds, decreasing = TRUE)),\n         int_rank = order(order(total_interceptions, decreasing = FALSE)),\n         epa_rank = order(order(mean_epa, decreasing = TRUE))) %&gt;%\n  select(season, player_display_name, player_id, cmp_rank, att_rank, yds_rank, tds_rank,\n         int_rank, epa_rank)\n\nThe data, as collected from load_player_stats(), does not contain information pertaining to MVP winners. To include this, we can load a prepared file using data from Pro Football Reference. The data contains two variables: season and player_name wherein the name represents the player that won that season’s MVP. After reading the data in, we can use the mutate function to create a new variable called mvp that is in binary (1 representing that the player was the MVP). After, we can merge this data into our qb_mvp_stats data frame. After merging, you will notice that the mvp column has mainly NA values. We must set all the NA values to 0 to indicate that those players did not win the MVP that season.\n\npfr_mvp_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/pfr-mvp\")\n\npfr_mvp_data$mvp &lt;- 1\n\nqb_mvp_stats &lt;- qb_mvp_stats %&gt;%\n  left_join(pfr_mvp_data, by = c(\"season\" = \"season\",\n                                 \"player_display_name\" = \"player_name\"))\n\nqb_mvp_stats$mvp[is.na(qb_mvp_stats$mvp)] &lt;- 0\n\nThe above results in a data frame with 723 observations with 6 predictive variables used to determine the probability of the QB being named MVP. We can now turn to the construction of the regression model.\n\n\n\n\n\n\nImportant\n\n\n\nIf you have not already done so, please install tidymodels and load it.\n\ninstall.packages(\"tidymodels\")\nlibrary(tidymodels)\n\n\n\nWhile we will not be building the binary model with the tidymodels package, we will be utilizing the associated rsample package - which is used to created various types of resamples and classes for analysis - to split our qb_mvp_stats data into both a training and testing set.\n\n\n\n\n\n\nImportant\n\n\n\nLike many of the models to follow in this chapter, it is important to have both a training and testing set of data when performing a binary regression study for three reasons:\n\n\nAssess model performance. Having a trained set of data allows us to evaluate the model’s performance on the testing set, allowing us to gather information regarding how we can expect the model to handle new data.\n\nIt allowd us to avoid overfitting. Overfitting is a process that occurs when the regression model recognizes the “statistical noise” in the training data but not necessarily the underlying patterns. When this happens, the model will perform quite well on the training data but will then fail when fed new, unseen data. By splitting the data, we can use the withheld testing data to make sure the model is not “memorizing” the information in the training set.\n\nModel selection. In the case that you are evaluating several different model types to identify the best performing one, having a testing set allows you to determine which model type is likely to perform best when provided unseen data.\n\n\n\nThe process of splitting the data into a training and testing set involves three lines of code with the rsample package. Prior to splitting the data, we will create a new data frame titled mvp_model_data that is our existing qb_mvp_stats information after using the ungroup() function to overwrite any prior group_by() that was applied to the data frame. We first use the initial_split function to create a split of the data into a training and testing set. Moreover, we use the strata argument to conduct what is called “stratified sampling.” Because there are very few MVPs in our data compared to those non-MVP players, using the strata argument allows us to create a training and test set with a similar amount of MVPs in each. We then use the training and testing argument to create the data for each from the initial split.\n\nmvp_model_data &lt;- qb_mvp_stats %&gt;%\n  ungroup()\n\nmvp_model_split &lt;- rsample::initial_split(mvp_model_data, strata = mvp)\nmvp_model_train &lt;- rsample::training(mvp_model_split)\nmvp_model_test &lt;- rsample::testing(mvp_model_split)\n\nWith the MVP data split into both a training and testing set, we use the glm package to conduct the regression analysis on the mvp_model_train data frame.\n\nmvp_model &lt;- glm(formula = mvp ~ cmp_rank + att_rank + yds_rank +\n                   tds_rank + int_rank + epa_rank,\n                 data = mvp_model_train, family = binomial)\n\nWhile the model does run, and placing the mvp_model results in the RStudio environment, we receive the following warning message in the Console:\nWarning message: glm.fit: fitted probabilities numerically 0 or 1 occurred\nAs the message indicates, this is not necessarily an error in the modeling process, but an indication that some of the fitted probabilities in the model are fitted numerically to 0 or 1. This most often occurs in binomial regression models when there is nearly perfect separation within the data underpinning the model, meaning there is one or more predictor variables (passing yards, passing touchdowns, etc.) that are able to perfectly classify the outcome (MVP) of the model. This type of “perfect separation” in modeling is usual when working with data that is both limited and “rare” - only one QB per season in our data is able to win the MVP award. We can create a quick histogram of the fitted probabilities wherein multitudes of probabilities close to either 0 or 1 is a good explanation for the warning message.\n\nplot(residuals(mvp_model))\n\n\n\n\nThe histogram clearly shows the separation issue in the model’s data. The majority of the QBs in the data have a 0% to an incredibly slim chance to win the MVP, while the bar for those with a fitted probability from 0.2 to 1.0 are barely visible in the plot. While a warning that “fitted probabilities numerically 0 or 1 occurred” is cause for further examination, we are able to diagnose the issue by using our “domain knowledge” of the data - that is, of course separation is going to occur since only one QB can be MVP in any season.\nTo verify that the model is predicting reasonable probabilities, we can calculate the predicted values and probabilities on the existing training data.\n\ntraining_probs &lt;- mvp_model_train %&gt;%\n  mutate(pred = predict(mvp_model, mvp_model_train,\n                        type = \"response\")) %&gt;%\n  group_by(season) %&gt;%\n  mutate(pred_mvp = as.numeric(pred == max(pred, na.rm = TRUE)),\n         mvp_prob = pred / sum(pred))\n\nWith the model now trained, we can take the results of the model and apply it to our withheld mvp_model_test data frame. Using the predict function, we instruct to take the modeled predictions from the trained mvp_model and generate further predictions on the mvp_model_test data frame. Because the type = \"response\" argument results in a probability between 0 and 1, we use ifelse to create a “cutoff” for the probabilities, where any number above 0.5 will be 1 while any number below will be 0.\n\ntest_predictions &lt;- predict(mvp_model,\n                            newdata = mvp_model_test, type = \"response\")\n\ntest_class_predictions &lt;- ifelse(test_predictions &gt; 0.5, 1, 0)\n\nWith the model now fitted to previously unseen data (the withheld mvp_model_test data frame), we can calculate its accuracy.\n\naccuracy &lt;- sum(test_class_predictions == mvp_model_test$mvp) / nrow(mvp_model_test)\n\naccuracy &lt;- round(accuracy, 2)\n\nprint(paste(\"Accuracy:\", accuracy))\n\n[1] \"Accuracy: 0.98\"\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA model that predicts with 97% accuracy seems great, right?\nHowever, we must keep in mind the significant class imbalance in our data as visualized on the earlier histogram. Roughly 3% of the quarterbacks in the qb_mvp_stats data frame won the MVP award, meaning the remaining 97% of the quarterbacks did not.\nBecause of this, a model could always predict “not MVP” and, as a result, also have an accuracy of 97%.\nWhile a high accuracy score like the model produced is a promising first sign, it is important to remember that accuracy - as a metric - only gives the broad picture. It does not depict how the model is performing on each individual class (that is, 0 and 1). To get a more granular understanding of the model’s performance, we can turn to metrics such as precision , recall , and the F1 score.\nTo begin determining the scores for each, we must conduct a bit of data preparation by ensuring that both test_class_predictions and mvp_model_test$mvp are both as.factor(). After, we use the confusionMatrix function from the caret package to build a matrix between the binary class predictions (from test_class_predictions) and the true labels (from mvp_model_test$mvp).\n\nlibrary(caret)\n\ntest_class_predictions &lt;- as.factor(test_class_predictions)\nmvp_model_test$mvp &lt;- as.factor(mvp_model_test$mvp)\n\nmvp_cm &lt;- confusionMatrix(test_class_predictions, mvp_model_test$mvp)\n\nAfter constructing the confusion matrix, we can calculate the results for precision, recall, and F1.\n\nprecision &lt;- mvp_cm$byClass['Pos Pred Value']\n\nrecall &lt;- mvp_cm$byClass['Sensitivity']\n\nf1 &lt;- 2 * (precision * recall) / (precision + recall)\n\nprint(paste(\"Precision:\", precision))\n\n[1] \"Precision: 0.988636363636364\"\n\nprint(paste(\"Recall:\", recall))\n\n[1] \"Recall: 0.994285714285714\"\n\nprint(paste(\"F1 Score:\", f1))\n\n[1] \"F1 Score: 0.991452991452991\"\n\n\nThe resulting scores further indicate a well-trained and performing model.\n\n\nPrecision - the Positive Predictive Value is the fraction of true positives among all positive predictions (including false positives). The resulting score of 0.96 means that the model correctly predicted the MVP 96% of the time.\n\nRecall - the Sensitivity or True Positive Rate is similar to precision in that it is also the fraction of true positive predictions, but only those among all true positives. Our recall scores of 0.994 means that the model correctly predicted the MVP 99.4% of the time.\n\nF1 Score - the F1 score is the “harmonic mean” between precision and recall, providing a balanced combination of both. With another score of 0.98, the model’s F1 Score indicates a healthy balance between both precision and recall in its predictions.\n\nThe model producing the same score for each metric is indicative that it contains an equal number of both false positives and false negatives, which is ultimately a sign of balance in the prediction making process.\n\n\nBased on the results of accuracy, precision, recall, and the F1 score, our model is quite accurate. We can now create our own “fictional” data frame with corresponding predictor variables to determine the probabilities for each of the quarterbacks to be named MVP.\n\nnew_mvp_data &lt;- data.frame(\n  cmp_rank = c(1, 4, 2),\n  att_rank = c(2, 1, 5),\n  yds_rank = c(5, 3, 7),\n  tds_rank = c(3, 1, 2),\n  int_rank = c(21, 17, 14),\n  epa_rank = c(3, 1, 5))\n\nnew_mvp_predictions &lt;- predict(mvp_model,\n                               newdata = new_mvp_data, type = \"response\")\n\nnew_mvp_predictions\n\n     1      2      3 \n0.2413 0.9350 0.0861 \n\n\nAccording to our fully trained and tested model, fictional quarterback 2 has just over a 90% probability of winning the MVP when compared to the two other opposing quarterbacks.\nFinally, we can use the broom package to transform the model’s information into a tidyverse-friendly data frame allowing us to visualize the information using ggplot.\n\ntidy_mvp_model &lt;- broom::tidy(mvp_model) %&gt;%\n  filter(term != \"(Intercept)\")\n\nggplot(tidy_mvp_model, aes(x = reorder(term, estimate),\n                           y = estimate, fill = estimate)) +\n  geom_col() +\n  coord_flip() +\n  labs(x = \"Predictive Variables\",\n       y = \"Estimate\",\n       title = \"**Probability of a QB Being NFL MVP**\",\n       subtitle = \"*GLM Model | F1-Score: 99.4%*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  theme(legend.position = \"none\") +\n  nfl_analytics_theme()\n\n\n\nVisualizing the results of the GLM model\n\n\n\nA negative Estimate for a Predictive Variable means that as that statistic increases (a player has a worse rank among his peers), the probable odds of that quarterback being awarded the Most Value Players decreases. For example, in our model, the epa_rank predictor has an Estimate of approximately -0.8 which means for each drop in rank, a quarterback’s chance of being named MVP decreases by -0.8. Within the model, int_rank, att_rank, yds_rank, tds_ranks, and epa_rank align with our domain knowledge intuition: as quarterbacks perform worse in these areas, their chance of being MVP decreases.\nWhy, then, does a worse performance in cmp_rank provide a quarterback an increased chance in becoming MVP? Several issues could be causing this, including statistical noise, a variable that is highly correlated with cmp_rank, or a general non-linear relationship between cmp_rank and the response variable. If we were to continue working on this model, a good first step would using the filter() function to remove those quarterbacks with a limited number of attempts during the season.\n\n5.2.3.2 Logistic Regression 2: Multinomial Regression with tidymodels\n\nIn our previous example building a binomial regression model, there were only two outcomes: 0 and 1 for whether the specific quarterback was named that season’s MVP. A multinomial regression is an extension of the binomial model, but is used when there are multiple categories of outcomes rather than just two. The model itself ultimately determines the probability of each dependent variable based on the given set of predictive values. Using the softmax function to ensure that the probabilities for each dependent variable total 1, the main assumptions of the model are:\n\nthe dependent variable must be categorical with multiple unordered categories\nthe predictor variables can be a combination of continuous values or categorical\nmuch like all the other models we’ve worked on, there must not be multicollinearity among the predictor values\nthe observations must be independent of each other\n\nThe specific formula for a multinomial regression is as follows:\n\\[\n\\log \\left( \\frac{P(Y = k)}{P(Y = K)} \\right) = \\beta_{k0} + \\beta_{k1} X_1 + \\beta_{k2} X_2 + \\ldots + \\beta_{kp} X_p\n\\]\n\n\n\\(Y\\) is the categorical dependent variable\nIn the formula, \\(X_{i1}, X_{i2}, ..., X_{ip}\\) are the predictive variables\nThe coefficients are represented by \\(\\beta_{k0}, \\beta_{k1}, ...,\\beta_{kp}\\)\n\nFinally, \\(K\\) represents the number of categories the dependent variable has\n\nAs an example, let’s build a multinomial regression model that predicts the type of offensive play call based on contextual factors of the game. We will build the model around five dependent categories:\n\nrun inside (noted as middle in the play-by-play data)\nrun outside (noted as left or right in the play-by-play data)\nshort pass (passes with air yards under 5 yards)\nmedium pass (passes with air yards between 6-10 yards)\nlong passes (passes with air yards of 11+ yards)\n\nThe construction of a multinomial regression will allow us to find the probability of each play type based on factors in the game. Such a model speaks directly to coach play-calling tendencies given the situation. To begin, we will gather data from the 2010-2022 NFL regular season. After loading the data, we will select the predictor variables and then conduct some featured engineering (which is, in this case, inspired by a presentation given by Thomas Mock on how to use tidymodels to predict a run or pass using binary regression)(Mock, 2020).\n\n\n\n\n\n\nImportant\n\n\n\nAn Introduction to the tidymodels package.\nAs just mentioned, tidymodels is a collection of various packages for statistical modeling and machine learning. Importantly, the tidymodels package is structured much in the same way that tidyverse is - meaning that the packages work together to provide a unified and efficient workflow for constructing, testing, and evaluating models. A total of 43 different model types come in the base installation of tidymodels, ranging from simple linear regressions to more complex approaches such as neural networks and Bayesian analysis. You can view the different model types, housed within the parsnip package, at the tidymodels website: Type of models included in tidymodels.\nBecause tidymodels follows the tidyverse philosophy of “providing a uniform interface” so that “packages work together naturally,” there is a general flow in which users construct a model.\n\nFirst, data preprocessing and feature engineering is done with the recipes package.\nNext, the parsnip package is used to create a model specification.\nIf any resampling or tuning is being conducted during the model, the rsample package is used to organize the necessary type of hyperparameter tuning (grid search, regular search, etc.).\nThe workflows package is then used to combine all of the prior steps into a singular object in your RStudio environment.\nAfter training the model on the initial split of data, the yardstick packages allows for various forms of evaluation, including accuracy, precision, recall, F1 score, RMSE, and MAE.\nOnce you are happy with the training evaluation of the model, the last_fit() function conducts the process on the entire dataset to generate predictions.\n\nBelow, we will take our newly created model_data_clean data frame and conduct a multinomial logistic regression using the tidymodels package.\n\n\n\npbp &lt;- nflreadr::load_pbp(2006:2022) %&gt;%\n  filter(season_type == \"REG\")\n\npbp_prep &lt;- pbp %&gt;%\n  select(\n    game_id, game_date, game_seconds_remaining,\n    week, season, play_type, yards_gained,\n    ydstogo, down, yardline_100, qtr, posteam,\n    posteam_score, defteam, defteam_score,\n    score_differential, shotgun, no_huddle,\n    posteam_timeouts_remaining, defteam_timeouts_remaining,\n    wp, penalty, half_seconds_remaining, goal_to_go,\n    td_prob, fixed_drive, run_location, air_yards) %&gt;%\n  filter(play_type %in% c(\"run\", \"pass\"),\n         penalty == 0, !is.na(down), !is.na(yardline_100)) %&gt;%\n  mutate(in_red_zone = if_else(yardline_100 &lt;= 20, 1, 0),\n         in_fg_range = if_else(yardline_100 &lt;= 35, 1, 0),\n         two_min_drill = if_else(half_seconds_remaining &lt;= 120, 1, 0)) %&gt;%\n  select(-penalty, -half_seconds_remaining)\n\nThere is a lot going in on the first bit of code to prepare our multinomial regression. We first collect regular season play-by-play data from 2006 to 2022. It is important to note that the 2006 season was not arbitrarily chosen. Rather, it is the first season in which the air_yards variable is included in the data. After gathering the data, we use the select() function to keep just those variables that may give an indication of what type of pay to expect next. Lastly, we conduct feature engineering by creating three new metrics in the data (in_red_zone, in_fg_range, and two_min_drill) and use if_else() to turn all three into a 1/0 binary.\n\n\n\n\n\n\nWarning\n\n\n\nBefore running the next bit of code, be sure that your installed version of dplyr is 1.1.1 or newer.\nAn issue was discovered in version 1.1.0 that created major computational slowdowns when using case_when() and mutate() together within a group_by() variable. Prior to discovering this issue, running the below code took an absurd 22 minutes.\nAs a result, you will notice I resorted to using the fcase() function from the data.table() package. You can also use the dt_case_when() function from tidyfast if you prefer to use the same syntax of case_when() but utilize the speed of data.table::ifelse().\nAfter switching to fcase(), the code finished running in 17.69 seconds. After upgrading to dplyr 1.1.1, the case_when() version of the code completed in 18.94 seconds.\n\n\n\nmodel_data &lt;- pbp_prep %&gt;%\n  group_by(game_id, posteam) %&gt;%\n  mutate(run_inside = fcase(\n    play_type == \"run\" & run_location == \"middle\", 1,\n    play_type == \"run\", 0,\n    default = 0),\n    run_outside = fcase(\n      play_type == \"run\" & (run_location == \"left\" |\n                              run_location == \"right\"), 1,\n      play_type == \"run\", 0,\n      default = 0),\n    short_pass = fcase(\n      play_type == \"pass\" & air_yards &lt;= 5, 1,\n      play_type == \"pass\", 0,\n      default = 0),\n    medium_pass = fcase(\n      play_type == \"pass\" & air_yards &gt; 5 &\n        air_yards &lt;= 10, 1,\n      play_type == \"pass\", 0,\n      default = 0),\n    long_pass = fcase(\n      play_type == \"pass\" & air_yards &gt; 10, 1,\n      play_type == \"pass\", 0,\n      default = 0),\n    run = if_else(play_type == \"run\", 1, 0),\n    pass = if_else(play_type == \"pass\", 1, 0),\n    total_runs = if_else(play_type == \"run\",\n                         cumsum(run) - 1, cumsum(run)),\n    total_pass = if_else(play_type == \"pass\",\n                         cumsum(pass) - 1, cumsum(pass)),\n    previous_play = if_else(posteam == lag(posteam),\n                            lag(play_type),\n                            \"First play of drive\"),\n    previous_play = if_else(is.na(previous_play),\n                            replace_na(\"First play of drive\"),\n                            previous_play)) %&gt;%\n  ungroup() %&gt;%\n  mutate(across(c(play_type, season, posteam, defteam,\n                  shotgun, down, qtr, no_huddle,\n                  posteam_timeouts_remaining,\n                  defteam_timeouts_remaining, in_red_zone,\n                  in_fg_range, previous_play, goal_to_go,\n                  two_min_drill), as.factor)) %&gt;%\n  select(-game_id, -game_date, -week, -play_type,\n         -yards_gained, -defteam, -run, -pass,\n         -air_yards, -run_location)\n\nWe use the mutate() and case_when() functions to create our response variables (run_inside, run_outside, short_pass, medium_pass, long_pass) by providing both the binary 1 and 0 arguments for the play_type. After, we create two more binary columns based on whether the called play was a rush or a pass and then use those two columns to calculate each team’s cumulative runs and passes for each game. We conclude the feature engineering process by using the lag() function to provide the previous play (or, to use “First play of drive” if there was no prior play).\nBefore moving on to building out the model, we use mutate(across(c()) to turn categorical variables into factors, as well as those numeric variables that are not continuous in nature. Variables that can take on just a limited number of unique values are typically made into a factor. For example, the previous_play variable is categorical and is only capable of being one of two values: run or pass. Because of this, it will be converted into a factor.\nDeciding which numeric variables to convert with as.factor() can be a bit more tricky. We can compare the down and half_seconds_remaining variables, as you see the first in the mutate() function but not the second. This is because down is not a continuous variable as it can only take on a specific number of unique values (1, 2, 3, or 4). On the other hand, half_seconds_remaining is not continuous as there is no rhyme or reason to how it appears in the data - or, in other hands, there is no specific amount by which the half_seconds_remaining decreases for each individual play (while there is an ordered way in how down changes).\nBecause we are focusing on completing a multinomial regression, we must now convert the various play type columns (run_inside, run_outside, etc.) from binary format and then unite all five into a single response variable column titled play_call.\n\nmodel_data &lt;- model_data %&gt;%\n  mutate(play_call = case_when(\n    run_outside == 1 ~ \"run_outside\",\n    run_inside == 1 ~ \"run_inside\",\n    short_pass == 1 ~ \"short_pass\",\n    medium_pass == 1 ~ \"medium_pass\",\n    long_pass == 1 ~ \"long_pass\",\n    TRUE ~ NA)) %&gt;%\n  select(-run_outside, -run_inside, -short_pass,\n         -medium_pass, -long_pass) %&gt;%\n  mutate(play_call = as.factor(play_call))\n\nmodel_data_clean &lt;- na.omit(model_data)\n\nBefore doing any feature engineering with the recipes package, we will first split the data into training and testing sets using the rsample package. The model is first trained on the training data and then evaluated on the testing data. This crucial first step ensures that the model is able to generalize well to any unseen data. As well, splitting the data helps in avoiding overfitting, which occurs when the model is able to recognize any random noise in the data. As a result, the model may perform very well on training data but then fail to reach acceptable evaluation metrics when fit on the withheld testing data.\nTo start the splitting process of the model_data_clean data frame, we first use set.seed() and pick a random number to run with it. The number inside set.seed() does not matter, as it is simply the initial value of what is otherwise a random-number as the rows are split - at random - into training and testing sets. However, using set.seed() allows for reproducible results in that conducting the split a second time, as the same initial value in set.seed() will result in an identical split in future splitting of the data.\nWe then use the initial_split function in rsample to create the multinom_play_split data frame. The initial_split process take the provided data frame and creates a single binary split of the information. With the initial_split() process completed, the data is divided into a multinom_play_train data frame and a multinom_play_test data frame using the training() and testing() functions within rsample to conduct the process on the multinom_play_split data. Last, we create folds of the data using the vfold_cv function in rsample that will allow use to conduct K-fold cross validation during the training process.\n\nset.seed(1984)\n\nmultinom_play_split &lt;- rsample::initial_split(model_data_clean,\n                                              strata = play_call)\nmultinom_play_train &lt;- rsample::training(multinom_play_split)\nmultinom_play_test &lt;- rsample::testing(multinom_play_split)\n\nset.seed(1958)\nmultinom_play_folds &lt;- rsample::vfold_cv(model_data_clean,\n                                         strata = play_call)\n\nThe recipes package within tidymodels, first and foremost, is where the formula for the model is passed into the eventful workflow. However, it also provides the opportunity for further preprocessing of the data by using the the step_ function that allows for multitudes of refinement, including imputation, individual transformations, discretization, the creation of dummy variables and encodings, interactions, normalization, multivariate transformations, filters, and row operations. The complete list of possible step_ functions can be fond on the reference section of the recipes website. We use several of these step_ functions below in preparing the recipes for our multinomial logisitic regression model.\n\n\n\n\n\n\nTip\n\n\n\nWhen building your recipe in the tidymodels framework, there is a general suggested order that your steps should be in to avoid issues with how the data becomes structured for the tuning/modeling process.\n\nImputation\nHandle factor levels\nTransformations\nDiscretize\nCreating dummy variables and (if needed) one-hot encoding\nCreating interactions between variables\nNormalization steps\nMultivariate transformations\n\nNot every recipe will require all of these steps, of course. But, when using more than one, it is important to follow the above order when building the recipe.\n\n\nWe provide the formula for our regression model (play_call ~ ., and then provide the name of our training data (multinom_play_train). We then apply the optional update_role() function to both the posteam and season variables from the data before adding additional steps into the recipe using the pipe operator.\n\n\nupdate_role() - by applying the update_role() function to the posteam and season variables, we are able to retain the information without including it in the model process allowing us to investigate the prediction on a season and team basis.\n\nstep_zv() - the step_zv() function will remove any variables that contain zero variance (that is, only a single value).\n\nstep_normalize() - in short, the step_normalize() function applies both the mean and the standard deviation from the split training set to the testing set. Doing so helps prevent data leakage during the process.\n\nstep_dummy() - the step_dummy() function creates a set of binary variables from the inputted factor.\n\n\nmultinom_play_recipe &lt;- \n  recipe(formula = play_call ~ ., data = multinom_play_train) %&gt;%\n  update_role(posteam, new_role = \"variable_id\") %&gt;%\n  update_role(season, new_role = \"variable_id\") %&gt;%\n  step_zv(all_predictors(),\n          -has_role(\"variable_id\")) %&gt;% \n  step_normalize(all_numeric_predictors(),\n                 - has_role(\"variable_id\")) %&gt;%\n  step_dummy(down, qtr, posteam_timeouts_remaining,\n             defteam_timeouts_remaining,\n             in_red_zone, in_fg_range,\n             two_min_drill, previous_play)\n\nWith the recipe for our model created, we can build the model itself and then pass both multinom_play_recipe and the below multinom_play_model into a combined workflow() created within the tidymodels framework.\n\nmultinom_play_model &lt;-\n  multinom_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"glmnet\", family = \"multinomial\")\n\nmultinom_play_workflow &lt;- workflow() %&gt;%\n  add_recipe(multinom_play_recipe) %&gt;%\n  add_model(multinom_play_model)\n\nThere are three important items provided to multinom_play_model: the model type, the mode, and the computational engine.\n\n\nModel type - the first argument provided is the specific type of model that will be used in the analysis. As of the writing of this book, the parsnip package within tidymodels provides access to 43 various model types. You can see the complete list on the tidymodels Explore Models page. In this case, we are using the multinom_reg() model type. In many cases, each type of each maintains its own page on the parsnip website that provides further details about the various engines that be used to run the model.\n\nMode - Because we are utilizing a multinomial regression, the only valid mode for the model is “classification.” The most common modes in the tidymodels universe are “classification” and “regression.” In those circumstances where a model’s mode can accept various arguments, the list of available options is provided on the engine’s website (such as “censored regression”, “risk regression”, and “clustering”).\n\nEngine - the engine provides instructions on how the model is to be fit. In many cases, the provide engine type results in tidymodels calling in outside R packages (such as randomForest or ranger) to complete the process. In the case of a multinom_reg() model, there are six available type of engines: nnet, brulee, glmnet, h20, keras, and spark. While not wanting to over complicate the process for first-time users of tidymodels, it is important to know that each engine type within a model type comes with various options for tuning parameters (if that is a desired part of your model design). Our model will be using the glmnet engine which provides the ability to tune for penalty (the amount of regularization within the model) and mixture (the proportion of Lasso Penalty, if desired, wherein a mixture of 1 results in a pure Lasso model, 0 results in a ridge regression and anything between is an elastic net model). Other engines, like brulee provide upwards of nine tuning parameters (such as epochs, learn_rate, momentum, and class_weights). The parameters you wish to tune during the training process are inputted when providing your model type. We will be conducting a tuning process for both penalty and mixture.\n\n\n\n\n\n\n\nNote\n\n\n\nYou will notice that we’ve included family = \"multinomial\" in the construction of our model’s engine, which makes since as the model we are using is a multinomial regression. However, the glmnet engine allows you to fit multiple family types, including gaussian, binomial, and Possion.\n\n\nBecause we provided specific tuning parameters in the model type, it is necessary to provide a grid for the process of finding the best hyperparameters - which are those parameters within the model (again, penalty and mixture) that are not provided in the original data frame. The use of these parameters ultimately control the learning process of the model and, as a result, can have a significant impact on the performance and accuracy of the final results. Because of this, we create what is called a “grid” in order to allow the model to run over and over again on a set range of each parameter, a process coined hyperparameter tuning. The end result is a model that has determined the best penalty and mixture.\nWe will use the crossing function from the tidyr package to manually create a grid for the hyperparameter tuning process.\n\nmultinom_play_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1,\n                                                       length.out = 20),\n                                      mixture = c(0.05, 0.2,\n                                                  0.4, 0.6,\n                                                  0.8, 1))\n\n\n\nPenalty - we are creating a sequence of 20 numbers that are evenly spaced on a log scale between 10^-6 and 10^-1. The training process will be conducted with each number in the penalty argument. A correctly tuned penalty allows us to avoid overfitting.\n\nMixture - to tune the mixture parameter, we create a simple sequence of 6 values between 0.05 and 1.\n\nThe model will use multinom_play_grid to train and evaluate against every possible combination of penalty and mixture as defined in our above arguments. With the hyperparameter tuning grid created, we now turn to actually running the model.\n\nall_cores &lt;- parallel::detectCores(logical = FALSE) - 1\n\nregisterDoParallel(all_cores)\n\nset.seed(1988)\n\nmultinom_play_tune &lt;- tune_grid(\n  multinom_play_workflow,\n  resample = multinom_play_folds,\n  grid = multinom_play_grid,\n  control = control_grid(save_pred = TRUE,\n                         verbose = TRUE))\n\ndoParallel::stopImplicitCluster()\n\n\n\n\n\n\n\nWarning\n\n\n\nThe process of using the paralell package is included in the above code.\nThe process is important if you want to speed up the training of your model as, by default, RStudio runs one just one core of your CPU. The processor in my computer, an Intel Core i5-10400F, has 6 cores. The paralell package allows us to provide more computational cores to RStudio, which in turns allows the modeling process to be conducting different tasks on different cores, then compiling the information back together when completed.\nWhen using the parallel package, our multinomial model took 25 minutes to complete on my computer. Out of curiosity, I spun up an Ubuntu server running R and RStudio in the Amazon cloud on an instance with 16 cores. Just this small increase in computing power dropped the model’s training time to under three minutes.\n\n\nAs mentioned, we are using the parallel package to provide the model as much computational power we can. We let the package discover the exact number of cores in the computer by using the detectCores function and then place this number in a vector called all_cores after subtracting one from it (as the computer will need one free core during the modeling process to stay operational). Next, we utilize the doParallel package to physically begin the process of parallel processing.\nFinally, we use the tune_grid() function, from tune housed within tidymodels, to begin the tuning process on our play-by-play data. At bare minimum, the tune_grid() function requires the model’s workflow and grid to successfully run. In our case, we are passing in multinom_play_workflow, our multinom_play_folds resample data to allow for K-fold cross validation, and multinom_play_grid. Additionally, we are requesting, via the control argument in grid, to save the model’s predictions and to provide verbose output in the Console regarding the current status of the tuning process. When the model completes the tuning process, be sure to run doParallel::stopImplicitCluster() to stop your computer from parallel processing.\n\n\n\n\n\n\nNote\n\n\n\nIf conducting the tuning process in a parallel processing environment, there will be not verbose output in the console regardless of how you set the argument. In many cases, even when parallel processing, I will set this option to TRUE, as the lack of verbose output is an indication itself that the model is properly tuning and still working.\n\n\nWhen the tuning process is complete, we use the show_best() function to retrieve the top 5 models as determined, in this case, by AUC (area under the curve). The output of show_best() includes important pieces of information pertaining to specific performance metrics for each tuning of the model.\n\n\npenalty and mixture- each respective column displays the value used from the hyperparameter tuning grid used in that specific model’s configuration. In model 1, the penalty was set to 0.000001 while the mixture used was 0.2.\n\n.metric - this column provides the type of performance metric being used to evaluate each model. As mentioned, we are used roc_auc to help in determining the best performing model.\n\n.estimator - the .estimator indicates the specific method used to calculate the .metric. In the case of our model using the glmnet engine, the “hand and till” estimator, a method described in Hand, Till (2001), was used to determine the area roc_auc.\n\nmean - this is the average of the roc_auc among those folds in the cross-validation with the same penalty and mixture configuration.\n\nn - this number represents the specific number of folds used in the cross validation process.\n\nstd_err - each model’s standard error metric.\n\n.config - the unique identifier for each model.\n\n\nshow_best(multinom_play_tune, metric = \"roc_auc\")\n\n`\n     penalty mixture .metric .estimator  mean     n  std_err .config               \n       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                 \n1 0.000001       0.2 roc_auc hand_till  0.668    10 0.000537 Preprocessor1_Model021\n2 0.00000183     0.2 roc_auc hand_till  0.668    10 0.000537 Preprocessor1_Model022\n3 0.00000336     0.2 roc_auc hand_till  0.668    10 0.000537 Preprocessor1_Model023\n4 0.00000616     0.2 roc_auc hand_till  0.668    10 0.000537 Preprocessor1_Model024\n5 0.0000113      0.2 roc_auc hand_till  0.668    10 0.000537 Preprocessor1_Model025`\n\nThere is very little difference among the five top-performing models from the tuning process. Among the five best, the mixture is the same (0.2), the mean is the same (0.668), as well as the n (10) and std_err (0.000537). The only difference between each model is found in the penalty, wherein a smaller number equates to a model with less regularization, allowing the model to fit more closely to the training data. On the other hand, a larger penalty value provides a model with more regularization, forcing the model to fit less closely to the training data. However, the amount of regularization in the model had no significant impact on any other performance metric, as they remained static across all models.\nRather than make an arbitrary choice in the model to take forward in the process, we will use the select_by_one_std_err() function to select the appropriate model. The function operates under the “one-standard-error-rule”, which argues that we should move forward with the model whose standard error is no more than one standard error above the error of the best model. Moreover, we can sort the models in descending order by penalty, thus following the principles of Occam’s Razor, as selecting the model with the largest penalty is essentially selecting simplest model we can based on the criteria of the “one-standard-error-rule.”\n\nfinal_penalty &lt;- multinom_play_tune %&gt;%\n  select_by_one_std_err(metric = \"roc_auc\", desc(penalty))\n\nAfter identifying the best model in the above step and storing it in final_penalty, we pass it along into the finalize_workflow(). This “finalized workflow” cancels out our previously constructed workflow() and replaces it with only our selected best model. We then use the last_fit() function to run the finalize_workflow() over the previously created multinom_play_split, which again fits the model on the training data and evaluates it on the withheld validation data.\n\nmultinom_final_results &lt;- multinom_play_workflow %&gt;%\n  finalize_workflow(final_penalty) %&gt;%\n  last_fit(multinom_play_split)\n\nWith the the finalize_workflow() fitted to our data, we can now make and collect predictions on both the training and test data frames and then combine both to explore the model’s performance.\n\nworkflow_for_merging &lt;- multinom_final_results$.workflow[[1]]\n\nmultinom_all_predictions &lt;- bind_rows(\n  augment(workflow_for_merging, new_data = multinom_play_train) %&gt;%\n    mutate(type = \"train\"),\n  augment(workflow_for_merging, new_data = multinom_play_test) %&gt;%\n    mutate(type = \"test\"))\n\nIn the first step, we are extracting the final workflow created in the prior step and placed the information into an object called workflow_for_merging. Next, we create a data frame called multinom_all_predictions that uses the augment function to make predictions on both the training and testing data using the workflow that holds the parameters of the best model. After creating a column called type that indicates whether the play was from the training or testing data, we combine both sets of predictions using the bind_rows() function.\nWith both sets of predictions combined, we can use the collect_predictions function to gather the predictions into a data frame that allows us to further evaluate how the model performed. Additionally, by piping into the roc_curve() function, we can calculate the curve for each class in our data frame (long_pass, short_pass, etc.). The resulting predictions_with_auc data frame includes the true positive and false positive rates for each different play type at various classification thresholds. Including the calculated ROC curve gives us the ability to visualize the model’s performance using the area under the curve (AUC).\n\npredictions_with_auc &lt;- collect_predictions(multinom_final_results) %&gt;%\n  roc_curve(truth = play_call, .pred_long_pass:.pred_short_pass)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe area under the curve (AUC) is a single-value of the model’s performance. In this case, we are calculating it for each play type classification. An AUC score ranges from 0 to 1, wherein a score of 1 (100%) means the model’s predictions were never incorrect. A score of 0.5 (50%) indicates that the model’s performance was no more predictive than flipping a coin to determine the response variable.\n\n\n\nggplot(data = predictions_with_auc, aes(1 - specificity,\n                                        sensitivity, color = .level)) +\n  geom_abline(slope = 1, color = \"black\", lty = 23, alpha = 0.8) +\n  geom_path(size = 1.5) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::number_format(accuracy = 0.01)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::number_format(accuracy = 0.01)) +\n  scale_color_brewer(palette = \"Paired\",\n                     labels = c(\"Long Pass\", \"Medium Pass\", \"Run Inside\",\n                                                    \"Run Outside\", \"Short Pass\")) +\n  coord_fixed() +\n  nfl_analytics_theme() +\n  theme(legend.background = element_rect(\"#F7F7F7\"),\n        legend.key = element_rect(\"#F7F7F7\")) +\n  xlab(\"1 - Specificity\") +\n  ylab(\"Sensitivity\")\n\n\n\n\n\nArea Under Curve (AOC) for the playtype predictive model\n\n\n\nThis visualization is a Receiver Operating Characteristic (ROC) curve, which is created by plotting the True Positive Rate against the False Positive Rate for each of the different play types in the data frame. The “arching” of the run_outside line towards the 0.80 mark indicates that the model is doing a commendable job at recognizing the patterns that result in the run_outside class against the others. The model had slightly more difficulty in distinguishing the other play types (long_pass, medium_pass, run_inside, and short_pass) from one another. But, each is still solidly above the line of no-discrimination (shown as the dashed, diagonal line in the plot) where the model’s performance would be no better than random guesses.\nWith reasonably good results given the complexity in both NFL offenses and decision making, we can use the results of our model to determine which teams over the course of the time period were most predictable. To start, we will create a data frame titled predictions_by_team from the combined multinom_all_predictions. Next, we need to switch .pred_class to as.character as well as play_call and posteam. After using summarize() to find the total_plays, the total_pred, and pred_pct for each team by season, we find that the 2017 New Orleans Saints were the most predictable team in the data, with the model selecting the right play type nearly 59% of the time.\n\npredictions_by_team &lt;- multinom_all_predictions %&gt;%\n  mutate(.pred_class = as.character(.pred_class),\n         play_call = as.character(play_call),\n         posteam = as.character(posteam)) %&gt;%\n  group_by(season, posteam) %&gt;%\n  summarize(total_plays = n(),\n            total_pred = sum(.pred_class == play_call),\n            pred_pct = total_pred / total_plays * 100) %&gt;%\n  ungroup()\n\npredictions_by_team &lt;- as.data.frame(predictions_by_team)\n\nFinally, to conclude the process of designing a multinomial regression model, we can use the data we just created in predictions_by_team and use knowledge we’ve gained in the Data Visualization chapter to place the results into a visually appealing table using gt and gtExtra.\n\nteams &lt;- nflreadr::load_teams(current = TRUE)\n\npredictions_by_team &lt;- predictions_by_team %&gt;%\n  left_join(teams, by = c(\"posteam\" = \"team_abbr\"))\n\noptions(digits = 3)\n\npredictions_by_team %&gt;%\n  dplyr::select(season, team_logo_wikipedia,\n                total_plays, total_pred, pred_pct) %&gt;%\n  dplyr::arrange(-pred_pct) %&gt;%\n  dplyr::slice(1:10) %&gt;%\n  gt() %&gt;%\n  tab_spanner(\n    label = \"Most Predictable Offenses: 2006 - 2022\",\n    columns = c(\"season\", \"team_logo_wikipedia\",\n                \"total_plays\", \"total_pred\", \"pred_pct\")) %&gt;%\n  cols_label(\n    season = \"Season\",\n    team_logo_wikipedia = \"\",\n    total_plays = \"# of Plays\",\n    total_pred = \"# Predicted\",\n    pred_pct = \"Percent Predicted\") %&gt;%\n  cols_align(align = \"center\", columns = everything()) %&gt;%\n  gt_img_rows(columns = team_logo_wikipedia, height = 25) %&gt;%\n  gt_color_rows(pred_pct, palette = \"ggsci::blue_material\") %&gt;%\n  gtExtras::gt_theme_espn()\n\n\n\n\n\n\n\nMost Predictable Offenses: 2006 - 2022\n\n\nSeason\n\n# of Plays\n# Predicted\nPercent Predicted\n\n\n\n\n2017\n\n925\n543\n58.7\n\n\n2017\n\n833\n485\n58.2\n\n\n2014\n\n929\n525\n56.5\n\n\n2018\n\n912\n497\n54.5\n\n\n2015\n\n891\n485\n54.4\n\n\n2009\n\n913\n496\n54.3\n\n\n2014\n\n998\n535\n53.6\n\n\n2009\n\n932\n499\n53.5\n\n\n2014\n\n964\n513\n53.2\n\n\n2015\n\n935\n497\n53.2"
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#advanced-model-creation-with-nfl-data",
    "href": "05-nfl-analytics-advanced-methods.html#advanced-model-creation-with-nfl-data",
    "title": "\n5  Advanced Model Creation with NFL Data\n",
    "section": "\n5.3 Advanced Model Creation with NFL Data",
    "text": "5.3 Advanced Model Creation with NFL Data\n\n5.3.1 K-means Clustering\nThe K-means technique, originally titled as “bagging predictors,” was formulated by Hugo Steinhaus, a Polish mathematician, in 1956 and was introduced in his paper “Sur la Division des Corps Materielsen Parties.” The modern and refined version of the algorithm that is widely-used today was developed a year later by Stuart Lloyd, an electrical engineer employed at Bell Labs. The process was dubbed with the K-means moniker in 1967.\nIn short, K-means is an unsupervised machine learning algorithm that is used for the clustering process. The algorithm aims to find similarities between data points and then group, or cluster, them into the same class (wherein “K” is the defined number of clusters for the data to be grouped into). The assumption that underpins the K-means process is that a closer data point to the center of the cluster is more related than a data point that is at a further distance, with this measure of closeness typically determined through the use of Euclidean distance.\nThe K-means process is used in a wide range of applications, including in business to segment customers into different groups based on their purchase history or used to help develop markets for advertising and promotional efforts. In this example, we will be conducting the process on running backs from the 2022 NFL season (with at least 50 rushing attempts) over various different metrics collected from both Sports Info Solutions and Pro Football Focus, including:\n\nthe player’s elusive rating from PFF (success of runner independent of blocking)\nthe number of broken and/or missed tackles created by the runner\nthe amount of runs that resulted in first downs\nthe number of times the runner was stuffed at the line\nthe number of times the runner used the play’s designed running gap\nthe player’s boom percent (rushes with an EPA of at least 1)\nthe player’s bust percent (rushes with an EPA of less than -1)\n\nBefore jumping into the K-means process, a principal component analysis (PCA) is conducted on the data frame. It is common practice to do so, as the PCA process reduces the dimensionality in the data resulting in noise reduction. The PCA process takes our provided data frame and converts it into linearly correlated sets of values called principal components. The first principal component produced (PC1) accounts for the largest amount of variance in the data, while PC2 accounts for the next largest amount with the continued results being cumulative in nature. That is, if PC1 accounts for 25% of the variance in the data and PC2 accounts for 15%, the cumulative variance explained between the two principal components is 40%.\nThe results of conducting a PCA and a K-means on the running backs data is a division of the players into groups, where those players in the same group have similar attributes based on the variables we have collected from SIS and PFF. This clustering process allows us to produce a quantitative and data-driven way to both understand and compare running back attributes and tendencies from the 2022 season.\n\n5.3.1.1 Principal Component Analysis\nThe Principal Component Analysis process is a common part of data preprocessing prior to conducting the k-means analysis, allowing us to achieve several results with the data.\n\n\nVariable scaling - In many cases, the variables within a data frame will be in different units (such as in our case, as some of the statistics are provided in whole numbers, like attempts and touchdowns, while others are in “per attempt” like fumbles and designed gap.). Because of this, scaling the data ensures that each of the variables - regardless of numeric unit - contributes equally to the K-means process. Otherwise, it is possible that one variable could become dominant, thus introducing bias into the process. The scaling process computes the data frame so that each variable has a standard deviation of one and a mean of zero, with the equation to produce these results below.\n\n\\[\n\\frac{x-1 - \\text{mean}(x)}{\\text{sd}(x)}\n\\]\n\n\nDimensionality Reduction - Our data frame has 12 variables, with each of these serving as a dimension. The PCA process seeks to find a simpler way to represent all 12 of these variables, while still capturing as much as possible of the original information. These simplified dimensions are outputted into numbered principal components (Principal Component 1, Principal Component 2, etc.) with each additional one adding to the cumulative total of variance explained in the data.\n\nNoise Reduction - Because the principal components outputted in the PCA process are ranked in the order of variance explained, the higher-numbered ones that provide little cumulative impact are typically the ones holding the noise and other minor nuances in the data. To provide a more robust data frame to the K-means process, we can drop these components that contain unhelpful data.\n\nTo begin this process, first download the rushing_kmeans_data. After, we create a vector of each rusher’s name and unique ID number from the rushing_kmeans_data data frame, as we will want to use the names as the data frame’s row names during the data visualization process and have the player’s ID to merge in further data, if needed (such as headshot URLs). After, we create a data frame titled rushers_pca that is a copy of rushing_kmeans_data but using filter() to drop both player and player_id, and then add each name back into the data frame using the rownames function. Now, rather than each row being associated by a number, each rusher’s name will be the identification method.\n\nrushing_kmeans_data &lt;- vroom(\"http://nfl-book.bradcongelio.com/kmeans-data\")\n\n\nrusher_names &lt;- rushing_kmeans_data$player\nrusher_ids &lt;- rushing_kmeans_data$player_id\n\nrushers_pca &lt;- rushing_kmeans_data %&gt;%\n  select(-player, -player_id)\n\nrownames(rushers_pca) &lt;- rusher_names\n\nrushers_pca &lt;- prcomp(rushers_pca, center = TRUE, scale = TRUE)\n\nAfter that brief bit of data cleaning and preparation, we use the prcomp function from the stats package to both center and scale the variables in the data frame. The results of the PCA process includes the first look at relationship between all of our variables.\n\n\nStandard deviations (1, .., p=7):\n[1] 1.583 1.312 0.987 0.869 0.731 0.546 0.456\n\nRotation (n x k) = (7 x 7):\n                  PC1      PC2     PC3     PC4     PC5      PC6\nelusive_rating -0.557  0.04196 -0.0755  0.2275 -0.3645  0.03989\nbroken_missed  -0.552 -0.00659 -0.1902  0.2322 -0.3176  0.10692\nfd_rush        -0.178 -0.56078 -0.0400 -0.6319 -0.0425  0.49942\nstuff_percent  -0.314  0.52663  0.0670 -0.0239  0.6037  0.50434\ndesigned_gap   -0.145 -0.08014  0.9750  0.0548 -0.1115  0.00318\nboom_percent   -0.461 -0.33119 -0.0254 -0.0885  0.5526 -0.60328\nbust_percent   -0.147  0.53876  0.0258 -0.6954 -0.2866 -0.34533\n                    PC7\nelusive_rating -0.70437\nbroken_missed   0.70196\nfd_rush        -0.04228\nstuff_percent  -0.01936\ndesigned_gap    0.08065\nboom_percent   -0.00129\nbust_percent    0.04956\n\n\nFirst, the results provide the standard deviations for each variable as well as the Rotation in which a number is provided to show how much each variable contributed to each principal component. For example, bust_percent and designed_gap both contributed very little to PC1 with values of -0.14, while elusive_rating had the largest contribution with a value of -0.55. A further examination of the results show that PC1 is perhaps a measure of a running back’s ability to use his elusive ability to break tackles and break out of the backfield for big gains. Further, PC2 has a strong contribution from fd_rush (-0.56) and very little from broken_missed (-0.006), suggesting that PC2 is favoring those running backs that are regularly gaining short yardage on the ground and getting first downs, but not producing much EPA (based on the high contribution from bust_percent).\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that a negative sign does that indicate “negative influence,” or a a lack of. Rather, a negative sign means that the specific variable is inversely related to the principal component. In PC3, for example, elusive_rating has a value of -0.399, which means that as the value of elusive_rating decreases, the values of PC3 increases.\n\n\nWe can also visualize these results using the factoextra package which provides the ability to extract and visualize the output of various types of multivariate data analyses, including a Principal Component Analysis. Using factoextra, we can explore the results in three different arrangements: the individual data points (in this case, the play callers) using fviz_pca_ind(), the variables using fviz_pca_var(), or a combination of both using fviz_pca_biplot(). As an example, let’s construct a biplot that shows PC1 on the x-axis and PC2 on the y-axis along with the positioning of the variables and rusher names.\n\nfviz_pca_biplot(rushers_pca, geom = c(\"point\", \"text\"),\n                ggtheme = nfl_analytics_theme()) +\n  xlim(-6, 3) +\n  labs(title = \"**PCA Biplot: PC1 and PC2**\") +\n  xlab(\"PC1 - 35.8%\") +\n  ylab(\"PC2 - 24.6%\")\n\n\n\nUsing a biplot to view the relationship between principal components\n\n\n\nThe resulting plot contains a few unique pieces of information regarding the principal component analysis. The information on each axis provides a numeric value regarding how much variance each principal component accounts for. In this case, PC1 captured 35.8% of the variance while PC2 captured 24.6%. Between both, a cumulative total of 60.4% of the variance was explained.\nEach point on the plot, represented by the rusher’s name, is a single sample in the reduced-dimensionality space created with the PCA process. The positioning of each rusher is determined with values from the principal components, where each value is a weighted average of the original variable for each sample. In other words, the placement of each runner is dependent on their traits and attributes. The arrows and associated variable labels are also representative of the reduced-dimensionality space with the direction and length of each arrow being determined by the contribution to the variable for each principal component.\n\n5.3.1.2 Determining the Amount of K\nIn our biplot, we determined that PC1 and PC2 accounted for 60.4% of the variance in the data. The total number of principal components we select, based on cumulative variance explained, will be the number of clusters the running backs are grouped into during the K-means process. To get a better understanding of the amount being explained, we can use the get_eigenvalue function from factoextra.\n\nget_eigenvalue(rushers_pca)\n\n      eigenvalue variance.percent cumulative.variance.percent\nDim.1      2.507            35.81                        35.8\nDim.2      1.722            24.61                        60.4\nDim.3      0.974            13.92                        74.3\nDim.4      0.755            10.79                        85.1\nDim.5      0.534             7.63                        92.8\nDim.6      0.299             4.27                        97.0\nDim.7      0.208             2.98                       100.0\n\n\nThe output provides the amount of variance explained by each principal (Dim) along with the cumulative value. We can see what we already discovered about PC1 (35.8%) and PC2 (24.6%) with the cumulative total of the two being correct at 60.4%. Going further, PC3 explains 13.9% more of the variance in the data, bringing the cumulative total to 74.3%. Adding PC4 brings the cumulative total to 85.1%. The fifth principal component brings the cumulative total to over 90%, with the remaining two (PC6, PC7) more than likely accounting for noise and small nuances within the data.\nUnfortunately, there is no widely-accepted process to decide with certainty how many components are enough. A popular method, however, is called the “elbow method” or the Scree plot wherein the “elbow of the curve” reflects the correct number of components to select, as the resulting cumulative amount of variance explained is not worth the noise brought in.\nWe can product a Scree plot using the fviz_eig function.\n\nfviz_eig(rushers_pca, addlabels = TRUE,\n         ggtheme = nfl_analytics_theme()) +\n  xlab(\"Principal Component\") +\n  ylab(\"% of Variance Explained\") +\n  labs(title = \"**PCA Analysis: Scree Plot**\")\n\n\n\nUsing a Scree plot to help determine the correct number of components\n\n\n\nIn the plot, each bar represents the amount of variance explained for each principal component with the included numeric value at top. In this case, there is a fairly evident “elbow” in the plot at the third principal component. Compared to the rapid growth in the cumulative gain in the first three principal components, the final six provide diminishing returns.\nWe can view the contribution to the variables of each component in a combined manner using the fviz_contrib function and then arranging the results together using the plot_grid function from cowplot.\n\npc1 &lt;- fviz_contrib(rushers_pca, choice = \"var\", axes = 1)\npc2 &lt;- fviz_contrib(rushers_pca, choice = \"var\", axes = 2)\npc3 &lt;- fviz_contrib(rushers_pca, choice = \"var\", axes = 3)\n\nplot_grid(pc1, pc2, pc3)\n\n\n\nViewing the contribution of each variable to each component\n\n\n\nThe results mirror the information we gathered from Rotation when we first explored the results of rusher_pca. However, the plots include the expected average contribution of each variable as represented by the dashed line. For each principal component, a variable that has a contribution higher than this average contribution is considered to be important in explaining the variance within that component.\nWith the PCA completed and knowing that four components account for an acceptable amount of variance in the data, the K-means process can be conducted.\n\n5.3.1.3 Conducting the K-means Process\nAs a result of the PCA process, we know we will be grouping the running back into four distinctive clusters, so we will create a value in our environment called k and set it to 3.\n\nk &lt;- 3\n\nWe then must extract the values created during the PCA process and write them into a new data frame in order to pass the information into the kmeans() function. This information is stored within rushers_pca$x.\n\npca_scores &lt;- rushers_pca$x\n\nWe write the PCA results into a data frame called pca_scores and are now able to conduct the K-means process on the data, passing the correct number of K into the function’s required arguments.\n\nset.seed(1928)\nrushing_kmeans &lt;- kmeans(pca_scores, centers = k)\n\nCompared to other models in this chapter, a K-means process completes almost instantly. After, the created rushing_kmeans provides access to eight components pertaining to the results, including cluster, centers, totss, withinss, tot.withinss, betweenss, size, iter, and ifault. For our purposes, we can view the cluster assigned to each running back. After, we write this information out into a data frame called cluster_assignment and then merge it back into the original data frame.\n\nrushing_kmeans$cluster\n\n      Derrick Henry         Josh Jacobs      Saquon Barkley \n                  3                   1                   2 \n         Nick Chubb       Miles Sanders Christian McCaffrey \n                  1                   2                   1 \n        Dalvin Cook        Najee Harris     Jamaal Williams \n                  3                   3                   2 \n    Ezekiel Elliott      Travis Etienne           Joe Mixon \n                  2                   1                   2 \n Kenneth Walker III        Alvin Kamara       Dameon Pierce \n                  1                   3                   1 \n      Austin Ekeler        Tony Pollard         Aaron Jones \n                  3                   1                   1 \n     Tyler Allgeier Rhamondre Stevenson       Isiah Pacheco \n                  1                   1                   2 \n Brian Robinson Jr.      D'Onta Foreman    David Montgomery \n                  2                   2                   3 \n  Leonard Fournette    Devin Singletary     Jonathan Taylor \n                  3                   2                   3 \n          Cam Akers     Jeff Wilson Jr.         A.J. Dillon \n                  3                   2                   2 \n       James Conner      Raheem Mostert     Latavius Murray \n                  2                   1                   2 \n\ncluster_assignment &lt;- rushing_kmeans$cluster\n\nrushing_kmeans_data$cluster &lt;- cluster_assignment\n\nThe results of the K-means process placed, for example, Derrick Henry, Josh Jacobs, and Nick Chubb all into cluster 2 and Najee Harris, Tony Pollard, Aaron Jones into cluster 3. To better visualize the traits and attributes associated with each cluster, we can conduct some preparation and then produce a plot.\n\nkmean_dataviz &lt;- rushing_kmeans_data %&gt;%\n  rename(c(\"Elusiveness\" = elusive_rating,\n           \"Broken/Missed\" = broken_missed,\n           \"1st Downs\" = fd_rush,\n           \"Stuffed\" = stuff_percent,\n           \"Desi. Gap\" = designed_gap,\n           \"Boom %\" = boom_percent,\n           \"Bust %\" = bust_percent))\n\nkmean_dataviz &lt;- kmean_dataviz %&gt;%\n  mutate(cluster = case_when(\n    cluster == 1 ~ \"Cluster 1\",\n    cluster == 2 ~ \"Cluster 2\",\n    cluster == 3 ~ \"Cluster 3\"))\n\nkmean_data_long &lt;- kmean_dataviz %&gt;%\n  gather(\"Variable\", \"Value\", -player, -player_id, -cluster)\n\n\nggplot(kmean_data_long, aes(x = Variable, y = Value, color = cluster)) +\n  geom_point(size = 3) +\n  facet_wrap(~ cluster) +\n  scale_color_brewer(palette = \"Set1\") +\n  gghighlight(use_direct_label = FALSE) +\n  nfl_analytics_theme() +\n  theme(axis.text = element_text(angle = 90, size = 8),\n        strip.text = element_text(face = \"bold\"),\n        legend.position = \"none\")\n\n\n\nVisualizing the results of the k-means clustering process\n\n\n\nWhen visualized, we are able to see and note the differences in the attributes and traits among the clustered running backs.\n\n\nCluster 1 - “Feast or Famine”\n\nHighest contribution in 1st down runs\nAverage broken/missed tackles\nHigh use of designed gap\nJust as likely to boom as bust per run\nMost elusiveness among the clusters\nGets stuffed at line more than others\n\n\n\nCluster 2 - “Blue Collar Backs”\n\nAlso high contribution in 1st down runs\nDoes not break or cause missed tackles\nNot very elusive running backs\nRarely gets stuffed at the line of scrimmage\n\n\n\nCluster 3 - “Cautious Carriers”\n\nLow contribution in 1st down runs\nLow boom percentage\nHigh bust percentage\nAverage elusiveness\nOccasionally gets stuffed at the line of scrimmage\n\n\n\nTo complete the process, we can take the completed model and transition the data into a presentable gt table. Before creating it, though, we must complete a bit of data preparation, such as merging in player headshots and team logos.\n\nroster &lt;- nflreadr::load_rosters(seasons = 2022) %&gt;%\n  select(pff_id, team, headshot_url) %&gt;%\n  mutate(pff_id = as.numeric(as.character(pff_id)))\n\nteams &lt;- nflreadr::load_teams(current = TRUE) %&gt;%\n  select(team_abbr, team_logo_wikipedia)\n\ngt_table_data &lt;- rushing_kmeans_data %&gt;%\n  left_join(roster, by = c(\"player_id\" = \"pff_id\"))\n\ngt_table_data &lt;- na.omit(gt_table_data)\n\ngt_table_data &lt;- gt_table_data %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\ngt_table_data &lt;- gt_table_data %&gt;%\n  select(player, cluster, headshot_url, team_logo_wikipedia)\n\ngt_table_data &lt;- gt_table_data %&gt;%\n  mutate(cluster = case_when(\n    cluster == 1 ~ \"Cluster 1 - Feast or Famine\",\n    cluster == 2 ~ \"Cluster 2 - Blue Collar Backs\",\n    cluster == 3 ~ \"Cluster 3 - Cautious Carriers\"))\n\nFirst, we use the load_rosters and load_teams function from nflreadr to bring in the necessary columns that contain the information for player headshots and team logos (matching on pff_id for roster information and the team column to bring in logos). Unfortunately, a small number of the running backs are missing headshot URLS1. Rather than including missing images in the table, we will use na.omit() to drop these players. Next, we create a data frame called gt_table_data that includes just the player, cluster, headshot_url, and team_logo_wikipedia. Last, we format the cluster information from numeric values to a character-based description based on the previous created titles for each.\nWith the data addition and preparation completed, we can use both the gt and gtExtras package to create the table.\n\ngt_table_data %&gt;%\n  mutate(cluster = fct_relevel(cluster,\n                               c(\"Cluster 1 - Feast or Famine\",\n                                 \"Cluster 2 - Blue Collar Backs\",\n                                 \"Cluster 3 - Cautious Carriers\"))) %&gt;%\n  arrange(cluster, player) %&gt;%\n  gt(groupname_col = \"cluster\") %&gt;%\n  tab_spanner(label = \"Clustered Running Backs\",\n              columns = c(\"player\", \"headshot_url\",\n                          \"team_logo_wikipedia\")) %&gt;%\n  cols_align(align = \"center\",\n             columns = everything()) %&gt;%\n  cols_label(headshot_url = \"\", team_logo_wikipedia = \"Team\") %&gt;%\n  gt_img_rows(columns = team_logo_wikipedia, height = 25) %&gt;%\n  gt_img_rows(columns = headshot_url, height = 25) %&gt;%\n  cols_label(player = \"Running Back\") %&gt;%\n  tab_source_note(source_note =\n                    md(\"**An Introduction to NFL Analytics with R**&lt;br&gt;*Brad J. Congelio*\")) %&gt;%\n  gtExtras::gt_theme_pff()\n\n\n\n\n\n\n\n\n\n\n\n\nClustered Running Backs\n\n\nRunning Back\n\nTeam\n\n\n\n\nCluster 1 - Feast or Famine\n\n\nAaron Jones\n\n\n\n\nChristian McCaffrey\n\n\n\n\nJosh Jacobs\n\n\n\n\nNick Chubb\n\n\n\n\nRaheem Mostert\n\n\n\n\nRhamondre Stevenson\n\n\n\n\nTony Pollard\n\n\n\n\nTravis Etienne\n\n\n\n\nCluster 2 - Blue Collar Backs\n\n\nA.J. Dillon\n\n\n\n\nD'Onta Foreman\n\n\n\n\nDevin Singletary\n\n\n\n\nEzekiel Elliott\n\n\n\n\nJamaal Williams\n\n\n\n\nJames Conner\n\n\n\n\nJeff Wilson Jr.\n\n\n\n\nJoe Mixon\n\n\n\n\nLatavius Murray\n\n\n\n\nMiles Sanders\n\n\n\n\nSaquon Barkley\n\n\n\n\nCluster 3 - Cautious Carriers\n\n\nAlvin Kamara\n\n\n\n\nAustin Ekeler\n\n\n\n\nCam Akers\n\n\n\n\nDalvin Cook\n\n\n\n\nDavid Montgomery\n\n\n\n\nDerrick Henry\n\n\n\n\nJonathan Taylor\n\n\n\n\nLeonard Fournette\n\n\n\n\nNajee Harris\n\n\n\n\n\n\nAn Introduction to NFL Analytics with RBrad J. Congelio\n\n\n\n\nResult of the running back k-means process\n\n\n\n5.3.2 Creating an XGBoost Model for Rushing Yards over Expected\nBack in January of 2021, Tej Seth posted an article to the Michigan Football Analytics Society blog that outlined his vision for creating a “public expected rushing yards model.” The structure of his model, as explained by Seth, was inspired by the prior work of Michael Egle, an honorable mention in both the 2021 and 2023 NFL Big Data Bowls, who previously used the college football equivalent of open-source data (cfbfastR) to create an RYOE model for the collegiate game.2 In Tej’s case, his approach to creating an NFL-centric RYOE model culminated with the creation of his RYOE Shiny App that allows anybody to explore his RYOE metric by season or team and even through three-way rusher comparisons.\nDespite a slightly intimidating title, rushing yards over expected is a fantastic entry point into exploring model creation and analysis in NFL analytics - in fact, the growing number of “over expected” metrics in the NFL are all great ways to begin learning about and understanding advanced modeling. Robby Greer, the owner of nfeloapp.com - a website that provides “data-driven analytics, picks, and predictions for the NFL” - explains that over expected metrics are a increasingly popular avenue in which analysts can “paint a more accurate picture of performance by adjusting familiar statistics like ‘completion percentage’ or ‘yards per rush’ for conflating factors like degree of difficulty or game text” (Greer, 2022).\nSome of these metrics, like completion percentage over expected (CPOE), are widely accepted. Specifically, CPOE calculates how likely any quarterback’s pass is going to be complete or incomplete compared to other passing attempts. It is considered “widely accepted” because the metric itself is considered “stable” in that the R Squared retains a strong correlation for individual quarterbacks across several seasons. In fact, as Greer points out, the R Squared value for CPOE for just one season is 0.226 which is extremely strong based on NFL analytics standards.\nOn the other hand, RYOE - based on Greer’s analysis - maintains an R Squared value below 0.15 until a running back’s fourth season, wherein the average improves to 0.263 (an otherwise stable value). But that does not mean that RYOE is not a metric worth further exploration. The effectiveness of any one metric to account for factors such as degree of difficulty or game text largely relies on our ability to provide adequate feature engineering - specifically, how much relevant data the machine learning model can ingest to begin making predictions.\nBecause of that, significant machine learning models have been built with information provided by the NFL’s Big Data Bowl as it is the one chance that the public receives to feature engineer with the NFL’s tracking data (which provides a player’s position, speed, direction, etc. via tracking data that is recorded every 1/10th of a second). Unfortunately, only small windows of data exist from the Big Data Bowl releases and, as a result, we are often required to find creative ways to provide further context to each play/player for the machine learning model.\nTo showcase this idea, we are going to begin exploring ways to add additional feature engineering to Tej Seth’s already fantastic Rushing Yard Over Expected model. While not the most stable metric, as mentioned, the idea of RYOE is generally easy to understand for even the most novice analyst. Broadly, given what we know about every other rushing play during a specific span of seasons, what is the most likely amount of yards a running back is going to gain on a specific rushing play as predicted by the model on other similar situations?\nThat difference is rushing yards over expected.\nUsing Tej’s Shiny app, we can explore all seasons between 2015 and 2022 for those running backs that had a minimum of 755 rushing attempts.\n\n\n\n\nExample results from Tej Seth’s RYOE model\n\n\n\nAccording to Tej’s model, since 2015, Nick Chubb of the Cleveland Browns earned - on average - 0.66 over expected. Aaron Jones is closely behind with 0.61 over expected and then a significant drop occurs for the third and fourth players.\nTo understand how Tej engineered his model and to begin exploring other possible features to feed into the model, we can dive into his publicly available code.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to immediately point out that Tej built his RYOE model using the xgboost package whereas we will construct ours using tidymodels.\nWhile the underlying eXtreme Gradient Boosting process is largely the same with both approaches, the necessary framework we will construct with tidymodels differs greatly from the coding used with the xgboostpackage.\nThe xgboost package is a standalone package in R that provide an implementation of the eXtreme Gradient Boosting algorithm. To that end, it offers a highly efficient and flexible way to train gradient boosting models for various machine learning tasks, such as classification, regression, and ranking. The package provides its own set of functions for training, cross-validation, prediction, and feature importance evaluation.\n\n\nJust like the model we will be building in this chapter, Tej constructed his model via eXtreme Gradient Boosting.\nWhich may lead to a very obvious question if you are new to machine learning: what exactly is eXtreme Gradient Boosting?\n\n5.3.2.1 eXtreme Gradient Boosting Explained\neXtreme Gradient Boosting is a powerful machine learning technique that is particularly good at solving supervised machine learning problems, such as classification (categorizing data into classes, for example) and regression (predicting numerical values).\neXtreme Gradient Boosting can be thought of as an “expert team” that combines the knowledge and skills of multiple “individual experts” to make better decisions or predictions. Each of these “experts” in this context is what we call a decision tree, which is a flowchart structure used for making decisions based on a series of question about the data.\nOnce provided data, XGBoost seeks to iteratively build a collection of “bad” decision trees and then build an ensemble of these poor ones into a more accurate and robust model. The term “gradient” comes from the fact that the algorithm uses the gradient (or the slope) of the loss function (a measure of how well the model fits the data) to guide the learning process.\n\n5.3.2.2 eXtreme Gradient Boosting with Tidymodels\nAs always, the first step in the modeling process is gathering the data and conducting the necessary feature engineering. In the case of our XGBoost model, we will gather play-by-play data from the 2016-2022 season and begin the work of creating additional metrics from the information contained in the data.\n\n5.3.2.2.1 Data Preparation and Feature Engineering\n\npbp &lt;- nflreadr::load_pbp(2016:2022)\n\nrush_attempts &lt;- pbp %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  filter(rush_attempt == 1, qb_scramble == 0,\n         qb_dropback == 0, !is.na(yards_gained))\n\ndef_ypc &lt;- rush_attempts %&gt;%\n  filter(!is.na(defteam)) %&gt;%\n  group_by(defteam, season) %&gt;%\n  summarize(def_ypc = mean(yards_gained))\n\nrush_attempts &lt;- rush_attempts %&gt;%\n  left_join(def_ypc, by = c(\"defteam\", \"season\"))\n\nAfter gathering the play-by-play data from 2016-2022 and doing a bit of preparation, we conduct our first bit of feature engineering by determining the average yards per carry allowed by each defense by season and then merge the results back into the main rush_attempts data frame.\nAside from the typical contextual variables such as down, yards to go, score, time remaining, etc., we can use the load_participation function from nflreadr to include information regarding what formation both the offense and defense were in, per play, as well as the personnel on the field for each and the total number of defenders in the box.\n\nparticipation &lt;- nflreadr::load_participation(seasons = 2016:2022) %&gt;%\n  select(nflverse_game_id, play_id, possession_team, offense_formation,\n         offense_personnel, defense_personnel, defenders_in_box)\n\nrush_attempts &lt;- rush_attempts %&gt;%\n  left_join(participation, by = c(\"game_id\" = \"nflverse_game_id\",\n                                  \"play_id\" = \"play_id\",\n                                  \"posteam\" = \"possession_team\"))\n\nAfter collecting the information for each play_id in the data from 2016-2022, we again use left_join() to bring it into the rush_attempts data frame, joining by the matching game_id, play_id, and posteam. Before continuing with the feature engineering process, we will create a secondary data frame to work with called rushing_data_join that will allow us to bring player names back into the data after the modeling process is complete.\n\nrushing_data_join &lt;- rush_attempts %&gt;%\n  group_by(game_id, rusher, fixed_drive) %&gt;%\n  mutate(drive_rush_count = cumsum(rush_attempt)) %&gt;%\n  ungroup() %&gt;%\n  group_by(game_id, rusher) %&gt;%\n  mutate(game_rush_count = cumsum(rush_attempt)) %&gt;%\n  mutate(rush_prob = (1 - xpass) * 100,\n         strat_score = rush_prob / defenders_in_box,\n         wp = wp * 100) %&gt;%\n  ungroup() %&gt;%\n  mutate(red_zone = if_else(yardline_100 &lt;= 20, 1, 0),\n         fg_range = if_else(yardline_100 &lt;= 35, 1, 0),\n         two_min_drill = if_else(half_seconds_remaining &lt;= 120, 1, 0)) %&gt;%\n  select(label = yards_gained, season, week, yardline_100,\n         quarter_seconds_remaining, half_seconds_remaining,\n         qtr, down, ydstogo, shotgun, no_huddle,\n         ep, wp, drive_rush_count, game_rush_count,\n         red_zone, fg_range, two_min_drill,\n         offense_formation, offense_personnel,\n         defense_personnel, defenders_in_box,\n         rusher, rush_prob, def_ypc, strat_score,\n         rusher_player_id, posteam, defteam) %&gt;%\n  na.omit()\n\nThere are multiple new features being created within our data in the above code:\n\nWe first group the data by game_id, rusher, and fixed_drive and then create a new column titled drive_rush_count. This column calculated, using the cumsum() function the cumulative total of rushes, per running back, on each the offensive drives in the game.\nThe pre-snap probability that the play is going to be a rush is calculated in the created rush_prob column that uses the xpass variable to determine the likelihood. The strat_score column attempts to quantify the team’s decision to run based on the just calculated rush_prob against the total number of defenders in the box.\nFinally, numeric 1 and 0 values are provided based on if the offense is in the red_zone, in fg_range, or if it is in a two-minute drill scenario.\n\nWe can continue to add to the data frame using information from the next_gen_stats() function, specifically information pertaining to the percent of rushing attempts that had eight defenders in the box and each running back’s average time from handoff to the line of scrimmage.\n\nnext_gen_stats &lt;- load_nextgen_stats(seasons = 2016:2022,\n                                     stat_type = \"rushing\") %&gt;%\n  filter(week &gt; 0 & season_type == \"REG\") %&gt;%\n  select(season, week, player_gsis_id,\n         against_eight = percent_attempts_gte_eight_defenders,\n         avg_time_to_los)\n\nrushing_data_join &lt;- rushing_data_join %&gt;%\n  left_join(next_gen_stats,\n            by = c(\"season\", \"week\",\n                   \"rusher_player_id\" = \"player_gsis_id\")) %&gt;%\n  na.omit()\n\nLast, we will conduct a bit of engineering on both the offense_personnel and defense_personnel we previously added into the data. Currently, the information for each is structured, for example, as 1 RB, 1 TE, 3 WR. Instead, we can create a new column for each position with the numeric value indicating the number on the field for each play.\n\nrushing_data_join &lt;- rushing_data_join %&gt;%\n  mutate(\n    ol = str_extract(offense_personnel,\n                     \"(?&lt;=\\\\s|^)\\\\d+(?=\\\\sOL)\") %&gt;% as.numeric(),\n    rb = str_extract(offense_personnel,\n                     \"(?&lt;=\\\\s|^)\\\\d+(?=\\\\sRB)\") %&gt;% as.numeric(),\n    te = str_extract(offense_personnel,\n                     \"(?&lt;=\\\\s|^)\\\\d+(?=\\\\sTE)\") %&gt;% as.numeric(),\n    wr = str_extract(offense_personnel,\n                     \"(?&lt;=\\\\s|^)\\\\d+(?=\\\\sWR)\") %&gt;% as.numeric()) %&gt;%\n  replace_na(list(ol = 5)) %&gt;%\n  mutate(extra_ol = if_else(ol &gt; 5, 1, 0)) %&gt;%\n  mutate(across(ol:wr, as.factor)) %&gt;%\n  select(-ol, -offense_personnel)\n\nrushing_data_join &lt;- rushing_data_join %&gt;%\n  mutate(dl = str_extract(defense_personnel,\n                          \"(?&lt;=\\\\s|^)\\\\d+(?=\\\\sDL)\") %&gt;% as.numeric(),\n         lb = str_extract(defense_personnel,\n                          \"(?&lt;=\\\\s|^)\\\\d+(?=\\\\sLB)\") %&gt;% as.numeric(),\n         db = str_extract(defense_personnel,\n                          \"(?&lt;=\\\\s|^)\\\\d+(?=\\\\sLB)\") %&gt;% as.numeric()) %&gt;%\n  mutate(across(dl:db, as.factor)) %&gt;%\n  select(-defense_personnel)\n\nWe use the str_extract function to identify the abbreviation for each position, and then place the associated number with each with the specific play_id. The offensive personnel includes the additional output of including a column for offensive linemen, but only if there are more than six downed linemen on the line of scrimmage. In this case, we use the ol column to determine if there is an extra lineman on the field (indicated by the new extra_ol variable).\n\nrushing_data_join &lt;- rushing_data_join %&gt;%\n  filter(qtr &lt; 5) %&gt;%\n  mutate(qtr = as.factor(qtr),\n         down = as.factor(down),\n         shotgun = as.factor(shotgun),\n         no_huddle = as.factor(no_huddle),\n         red_zone = as.factor(red_zone),\n         fg_range = as.factor(fg_range),\n         two_min_drill = as.factor(two_min_drill),\n         extra_ol = as.factor(extra_ol))\n\nrushes &lt;- rushing_data_join %&gt;%\n  select(-season, -week, -rusher, -rusher_player_id,\n         -posteam, -defteam) %&gt;%\n  mutate(across(where(is.character), as.factor))\n\nIn the last step of data preparation, we use filter() to remove any plays that took place in overtime, turn the created binary variables into factors using as.factor() and then create a data frame called rushes to use in our model that excludes identifying information and turns any remaining character variables into factors as well.\n\n5.3.2.2.2 Model Creation in tidymodels\n\nTo begin creating the model, we will follow much the same steps as we did with our multinomial regression by following the tidymodels framework which needs split data, folds for cross-validation, a recipe, a model specification, a grid for hyperparameter tuning, and a workflow that combines all these elements. Using the rushes data frame, we will use the rsample package to create an initial split of the data and then produce the model’s training and testing data sets from that split. Lastly, we utilize the vfold_cv() function to create folds of the training data to pass into the hyperparameter tuning grid.\n\nset.seed(1988)\n\nrushing_split &lt;- initial_split(rushes)\nrushing_train &lt;- training(rushing_split)\nrushing_test &lt;- testing(rushing_split)\nrushing_folds &lt;- vfold_cv(rushing_train)\n\nWith the data split into the required components, we can create the recipe for our XGBoost model.\n\nrushing_recipe &lt;-\n  recipe(formula = label ~ ., data = rushing_train) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nThe formula in our recipe is label ~ . (which is what the rushing yards gained column was renamed) and all other columns in the data frame are the predictive variables. We are also using step_dummy to one hot encode all the model’s nominal predictors, which are those variables that contain two or more categories but do not contain any intrinsic order.\nThe next required component to push into the model’s workflow() is the model specification.\n\nrushing_specs &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(), \n  min_n = tune(),\n  mtry = tune(),\n  loss_reduction = tune(),\n  sample_size = tune(),\n  learn_rate = tune(),\n  stop_iter = tune()) %&gt;%\n  set_engine(\"lightgbm\", num_leaves = tune()) %&gt;%\n  set_mode(\"regression\")\n\nGiven that we are building an XGBoost model, we will specify the use of the boost_tree() function from the parsnip package which will create a decision tree wherein each tree depends on the results of the previous trees. Within the tidymodels framework, there are six engines that can drive a boost_tree() model: xgboost, C5.0, h20, lightgbm, mboost, and spark. As you will notice, rather than using the default xgboost engine, we are opting to run the model using lightgbm.\n\n\n\n\n\n\nNote\n\n\n\nWhy are we using lightgbm instead of xgboost as the engine for the model?\nUsing lightgbm and xgboost on the same data will provide nearly identical results, as they are both decision tree frameworks at their core. However, lightgbm performs significantly faster than xgboost, as it uses a histogram-based process for optimization that reduces the amount of data required to complete one tree in the ensemble.\nThis speed increase is the result of lightgbm building the tree in a vertical growth pattern while xgboost does so in a horizontal fashion. While the vertical ensemble-building approach is unquestionably faster, it is prone to overfitting in the training process (but this can be adequately addressed through hyperparameter tuning). The caveat is that despite using lightgbm, the training process still required 4.5 hours when tuning for hyperparameters over 100 grids.\n\n\nRegardless of using lightgbm or xgboost as the model’s engine, we have the ability to tune the same items, including tree, tree_depth, min_n, mtry, loss_reduction, sample_size, learn_rate, and stop_iter. The lightgbm engine, specifically, can tune for the num_leaves in the vertical growth pattern. After creating the model’s specifications, we pass the required tune() information into grid_latin_hypercube() and then pass the created recipe and model specifications into our workflow().\n\nrushing_grid &lt;- grid_latin_hypercube(\n  trees(),\n  tree_depth(),\n  finalize(mtry(), rushes),\n  min_n(),\n  num_leaves(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  learn_rate(),\n  stop_iter(),\n  size = 100)\n\nrushing_workflow &lt;-\n  workflow() %&gt;%\n  add_recipe(rushing_recipe) %&gt;%\n  add_model(rushing_specs)\n\nWith a completed workflow that contains the recipe and the model’s engine and tuning requirements, we can tune the hyperparameters using the tune_grid() function.\n\nrushing_tune &lt;- tune_grid(rushing_workflow,\n                          resample = rushing_folds,\n                          grid = rushing_grid,\n                          control_grid(save_pred = TRUE))\n\nDepending on your computing power, the process of tuning the hyperparameters can take a significant amount of time, even when using DoParallel to conduct the process under parallel processing. Manually setting for 100 trees over 30 grids took just 5 minutes, while increasing it to 1,000 trees over 60 grids took 96 minutes. Tuning for the number of trees over 100 grids took, as mentioned, 4.5 hours.\nWith the hyperparameter tuning process complete we can view the best performing ones based on RMSE (Root Mean Square Error).\n\nbest_params &lt;- rushing_tune %&gt;%\n  select_best(metric = \"rmse\")\n\nbest_params\n\n# A tibble: 1 x 10\n   mtry trees min_n tree_depth learn_rate loss_reduction sample_size\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1     5   644    37          8     0.0257         0.0359       0.533\n# i 3 more variables: stop_iter &lt;int&gt;, num_leaves &lt;int&gt;,\n#   .config &lt;chr&gt;\n\n\nThe values in the output of best_params are the result of the hyperparameter tuning process and represent the values for each item that provided the best performance on the withheld validation set.\n\n\nmtry (5) - this represented the number of variables that are randomly sampled as candidates at each split in the ensemble building process\n\ntrees (644) - the trees item is the total number of trees within the model, with each subsequent tree helping to correct the errors made in prior trained trees\n\nmin_n (37) - this is the minimum number of observations that need to reside in a node before the model permits a split to occur\n\ntree_depth (8) - this is the maximum number of nodes allowed in any one tree\n\nlearn_rate (0.0257) - the learn_rate represents the “shrinkage” within the model and controls the contribution of each tree in the model, wherein lower rates require more trees but generally provide more robust results\n\nloss_reduction (0.0359) - this is the minimum loss reduction the model requires before making a split in the trees. A larger value equates to a more conservative model\n\nsample_size (0.533) - the sample_size value is the fraction of the training data used to sample the building of each tree within the tuning process.\n\nstop_iter (9) - This is the number of iterations permitted if the validation scores does not make enough improvement\n\nnum_leaves (71) - the total number of leaves permitted in each node before a new tree must be created\n\nIn totality , the tuning of our model’s hyperparameters suggest that it is fairly complex, as indicated by the number of trees and tree_depth. Specifically, the high number of trees suggests that the model is likely detecting and capturing very subtle patterns within the data. The low learning_rate and stop_itr will help the model avoid overfitting, while the moderate sample_size and mtry values indicate the potential for a robust final result.\nWith the tuning process complete and the best hyperparameters selected, we can take those parameters and pass them into a final workflow to verify the model on the testing data using only those metrics.\n\nrushing_final_workflow &lt;- rushing_workflow %&gt;%\n  finalize_workflow(best_params)\n\nfinal_model &lt;- rushing_final_workflow %&gt;%\n  fit(data = rushing_test)\n\nrushing_predictions &lt;- predict(final_model, rushing_data_join)\n\nryoe_projs &lt;- cbind(rushing_data_join, rushing_predictions) %&gt;%\n  rename(actual_yards = label,\n         exp_yards = .pred)\n\nIn the above code, we are creating a second workflow titled rushing_final_workflow that will still contain the information from rushing_recipe and rushing_specs but will now only conduct the process using the tuned hyperparamters. We then take the final workflow and fit it to the testing data.\nWhen the final fitting process is complete, which is typically much faster than the tuning process, we can take the results of final_model and use the predict() function to create a combined data frame and then, in the next step, use cbind() to create a file containing all of the projections created by the model called ryoe_projs.\nAt this point, the modeling process is complete and all there is left to do is explore the results To do so, we will use basic commands from tidyverse to make some necessary manipulations, merge in team color information from nflreadr and then visualize the data.\n\nmean_ryoe &lt;- ryoe_projs %&gt;%\n  dplyr::group_by(season) %&gt;%\n  summarize(nfl_mean_ryoe = mean(actual_yards) - mean(exp_yards))\n\nryoe_projs &lt;- ryoe_projs %&gt;%\n  left_join(mean_ryoe, by = c(\"season\" = \"season\"))\n\nryoe_projs &lt;- ryoe_projs %&gt;%\n  mutate(ryoe = actual_yards - exp_yards + nfl_mean_ryoe)\n\nfor_plot &lt;- ryoe_projs %&gt;%\n  group_by(rusher) %&gt;%\n  summarize(\n    rushes = n(),\n    team = last(posteam),\n    yards = sum(actual_yards),\n    exp_yards = sum(exp_yards),\n    ypc = yards / rushes,\n    exp_ypc = exp_yards / rushes,\n    avg_ryoe = mean(ryoe)) %&gt;%\n  arrange(-avg_ryoe)\n\nteams &lt;- nflreadr::load_teams(current = TRUE)\n\nfor_plot &lt;- for_plot %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\n\nggplot(data = for_plot, aes(x = yards, y = exp_yards)) +\n  stat_poly_line(method = \"lm\", se = FALSE,\n                 linetype = \"dashed\", color = \"black\") +\n  stat_poly_eq(mapping = use_label(c(\"R2\", \"P\")),\n               p.digits = 2, label.x = .35, label.y = 3) +\n  geom_point(color = for_plot$team_color2, size = for_plot$rushes / 165) +\n  geom_point(color = for_plot$team_color, size = for_plot$rushes / 200) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  labs(title = \"**Actual Rushing Yards vs. Expected Rushing Yards**\",\n       subtitle = \"*2016 - 2022 | Model: LightGBM*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"Actual Rushing Yards\") +\n  ylab(\"Expected Rushing Yards\") +\n  nfl_analytics_theme() +\n  geom_text_repel(data = filter(for_plot, yards &gt;= 4600),\n                  aes(label = rusher),\n                  box.padding = 1.7,\n                  segment.curvature = -0.1,\n                  segment.ncp = 3, segment.angle = 20,\n                  family = \"Roboto\", size = 4, fontface = \"bold\")\n\n\n\nActual Yards vs. Expected Yards\n\n\n\nIn the completed plot, hose running backs that outperformed the model’s expectations (that is, more actual rushing yards than expected rushing yards) fall below the line, while those that underperformed the model are above the line of best fit. Nick Chubb, for example, accumulated a total of 5,982 rushing yards while the model expected him to gain just 5,097 (a net difference of 885 yards over expected). On the other side of the line of best fit is Joe Mixon, whose 5,025 actual rushing yards fell short of the model’s expectation of 5,156 (a net difference of -131 rushing yards over expected). Those rushers very close to being on the line of best fit had an expected rushing yards prediction nearly identical to their actual performs (such as Dalvin Cook’s 5,656 actual yards and 5,342 expected yards).\nHowever, these results are over all of 2016-2022. We can use the results stored in our ryoe_projs data frame to produce more granular results to see which running backs performed the best against the model’s expectation based on a number of different factors. For example, let’s view the data by season and determine which rusher had the largest positive difference between their actual yards and expected yards.\n\ndiff_per_season &lt;- ryoe_projs %&gt;%\n  group_by(season, rusher) %&gt;%\n  summarize(\n    rushes = n(),\n    team = last(posteam),\n    yards = sum(actual_yards),\n    exp_yards = sum(exp_yards),\n    yards_diff = yards - exp_yards)\n\ndiff_per_season &lt;- diff_per_season %&gt;%\n  left_join(teams, by = c(\"team\" = \"team_abbr\"))\n\ndiff_per_season &lt;- diff_per_season %&gt;%\n  group_by(season) %&gt;%\n  mutate(is_max_diff = ifelse(yards_diff == max(yards_diff), 1, 0))\n\nTo plot the leaders per season required just two additions to the current ryoe_projs data frame. We first use summarize() to gather each running back’s rushes, team, actual yards, and expected rushing yards per season and then created the yards_diff column which is simply the difference between the player’s yards and exp_yards. In order to label just the leader’s name in the plot for each season, we use mutate() to create the is_max_diff column where a rusher is provided a numeric 1 if their yards_diff was the highest positive value for the season.\n\nggplot(diff_per_season, aes(x = yards, y = exp_yards)) +\n  stat_poly_line(method = \"lm\", se = FALSE,\n                 linetype = \"dashed\", color = \"black\") +\n  stat_poly_eq(mapping = use_label(c(\"R2\", \"P\")),\n               p.digits = 2, label.x = .20, label.y = 3) +\n  geom_point(color = diff_per_season$team_color2,\n             size = diff_per_season$rushes / 165) +\n  geom_point(color = diff_per_season$team_color,\n             size = diff_per_season$rushes / 200) +\n  scale_x_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::comma_format()) +\n  geom_label_repel(data = subset(diff_per_season,\n                                 is_max_diff == 1),\n                  aes(label = rusher),\n                  box.padding = 1.5, nudge_y = 1, nudge_x = 2,\n                  segment.curvature = -0.1,\n                  segment.ncp = 3, segment.angle = 20,\n                  family = \"Roboto\", size = 3.5,\n                  fontface = \"bold\") +\n  labs(title = \"**Rushing Yards over Expected Leader Per Season**\",\n       subtitle = \"*2016 - 2022 | Model: LightGBM*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"Actual Rushing Yards\") +\n  ylab(\"Expected Rushing Yards\") +\n  facet_wrap(~ season, scales = \"free\") +\n  nfl_analytics_theme() +\n  theme(strip.text = element_text(face = \"bold\", family = \"Roboto\", size = 12),\n        strip.background = element_rect(fill = \"#F6F6F6\"))\n\n\n\nRYOE Leaders per Season\n\n\n\nLast, given the success of Jonathan Taylor’s 2021 season, it may be interesting to plot the trajectory of both his cumulative actual rushing yards and cumulative expected rushing yards against the his carries. Given the difference between his actual rushing yards and expected rushing yards was 414, there should be a increasingly growing gap between the lines on the plot that represent each.\n\nj_taylor_2021 &lt;- ryoe_projs %&gt;%\n  filter(rusher == \"J.Taylor\" & posteam == \"IND\" & season == 2021) %&gt;%\n  reframe(\n    rusher = rusher,\n    team = last(posteam),\n    cumulative_yards = cumsum(actual_yards),\n    cumulative_exyards = cumsum(exp_yards))\n\nj_taylor_2021$cumulative_rushes = as.numeric(rownames(j_taylor_2021))\n\nj_taylor_image &lt;- png::readPNG(\"./images/j_taylor_background.png\")\n\nIn order to create the j_taylor_2021 data frame, we first create a new variable titled cumulative_rushes that is copying the numeric row names so that each subsequent rushing attempt increases this number, resulting in his total number of carries in the season. Similarly , the cumulative_yards and cumulative_exyards are calculated using the cumsum function which sums the results, play by play, in a rolling fashion.\nLastly, in order to provide a bit of “eye candy” to the plot, we use the readPNG() function from the png package to read in an image of Taylor rushing the football. We will place this image into the plot.\n\nggplot() +\n  geom_line(aes(x = j_taylor_2021$cumulative_rushes,\n                y = j_taylor_2021$cumulative_yards),\n            color = \"#002C5F\", size = 1.75) +\n  geom_line(aes(x = j_taylor_2021$cumulative_rushes,\n                y = j_taylor_2021$cumulative_exyards),\n            color = \"#A2AAAD\", size = 1.75) +\n  scale_x_continuous(breaks = scales::pretty_breaks(n = 12),\n                     labels = scales::comma_format()) +\n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10),\n                     labels = scales::comma_format()) +\n  annotate(geom = \"text\", label = \"Cumulative Actual Yards\", x = 200, y = 1050,\n           angle = 30, family = \"Roboto\", size = 5) +\n  annotate(geom = \"text\", label = \"Cumulative Expected Yards\", x = 200, y = 700,\n           angle = 30, family = \"Roboto\", size = 5) +\n  annotation_custom(grid::rasterGrob(j_taylor_image, \n                               width = unit(1,\"npc\"), \n                               height = unit(1,\"npc\")),\n                    175, 375, 0, 1000) +\n  labs(title = \"**Jonathan Taylor: 2021 Cumulative Actual Yards\n       vs. Expected Yards**\",\n       subtitle = \"*Model: **LightGBM** Using ***boost_trees()***\n       in ***tidymodels****\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\") +\n  xlab(\"Cumulative Rushes in 2021 Season\") +\n  ylab(\"Cumulative Actual Yards and Expected Yards\") +\n  nfl_analytics_theme()\n\n\n\nJonathan Taylor’s 2021 Cumulative Actual Yards vs. Expected Yards"
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#exercises",
    "href": "05-nfl-analytics-advanced-methods.html#exercises",
    "title": "\n5  Advanced Model Creation with NFL Data\n",
    "section": "\n5.4 Exercises",
    "text": "5.4 Exercises\nThe answers for the following answers can be found here: http://nfl-book.bradcongelio.com/ch5-answers.\n\n5.4.1 Exercise 1\nFirst, run the code below to create the data frame titled fieldgoal_regression\n\npbp &lt;- nflreadr::load_pbp(2015:2022) %&gt;%\n  filter(season_type == \"REG\")\n\nfieldgoal_regression &lt;- pbp %&gt;%\n  filter(play_type == \"field_goal\" & field_goal_result != \"blocked\") %&gt;%\n  select(play_type, field_goal_result, kick_distance) %&gt;%\n  mutate(field_goal_result = ifelse(\n    field_goal_result == \"made\", 1, 0))\n\nUsing the data, construct a generalized linear model (glm()) with the family set to binomial. Use field_goal_result as the target variable and kick_distance as the predictor. View the results using summary().\n\n5.4.2 Exercise 2\nReproduce the above model into a data frame called fieldgoal_regression_2 and include information pertaining to the field playing surface, the temperature, and the wind. Use na.omit() to exclude those plays with missing information. After, rerun the glm() model and view the results using summary().\n\n\n\n\nGreer, R. (2022). Over expected metrics explained – what are CPOE, RYOE, and YACOE. Retrieved from https://www.nfeloapp.com/analysis/over-expected-explained-what-are-cpoe-ryoe-and-yacoe/\n\n\nMock, T. (2020). Intro to tidyodels with nflscrapR play-by-play. Retrieved from https://jthomasmock.github.io/nfl_hanic/#1\n\n\nWikipedia. (2023). R package. Retrieved from https://en.wikipedia.org/wiki/R_package"
  },
  {
    "objectID": "05-nfl-analytics-advanced-methods.html#footnotes",
    "href": "05-nfl-analytics-advanced-methods.html#footnotes",
    "title": "\n5  Advanced Model Creation with NFL Data\n",
    "section": "",
    "text": "At the time of writing this book, this was an ongoing issue. Most missing headshots in the data are those belonging to rookies. The issue is a result of an issue with how the the data is collected and a change to the API that provides access to the information.↩︎\nThanks to Tej Seth for briefly chatting with me over Twitter regarding the history of both Michael’s RYOE model and his own.↩︎"
  },
  {
    "objectID": "06-nfl-references.html",
    "href": "06-nfl-references.html",
    "title": "References",
    "section": "",
    "text": "Awbrey, J. (2020). The future of NFL analytics. Retrieved from\nhttps://www.samford.edu/sports-analytics/fans/2020/The-Future-of-NFL-Data-Analytics\n\n\nBechtold, T. (2021). How the analytics movement has changed the NFL\nand where it has fallen short. Retrieved from https://theanalyst.com/na/2021/04/evolution-of-the-analytics-movement-in-the-nfl/\n\n\nBig data bowl: The annual analytics contest explores statistical\ninnovations in football. (n.d.). Retrieved from https://operations.nfl.com/gameday/analytics/big-data-bowl/\n\n\nBushnell, H. (2021). NFL teams are taking 4th-down risks more than\never - but still not often enough. Retrieved from https://sports.yahoo.com/nfl-teams-are-taking-4th-down-risks-more-than-ever-but-still-not-often-enough-163650973.html\n\n\nCarl, S. (2022). nflplotR. Retrieved from https://nflplotr.nflverse.com/\n\n\nFortier, S. (2020). The NFL’s analytics movement has finally reached\nthe sport’s mainstream. Retrieved from https://www.washingtonpost.com/sports/2020/01/16/nfls-analytics-movement-has-finally-reached-sports-mainstream/\n\n\nGreer, R. (2022). Over expected metrics explained – what are CPOE,\nRYOE, and YACOE. Retrieved from https://www.nfeloapp.com/analysis/over-expected-explained-what-are-cpoe-ryoe-and-yacoe/\n\n\nHeifetz, D. (2019). We salute you, founding father of the NFL’s\nanalytics movement. Retrieved from https://www.theringer.com/nfl-preview/2019/8/15/20806241/nfl-analytics-pro-football-focus\n\n\nKirschner, A. (2022). The rams’ super bowl afterparty turned into a\nhistoric hangover. Retrieved from https://fivethirtyeight.com/features/the-rams-super-bowl-afterparty-turned-into-a-historic-hangover/\n\n\nKozora, A. (2015). Tomlin prefers \"feel over analytics\".\nRetrieved from http://steelersdepot.com/2015/09/tomlin-prefers-feel-over-analytics/\n\n\nMock, T. (2020). Intro to tidyodels with nflscrapR\nplay-by-play. Retrieved from https://jthomasmock.github.io/nfl_hanic/#1\n\n\nRosenthal, G. (2018). Super bowl LII: How the 2017 philadelphia\neagles were built. Retrieved from https://www.nfl.com/news/super-bowl-lii-how-the-2017-philadelphia-eagles-were-built-0ap3000000912753\n\n\nSilge, J. (n.d.). Tidymodels. Retrieved from https://tidymodels.org\n\n\nStikeleather, J. (2013). The three elements of successful data\nvisualizations. Retrieved from https://hbr.org/2013/04/the-three-elements-of-successf\n\n\nWickham, H. (2022). Tidyverse packages. Retrieved from https://www.tidyverse.org/packages/\n\n\nWikipedia. (2023). R package. Retrieved from https://en.wikipedia.org/wiki/R_package"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#air-yards",
    "href": "a1-nfl-analytics-dictionary.html#air-yards",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.1 Air Yards",
    "text": "A.1 Air Yards\nAir yards is the measure that the ball travels through the air, from the line of scrimmage, to the exact point where the wide receivers catches, or does not catch, the football. It does not take into consideration the amount of yardage gained after the catch by the wide receiver (which would be yards after catch).\nFor an example, please see the below illustration. In it, the line of scrimmage is at the 20-yardline. The QB completes a pass that is caught at midfield (the 50-yardline). After catching the football, the wide receiver is able to advance the ball down to the opposing 30-yardline before getting tackled. First and foremost, the quarterback is credited with a total of 50 passing yards on the play, while the wide receiver is credited with the same.\nHowever, because air yards is a better metric to explore a QB’s true impact on a play, he is credited with 30 air yards while the wide receiver is credited with 20 yards after catch.\nIn the end, quarterbacks with higher air yards per attempt are generally assumed to be throwing the ball deeper downfield than QBs with lower air yards per attempt.\n\n\n\n\n\nVisual representation of air yards\n\n\n\n\nThere are multiple ways to collect data pertaining to air yards. However, the most straightforward way is to use load_player_stats:\n\ndata &lt;- nflreadr::load_player_stats(2021)\n\nair.yards &lt;- data %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(player_id) %&gt;%\n  summarize(\n    attempts = sum(attempts),\n    name = first(player_name),\n    air.yards = sum(passing_air_yards),\n    avg.ay = mean(passing_air_yards)) %&gt;%\n  filter(attempts &gt;= 100) %&gt;%\n  select(name, air.yards, avg.ay) %&gt;%\n  arrange(-air.yards)\n\nair.yards\n\n# A tibble: 42 x 3\n   name       air.yards avg.ay\n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt;\n 1 T.Brady         5821   342.\n 2 J.Allen         5295   311.\n 3 M.Stafford      5094   300.\n 4 D.Carr          5084   299.\n 5 J.Herbert       5069   298.\n 6 P.Mahomes       4825   284.\n 7 T.Lawrence      4732   278.\n 8 D.Prescott      4612   288.\n 9 K.Cousins       4575   286.\n10 J.Burrow        4225   264.\n# i 32 more rows\n\n\nIn the above example, we can see that Tom Brady led the NFL during the 2021 regular season with a comined total of 5,821 air yards which works out to an average of 342 air yards per game."
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#air-yards-share",
    "href": "a1-nfl-analytics-dictionary.html#air-yards-share",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.2 Air Yards Share",
    "text": "A.2 Air Yards Share\nA receiving statistics, air yards share is the measure of a player’s share of the team’s total air yards in a game/season. This metric can be found using load_player_stats().\n\nnfl_stats &lt;- nflreadr::load_player_stats()\n\ntotal_ay_share &lt;- nfl_stats %&gt;%\n  filter(position == \"WR\") %&gt;%\n  group_by(player_name) %&gt;%\n  summarize(\n    total_rec = sum(receptions, na.rm = TRUE),\n    ay_share = sum(air_yards_share, na.rm = TRUE)) %&gt;%\n  filter(total_rec &gt;= 100) %&gt;%\n  arrange(-ay_share) %&gt;%\n  slice(1:10)\n\ntotal_ay_share\n\n# A tibble: 10 x 3\n   player_name total_rec ay_share\n   &lt;chr&gt;           &lt;int&gt;    &lt;dbl&gt;\n 1 D.Adams           103     7.61\n 2 D.Moore           101     7.53\n 3 A.Brown           106     7.10\n 4 T.Hill            119     7.04\n 5 C.Lamb            135     6.14\n 6 S.Diggs           107     5.77\n 7 J.Chase           100     5.67\n 8 P.Nacua           105     5.56\n 9 K.Allen           108     5.42\n10 M.Pittman         109     5.42"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#average-cushion",
    "href": "a1-nfl-analytics-dictionary.html#average-cushion",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.3 Average Cushion",
    "text": "A.3 Average Cushion\nThe average cushion measures the distance, in yards, between a WR/TE and the defender lined up against them at the line of scrimmage. This metric is included in the load_nextgen_stats() function.\n\nnextgen_stats &lt;- nflreadr::load_nextgen_stats(stat_type = \"receiving\")\n\nwr_cushion &lt;- nextgen_stats %&gt;%\n  filter(week == 0 & season == 2022 & receptions &gt;= 100) %&gt;%\n  select(player_display_name, avg_cushion) %&gt;%\n  arrange(-avg_cushion) %&gt;%\n  slice(1:10)\n\nwr_cushion\n\n# A tibble: 8 x 2\n  player_display_name avg_cushion\n  &lt;chr&gt;                     &lt;dbl&gt;\n1 Chris Godwin               6.68\n2 CeeDee Lamb                6.56\n3 Amon-Ra St. Brown          6.52\n4 Tyreek Hill                6.38\n5 Travis Kelce               6.28\n6 Davante Adams              5.55\n7 Justin Jefferson           5.43\n8 Stefon Diggs               5.36"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#average-separation",
    "href": "a1-nfl-analytics-dictionary.html#average-separation",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.4 Average Separation",
    "text": "A.4 Average Separation\nAverage separation measures the distance (in yards) between the receivers and the nearest defender at the time of catch/incompletion.\n\nnextgen_stats &lt;- nflreadr::load_nextgen_stats(stat_type = \"receiving\")\n\nwr_separation &lt;- nextgen_stats %&gt;%\n  filter(week == 0 & season == 2022 & receptions &gt;= 100) %&gt;%\n  select(player_display_name, avg_separation) %&gt;%\n  arrange(-avg_separation) %&gt;%\n  slice(1:10)\n\nwr_separation\n\n# A tibble: 8 x 2\n  player_display_name avg_separation\n  &lt;chr&gt;                        &lt;dbl&gt;\n1 Tyreek Hill                   3.31\n2 Amon-Ra St. Brown             3.10\n3 Justin Jefferson              3.09\n4 CeeDee Lamb                   3.07\n5 Chris Godwin                  2.98\n6 Davante Adams                 2.95\n7 Travis Kelce                  2.88\n8 Stefon Diggs                  2.83"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#average-depth-of-target",
    "href": "a1-nfl-analytics-dictionary.html#average-depth-of-target",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.5 Average Depth of Target",
    "text": "A.5 Average Depth of Target\nAs mentioned above, a QB’s air yards per attempt can highlight whether or not he is attempting to push the ball deeper down field than his counterparts. The official name of this is Average Depth of Target (or ADOT). We can easily generate this statistic using the load_player_stats function within nflreader:\n\ndata &lt;- nflreadr::load_player_stats(2021)\n\nadot &lt;- data %&gt;%\n  filter(season_type == \"REG\") %&gt;%\n  group_by(player_id) %&gt;%\n  summarize(\n    name = first(player_name),\n    attempts = sum(attempts),\n    air.yards = sum(passing_air_yards),\n    adot = air.yards / attempts) %&gt;%\n  filter(attempts &gt;= 100) %&gt;%\n  arrange(-adot)\n\nadot\n\n# A tibble: 42 x 5\n   player_id  name       attempts air.yards  adot\n   &lt;chr&gt;      &lt;chr&gt;         &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 00-0035704 D.Lock          111      1117 10.1 \n 2 00-0029263 R.Wilson        400      3955  9.89\n 3 00-0036945 J.Fields        270      2636  9.76\n 4 00-0034796 L.Jackson       382      3531  9.24\n 5 00-0036389 J.Hurts         432      3882  8.99\n 6 00-0034855 B.Mayfield      418      3651  8.73\n 7 00-0026498 M.Stafford      601      5094  8.48\n 8 00-0031503 J.Winston       161      1340  8.32\n 9 00-0034857 J.Allen         646      5295  8.20\n10 00-0029604 K.Cousins       561      4575  8.16\n# i 32 more rows\n\n\nAs seen in the results, if we ignore Drew Lock’s 10.1 ADOT on just 111 attempts during the 2021 regular season, Russell Wilson attempted to push the ball, on average, furthest downfield among QBs with at least 100 attempts."
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#completion-percentage-over-expected-cpoe",
    "href": "a1-nfl-analytics-dictionary.html#completion-percentage-over-expected-cpoe",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.6 Completion Percentage Over Expected (CPOE)",
    "text": "A.6 Completion Percentage Over Expected (CPOE)\nAt the conclusion fo the 2016 season, Sam Bradford, the quarterback of the Minnesota Vikings, recorded the highest completion percentage in NFL history, connecting on 71.6% of his attempts during the season. However, Bradford achieved this record by averaging just 6.4 yards per attempt. Bradford’s record-breaking completion percentage is suddenly less impressive when one realizes that he was rarely attempting downfield passes.\nBecause of this example, we can conclude that a quarterback’s completion percentage may not tell us the whole “story.” To adjust a quarterback’s completion percentage to include such contextual inputs such as air yards, we can turn to using completion percentage over expected (CPOE). A pre-calculated metric based on historical attempts in similar situations, CPOE take into account multiple variables, including: field position, down, yards to go, total air yards, etc.\nThe CPOE metric in the nflverse was developed by Ben Baldwin with a further explanation of it here: nflfastR EP, WP, CP, xYAC, and xPass Models. The data is included in the load_pbp() function of nflreadR.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\ncpoe_2022 &lt;- pbp %&gt;%\n  group_by(passer) %&gt;%\n  filter(complete_pass == 1 |\n           incomplete_pass == 1 |\n           interception == 1,\n         !is.na(down)) %&gt;%\n  summarize(total_attempts = n(),\n            mean_cpoe = mean(cpoe, na.rm = TRUE)) %&gt;%\n  filter(total_attempts &gt;= 300) %&gt;%\n  arrange(-mean_cpoe) %&gt;%\n  slice(1:10)\n\ncpoe_2022\n\n# A tibble: 10 x 3\n   passer       total_attempts mean_cpoe\n   &lt;chr&gt;                 &lt;int&gt;     &lt;dbl&gt;\n 1 G.Smith                 571      5.68\n 2 P.Mahomes               648      3.59\n 3 J.Brissett              366      2.86\n 4 J.Burrow                605      2.74\n 5 J.Hurts                 460      2.73\n 6 D.Jones                 467      2.32\n 7 T.Lawrence              582      1.45\n 8 T.Tagovailoa            399      1.43\n 9 J.Herbert               697      1.35\n10 K.Cousins               640      1.26"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#dakota",
    "href": "a1-nfl-analytics-dictionary.html#dakota",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.7 DAKOTA",
    "text": "A.7 DAKOTA\nA QB’s DAKOTA score is the adjusted EPA+CPOE composite that is based on the coefficients which best predicted the adjusted EPA/play in the prior year. The DAKOTA score is available in the load_player_stats() function within the nflverse.\n\nnfl_stats &lt;- nflreadr::load_player_stats()\n\nmean_dakota &lt;- nfl_stats %&gt;%\n  filter(position == \"QB\") %&gt;%\n  group_by(player_name) %&gt;%\n  summarize(\n    total_cmp = sum(completions, na.rm = TRUE),\n    mean_dakota = mean(dakota, na.rm = TRUE)) %&gt;%\n  filter(total_cmp &gt;= 250) %&gt;%\n  arrange(-mean_dakota) %&gt;%\n  slice(1:10)\n\nmean_dakota\n\n# A tibble: 10 x 3\n   player_name  total_cmp mean_dakota\n   &lt;chr&gt;            &lt;int&gt;       &lt;dbl&gt;\n 1 B.Purdy            308       0.218\n 2 D.Prescott         410       0.164\n 3 T.Tagovailoa       388       0.151\n 4 J.Allen            385       0.147\n 5 L.Jackson          307       0.134\n 6 J.Love             372       0.132\n 7 J.Hurts            352       0.129\n 8 J.Goff             407       0.124\n 9 P.Mahomes          401       0.119\n10 D.Carr             375       0.116"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#running-back-efficiency",
    "href": "a1-nfl-analytics-dictionary.html#running-back-efficiency",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.8 Running Back Efficiency",
    "text": "A.8 Running Back Efficiency\nA running back’s efficiency is measured by taking the total distance traveled, according to NextGen Stats, per the total of yards gained on the run. A lower number indicates a more North/South type runner, while a higher number indicates a running back that “dances” and runs laterally relevant to the line of scrimmage.\n\nnextgen_stats &lt;- nflreadr::load_nextgen_stats(stat_type = \"rushing\")\n\nrb_efficiency &lt;- nextgen_stats %&gt;%\n  filter(week == 0 & rush_attempts &gt;= 300) %&gt;%\n  select(player_display_name, efficiency) %&gt;%\n  arrange(efficiency)\n\nrb_efficiency\n\n# A tibble: 12 x 2\n   player_display_name efficiency\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 Jonathan Taylor           3.17\n 2 Josh Jacobs               3.33\n 3 Derrick Henry             3.45\n 4 Ezekiel Elliott           3.49\n 5 Ezekiel Elliott           3.57\n 6 Derrick Henry             3.57\n 7 Ezekiel Elliott           3.65\n 8 Dalvin Cook               3.74\n 9 Nick Chubb                3.80\n10 Derrick Henry             3.85\n11 Najee Harris              3.97\n12 Le'Veon Bell              4.17"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#expected-points-added-epa",
    "href": "a1-nfl-analytics-dictionary.html#expected-points-added-epa",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.9 Expected Points Added (EPA)",
    "text": "A.9 Expected Points Added (EPA)\nExpected Points Added is a measure of how well a team/player performed on a single play against relative expectations. At its core, EPA is the difference in expected points before and after each play. Because of this, on any given play, a team has the ability to either increase or decrease the expected points, with EPA being that specific difference. Importantly, EPA includes various contextual factors into its calculation such as down, distance, position on the field, etc. Then, based on historical data, an estimation of how many points, on average, a team is expected to score on a given situation is provided.\nFor instance, if the Chiefs are on their own 20-yard line with a 1st and 10, the expected points might be 0.5 based on historical data. This is the expected points before the play. If Mahomes completes a 10-yard pass, and now it is 1st and 10 on their own 30-yard line, the expected points for this new situation may increase to 0.8.\nTherefore, the expected points added (EPA) of that 10-yard pass would be 0.8 - 0.5 = 0.3 points. The resulting positive number indicates that the pass was beneficial and increased the team’s expected points.\nThe EP and EPA values are provided for each play in the play-by-play data.\n\nep_and_epa &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\" & posteam == \"KC\") %&gt;%\n  filter(!play_type %in% c(\"kickoff\", \"no_play\")) %&gt;%\n  select(posteam, down, ydstogo, desc, play_type, ep, epa)\n\nep_and_epa\n\n# A tibble: 1,252 x 7\n   posteam  down ydstogo desc                   play_type    ep    epa\n   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1 KC          1      10 (15:00) (Shotgun) 25-~ run       0.931  0.841\n 2 KC          2       1 (14:27) (Shotgun) 15-~ run       1.77  -0.263\n 3 KC          1      10 (13:52) 25-C.Edwards-~ run       1.51   0.454\n 4 KC          2       3 (13:15) (Shotgun) 25-~ run       1.96   1.37 \n 5 KC          1      10 (12:36) (Shotgun) 15-~ pass      3.34  -0.573\n 6 KC          2      10 (12:30) (Shotgun) 15-~ pass      2.76   1.32 \n 7 KC          1      10 (11:54) 15-P.Mahomes ~ pass      4.09  -0.178\n 8 KC          2       7 (11:11) (Shotgun) 25-~ run       3.91  -0.634\n 9 KC          3       7 (10:28) (Shotgun) 15-~ pass      3.28   2.03 \n10 KC          1       9 (9:47) (Shotgun) 15-P~ pass      5.30  -0.565\n# i 1,242 more rows\n\n\nAs well, the load_player_stats() function provides calculated passing_epa, rushing_epa, and receiving_epa per player on a weekly basis.\n\nweekly_epa &lt;- nflreadr::load_player_stats() %&gt;%\n  filter(player_display_name == \"Tom Brady\") %&gt;%\n  select(player_display_name, week, passing_epa)\n\nweekly_epa\n\n# A tibble: 0 x 3\n# i 3 variables: player_display_name &lt;chr&gt;, week &lt;int&gt;,\n#   passing_epa &lt;dbl&gt;"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#expected-yards-after-catch-success-xyac-success",
    "href": "a1-nfl-analytics-dictionary.html#expected-yards-after-catch-success-xyac-success",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.10 Expected Yards After Catch Success (xYAC Success)",
    "text": "A.10 Expected Yards After Catch Success (xYAC Success)\nxYAC Success is the probability that a play results in positive EPA (relative to where the play started) based on where the receiver caught the ball.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\nxyac &lt;- pbp %&gt;%\n  filter(!is.na(xyac_success)) %&gt;%\n  group_by(receiver) %&gt;%\n  summarize(\n    completetions = sum(complete_pass == 1, na.rm = TRUE),\n    mean_xyac = mean(xyac_success, na.rm = TRUE)) %&gt;%\n  filter(completetions &gt;= 100) %&gt;%\n  arrange(-mean_xyac) %&gt;%\n  slice(1:10)\n\nxyac\n\n# A tibble: 8 x 3\n  receiver    completetions mean_xyac\n  &lt;chr&gt;               &lt;int&gt;     &lt;dbl&gt;\n1 S.Diggs               100     0.872\n2 T.Kelce               106     0.872\n3 J.Jefferson           124     0.863\n4 T.Hill                125     0.856\n5 C.Lamb                103     0.832\n6 A.St. Brown           103     0.810\n7 C.Godwin              103     0.788\n8 A.Ekeler              106     0.548\n\n\nConsidering only those receivers with 100 or more receptions during the 2022 regular season, Stefon Diggs and Travis Kelcoe had the highest expected yards after catch success rate, with the model predicting that just over 82% of their receptions would result with a positive EPA once factoring in yards after catch."
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#expected-yards-after-catch-mean-yardage-xyac-mean-yardage",
    "href": "a1-nfl-analytics-dictionary.html#expected-yards-after-catch-mean-yardage-xyac-mean-yardage",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.11 Expected Yards After Catch Mean Yardage (xYAC Mean Yardage)",
    "text": "A.11 Expected Yards After Catch Mean Yardage (xYAC Mean Yardage)\nJust as above XYAC Success is the probability that the reception results in a positive EPA, xYAC Mean Yardage is the expected yards after catch based on where the ball was caught. We can use this metric to determine how much impact the receiver had after the reception against what the xYAC Mean Yardage model predicted.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\nxyac_meanyardage &lt;- pbp %&gt;%\n  filter(!is.na(xyac_mean_yardage)) %&gt;%\n  group_by(receiver) %&gt;%\n  mutate(mean_yardage_result = ifelse(yards_after_catch &gt;= xyac_mean_yardage,\n                                      1, 0)) %&gt;%\n  summarize(total_receptions = sum(complete_pass == 1,\n                                   na.rm = TRUE),\n            total_higher = sum(mean_yardage_result,\n                               na.rm = TRUE),\n            pct = total_higher / total_receptions) %&gt;%\n  filter(total_receptions &gt;= 100) %&gt;%\n  arrange(-pct) %&gt;%\n  slice(1:10)\n\nxyac_meanyardage\n\n# A tibble: 8 x 4\n  receiver    total_receptions total_higher   pct\n  &lt;chr&gt;                  &lt;int&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 T.Kelce                  106           51 0.481\n2 A.Ekeler                 106           47 0.443\n3 A.St. Brown              103           44 0.427\n4 J.Jefferson              124           47 0.379\n5 S.Diggs                  100           37 0.37 \n6 C.Lamb                   103           35 0.340\n7 T.Hill                   125           41 0.328\n8 C.Godwin                 103           29 0.282"
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#pass-over-expected",
    "href": "a1-nfl-analytics-dictionary.html#pass-over-expected",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.12 Pass Over Expected",
    "text": "A.12 Pass Over Expected\nPass Over Expected is the probability that a play will be a pass scaled from 0 to 100 and is based on multiple factors, including yard line, score differential, to who is the home team. The numeric value indicates how much over (or under) expectation each offense called a pass play in a given situation.\nFor example, we can use the metric to determine the pass over expected value for Buffalo, Cincinnati, Kansas City, and Philadelphia on 1st down with between 1 and 10 yards to go.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\npass_over_expected &lt;- pbp %&gt;%\n  filter(down == 1 & ydstogo &lt;= 10) %&gt;%\n  filter(posteam %in% c(\"BUF\", \"CIN\", \"KC\", \"PHI\")) %&gt;%\n  group_by(posteam, ydstogo) %&gt;%\n  summarize(mean_passoe = mean(pass_oe, na.rm = TRUE))\n\nggplot(pass_over_expected, aes(x = ydstogo, y = mean_passoe,\n                               group = posteam)) +\n  geom_smooth(se = FALSE, aes(color = posteam), size = 2) +\n  nflplotR::scale_color_nfl(type = \"primary\") +\n  scale_x_continuous(breaks = seq(1,10, 1)) +\n  scale_y_continuous(breaks = scales::pretty_breaks(),\n                     labels = scales::percent_format(scale = 1)) +\n  nfl_analytics_theme() +\n  xlab(\"1st Down - Yards To Go\") +\n  ylab(\"Average Pass Over Expected\") +\n  labs(title = \"**Average Pass Over Expected**\",\n       subtitle = \"*1st Down: BUF, CIN, KC, PHI*\",\n       caption = \"*An Introduction to NFL Analytics with R*&lt;br&gt;\n       **Brad J. Congelio**\")\n\n\n\nPass Over Expected Values on 1st Down for Buffalo, Cincinnati, Kansas City, and Philadelphia\n\n\n\nNoticeably, the Chiefs pass well over expected (especially compared to the other three teams) when it is 1st down with six yards to go."
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#success-rate",
    "href": "a1-nfl-analytics-dictionary.html#success-rate",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.13 Success Rate",
    "text": "A.13 Success Rate\nPrior to the formulation of EP and EPA, success rate was calculated based on the percentage of yardage gained on a play (40% of the necessary yards on 1st down, 60% on 2nd down, and 100% of the yards needed for a 1st down on both 3rd and 4th downs). However, modern success rate is determined simply by whether or not the specific play had an EPA greater than 0. Using success rate allows us to determine, for example, whether an offensive unit is stronger in rushing or passing attempts, as well as serving as a baseline indicator of a team’s consistency.\n\npbp &lt;- nflreadr::load_pbp(2022) %&gt;%\n  filter(season_type == \"REG\")\n\nsuccess_rate &lt;- pbp %&gt;%\n  filter(play_type %in% c(\"pass\", \"run\")) %&gt;%\n  group_by(posteam, play_type) %&gt;%\n  summarize(mean_success = mean(success, na.rm = TRUE)) %&gt;%\n  filter(posteam %in% c(\"BAL\", \"CIN\", \"CLE\", \"PIT\"))\n\nsuccess_rate\n\n# A tibble: 8 x 3\n# Groups:   posteam [4]\n  posteam play_type mean_success\n  &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;\n1 BAL     pass             0.423\n2 BAL     run              0.484\n3 CIN     pass             0.492\n4 CIN     run              0.439\n5 CLE     pass             0.433\n6 CLE     run              0.449\n7 PIT     pass             0.434\n8 PIT     run              0.470\n\n\nAs seen in the output of success_rate, the teams in the AFC North were generally evenly matched. The Ravens had more success running the ball (.48 to .42) while the Bengals found more success in the air (.49 to .43). The Browns’ success rate on passing and rushing attempts were nearly equal (.43 to .44)."
  },
  {
    "objectID": "a1-nfl-analytics-dictionary.html#time-to-line-of-scrimmage",
    "href": "a1-nfl-analytics-dictionary.html#time-to-line-of-scrimmage",
    "title": "Appendix A — NFL Analytics Quick Reference Guide",
    "section": "\nA.14 Time to Line of Scrimmage",
    "text": "A.14 Time to Line of Scrimmage\nMeasured by NextGen Stats, this is a calculation (to the 1/10th of a second), regarding how long it take the running back to cross the line of scrimmage.\n\nnextgen_stats &lt;- nflreadr::load_nextgen_stats(stat_type = \"rushing\")\n\navg_los &lt;- nextgen_stats %&gt;%\n  filter(week == 0 & season == 2022 & rush_attempts &gt;= 200) %&gt;%\n  select(player_display_name, avg_time_to_los) %&gt;%\n  arrange(avg_time_to_los) %&gt;%\n  slice(1:10)\n\navg_los\n\n# A tibble: 10 x 2\n   player_display_name avg_time_to_los\n   &lt;chr&gt;                         &lt;dbl&gt;\n 1 Jamaal Williams                2.64\n 2 D'Onta Foreman                 2.65\n 3 Joe Mixon                      2.66\n 4 Saquon Barkley                 2.70\n 5 Derrick Henry                  2.70\n 6 Najee Harris                   2.72\n 7 Ezekiel Elliott                2.72\n 8 Rhamondre Stevenson            2.73\n 9 Alvin Kamara                   2.75\n10 Austin Ekeler                  2.76"
  },
  {
    "objectID": "a2-nfl-further-reading.html#introduction-to-r-programming-books",
    "href": "a2-nfl-further-reading.html#introduction-to-r-programming-books",
    "title": "Appendix B — Further Reading Suggestions",
    "section": "B.1 Introduction to R Programming Books",
    "text": "B.1 Introduction to R Programming Books\n\nR for Data Science: Import, Tidy, Transform, Visualize, and Model Data\nHands-On Programming with R: Write Your Own Functions and Simulations\nThe Book of R: A First Course in Programming and Statistics\nLearning R: A Step-by-Step Function Guide to Data Analysis\nThe Art of R Programming: A Tour of Statistical Software Design\nAdvanced R (Second Edition)"
  },
  {
    "objectID": "a2-nfl-further-reading.html#data-visualization-in-r-and-visualization-guides",
    "href": "a2-nfl-further-reading.html#data-visualization-in-r-and-visualization-guides",
    "title": "Appendix B — Further Reading Suggestions",
    "section": "B.2 Data Visualization in R and Visualization Guides",
    "text": "B.2 Data Visualization in R and Visualization Guides\n\nR Graphics Cookbook: Practical Recipes for Visualizing Data\nStorytelling with Data: A Data Visualization Guides for Business Professionals\nBetter Data Visualizations: A Guide for Scholars, Researchers, and Wonks"
  },
  {
    "objectID": "a2-nfl-further-reading.html#sport-analytics-guidesbooks",
    "href": "a2-nfl-further-reading.html#sport-analytics-guidesbooks",
    "title": "Appendix B — Further Reading Suggestions",
    "section": "B.3 Sport Analytics Guides/Books",
    "text": "B.3 Sport Analytics Guides/Books\n\nThe Midrange Theory: Basketball’s Evolution in the Age of Analytics\nAnalyzing Baseball Data with R (2nd edition)\nA Fan’s Guide to Baseball Analytics: Why WAR, WHIP, wOBA, and other Advanced Sabermetrics Are Essential to Understanding Modern Baseball\nThe Book: Playing the Percentages in Baseball\nThe Hidden Game of Baseball: A Revolutionary Approach to Baseball and Its Statistics\nThe Hidden Game of Football: A Revealing and Lively Look at the Pro Game, With New Stats, Revolutionary Strategies, and Keys to Picking the Winners\nMathletics: How Gamblers, Managers, and Fans Use Mathematics in Sports\nBasketball Data Science: With Applications in R\nData Analytics in Football (Soccer): Positional Data Collection, Modelling, and Analysis"
  },
  {
    "objectID": "a3-nfl-errata.html",
    "href": "a3-nfl-errata.html",
    "title": "Appendix C — Errata",
    "section": "",
    "text": "While every effort was made to write and produce a clean book (from both myself and the copyediting team at CRC Press), it is likely that minor issues go unnoticed or resources used in this book change locations (such as URLs pointed to specific data, etc.).\nBecause of this, this page enumerates errors in the print version of Introduction to NFL Analytics with R. Please notify me if you find issues (typos, lack of figure captions, issues with variables names, incorrect URLs, etc.) so that I may keep the online version updated here.\n\nChapter 5 (p. 241): Both variables are labeled as total_yards. The Y (dependent variable) should be total_points and the X (predictor variable) should remain total_yards."
  }
]