# Advanced Methods: Modeling and Big Data Bowl

```{r setup-ch5, include = FALSE}

source("book-functions.R")

library(tidyverse)
library(nflverse)
library(ggpmisc)
library(tidymodels)
library(caret)
library(nnet)
library(lightgbm)
library(bonsai)
library(ggtext)
library(extrafont)
library(ggpmisc)
library(ggimage)
library(ggcorrplot)
library(reshape2)
library(RColorBrewer)
```

## Introduction to Statistics and Modeling with NFL Data

## Basic Statistical Modeling

## Deciphering Statistical Results

information here will be provided to assist in understanding what the results of the models mean and will serve as a reference point for the rest of the chapter.

### Understanding Regression Results

Using the below example that explores the relationship between an NFL's teams total yards and total points over the course of a season, the results of a linear regression model calculated using the `lm()` function includes the following information (explained afterward):

```{r example-regression-results-explained, eval = FALSE, echo = TRUE, output = FALSE}

`Residuals:
    Min      1Q  Median      3Q     Max 
-71.443 -22.334   1.157  19.145  68.080 

Coefficients:
              Estimate Std. Error t value      Pr(>|t|)    
(Intercept) -225.03520   65.44927  -3.438       0.00174 ** 
total_yards    0.10341    0.01132   9.135 0.00000000036 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 32.04 on 30 degrees of freedom
Multiple R-squared:  0.7356,	Adjusted R-squared:  0.7268 
F-statistic: 83.45 on 1 and 30 DF,  p-value: 0.0000000003599`
```

#### Residuals

A model's residuals are the calculated difference between actual values of the dependent values as found in the data used to build the model and those values predicted by the regression model. In a perfect uniform relationship, all of the values from a dataset would sit perfectly on top of the "line of best fit." Take the below graph, for example.

```{r perfect-line-of-fit, echo = FALSE, output = TRUE, message = FALSE, fig.align='center', fig.dpi = 400}

example_fit <- tibble(
  x = 1:10,
  y = 2 * x + 3)

example_perfect_fit <- lm(y ~ x, data = example_fit)

ggplot(example_perfect_fit, aes(x = x, y = y)) +
  geom_smooth(method = "lm", se = FALSE, color = "black", size = .8, linetype = "dashed") +
  geom_image(image = "./images/football-tip.png", asp = 16/9) +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  labs(title = "A Perfect Regression Model",
       subtitle = "Every Point Falls on the Line",
       caption = "**An Introduction to NFL Analytics with R**<br>Brad J. Congelio") +
  xlab("Our Predictor Variable") +
  ylab("Our Response Variable") +
  nfl_analytics_theme()
```

In this example model, our regression model was able to successfully capture the entirety of the relationship between `Our Predictor Variable` on the x-axis and the `Our Response Variable` on the y-axis. This means that the model leaves no unexplained or undetermined variance between the variables and, because of this, the model can take additional unused data to predict, with 100% accuracy, the resulting value of the dependent variable. **It is exceedingly rare to have real-world data be perfectly situated on the line of best fit**. In fact, it is more often than not a sign of "overfitting," which occurs when the model successfully discovers the "random noise" in the data. In the majority of cases, a regression model with a perfect line of fit will perform exceedingly poorly when introduced to unseen data.

A regression model that is not "overfitted" will have data points that do not fit on the line of best fit, but fall over and under it. The results of the regression model uses a simple formula - `residual = observed_value - predicted_value` to help us interpret the difference between those actual and estimated values.

The information produced in the example `lm()` summary above includes statistical information about the distribution of the model's residual errors.

|   Summary Distribution    |                                                                                                                                                                          Meaning                                                                                                                                                                          |
|:-------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|  The `Min` Distribution   |           The `Min` distribution provides the smallest difference between the actual values of the model's predictor variable (total points) and the predicted. In the example summary, **the minimum residual is -71.443 which means that the `lm()` model predicted that one specific team scored 71.443 more points than it actually did.**            |
|   The `1Q` Distribution   |                             The `1Q` distribution is based on the first quartile of the data (or where the first 25% of the model's residual fall on the line of best fit). **The `1Q` residual is -22.334, which means the `lm()` model predicted that 25% of the teams scored 22.334 more points than the actual values.**                              |
| The `Median` Distribution | The `Median` distribution, much like the `1Q` distribution is data from the first quartile, is the residuals from the 50th percentile of the data. **The `Median` residual in the above summary is 1.157, which means that the `lm()` model - for 50% of teams - either overestimated or underestimated a teams total points by less than 1.157 points.** |
|   The `3Q` Distribution   |                                                           Covering the third quartile of the residuals, **the `3Q` Distribution is 19.145 which means that 75% of the NFL teams in the data had a total points prediction either overestimated or underestimated by less than 19.145 points.**                                                            |
|  The `Max` Distribution   |                               The opposite of the `Min` distribution, the `Max` distribution is the largest difference between the model's observed and predicted values for a team's total points. In this case, for one specific team, **the model predicted the team scored 68.080 points less than the actual value.**                                |

::: callout-tip
A model's residuals allow you to quickly get a broad view of how accurately it is predicting the data. Ideally, a well-performing model will return residuals that are small and distributed around zero in a consistent fashion. Residuals that are both small and evenly dispersed around zero are the first sign that the model is making predictions that are close to the actual value in the data and is avoiding both over- and underestimating.

**But this is not always the case**.

For example, we can compare our above example's residuals to the below residual output produced by manually creating a dataset.

```{r creating-example-residuals, echo = TRUE, echo = FALSE, output = TRUE}

set.seed(42)
residual_example <- tibble(
  x = 1:50,
  y = 3 * x + 5 + rnorm(50, mean = 0, sd = 5)
)

residual_example_output <- lm(y ~ x, data = residual_example)

summary(residual_example_output$residuals)
```

Compared to the residuals from the above NFL data, the residuals from the randomly created data are small in comparison and are more evenly distributed around zero. Given that, it is likely that the linear model is doing a good job at making predictions that are close to the actual value.

**But that does not mean the residuals from the NFL data indicate a flawed and unreliable model**. It needs to be noted that the "goodness" of any specific linear regression model is wholly dependent on both the context and the specific problem that the model is attempting to predict. To that end, it is also a matter of trusting your subject matter expertise on matters regarding the NFL.

There could be any number of reasons that can explain why the residuals from the regression model are large and not evenly distributed from zero. For example:

1.  **Red-zone efficiency**: a team that moves the ball downfield with ease, but then struggles to score points once inside the 20-yardline , will accumulate `total_yards` but failed to produce `total_points` in the way the model predicted.
2.  **Turnovers:** Similar to above, a team may rack up `total_yards` but ultimately continue to turn the ball over prior to being able to score.
3.  **Defensive scoring**: a score by a team's defense, in this model, still counts towards `total_points` but does not count towards `total_yards`.
4.  **Strength of Opponent**: At the end of the 2022 season, the Philadelphia Eagles and the New York Jets both allowed just 4.8 yards per play. The model's predicted values of the other three teams in each respective division (NFC East and AFC East) could be incorrect because information, for example, the opponent's strength of defense was not included in the model.

All that to say: residuals are a first glance at the results of the data and provide a **broad** generalization of how the model performed without taking outside contextual factors into consideration.
:::

#### Coefficients

ffff

#### Simple Linear Regression

A simple linear regression is a fundamental statistical technique that is used to explore the relationship between two variables, specifically the dependent variable (also called the "response variable") and the independent variable (also called the "predictor"). By using a simple linear regression, we can model the relationship between the two variables as a linear equation that best fits the observed data points.

A simple linear regression aims to fit a straight line through all the observed data points in such a way that the total squared distance between the actual observations and the values predicted by the model are minimal. This line is often referred to as either the "line of best fit" or the "regression line" and it represents the interaction between the dependent and independent variables. Mathematically, the equation for a simple linear regression is as follows:

$$
Y = {\beta}_0 + {\beta}_1 * X + \epsilon
$$

1.  $Y$, in the above equation, is the dependent variable where the $X$ represents the independent variable.
2.  ${\beta}_o$ is the intercept of the regression model.
3.  ${\beta}_1$ is the slope of the model's "line of best fit."
4.  $\epsilon$ represents the error term.

To better illustrate this, let's use basic football terms using the above regression equation to compare a team's offensive points scored in a season based on how many offensive yards it accumulated. The intercept (${\beta}_o$) represents the value when a team's points scored and offensive yards are both zero. The slope (${\beta}_1$) represents the rate of change in $Y$ as the unit of $X$ changes. The error term ($\epsilon$) is represents the difference between the actual observed values of the regression's dependent variable and the value as predicted by the model.

Using our points scored/total yards example, a team's total yards gained is the **independent variable** and total points scored is the **dependent variable**, as a team's total yardage **is what drives the change in total points** (in other words, a team's total points *is dependent* on its total yardage). A team will not score points in the NFL if it is not also gaining yardage. We can view this relationship by building a simple linear regression model in R using the `lm()` function.

::: callout-note
The `lm()` function is a built-in function in RStudio that stands for "linear model" and is used, as described above, to fit a linear regression to the data that you provide. The completed regression estimates the coefficients of the regression, and also includes both the intercept and slope, which are the main factors in explaining the relationship between your data's response and predictor variables.

The `lm()` function requires just two arguments in order to provide results: a formula and the dataframe to use and is structured like so: `model_results <- lm(formula, data)`.

The `formula` argument require that you specify both the response and predictor variables, as named in your dataframe, in the structure of `Y ~ X`(wherein `Y` is the response variables and `X` is the predictor). In the case that you have more than one predictor variable, the `+` is used to add to the formula (`lm(Y ~ X1 + X2`).

The `lm()` function returns the coefficients, residuals, and other statistics of the model in a `lm` data object. There are There are numerous ways to access this data which are discussed in further detail below.
:::

Let's build a simple linear regression model that explores the relationship between the total yardage earned by a team over the course of a season and the number of points scored. To begin, place the prepared data in the `simple_regression_data` dataframe by running the code below.

```{r load-simple-regression-data, eval = TRUE, echo = TRUE, output = FALSE}

simple_regression_data <- vroom::vroom("https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/simple_regression_data.csv")

simple_regression_data
```

The data contains the total yardage and points scored for each NFL team between 2012 and 2022. The data does not include any playoff games. Before running a model on all ten years of data, we will begin by selecting just information from 2022 and then build our `lm()` model. Beforehand, though, we can take the raw values from `total_yards` and `total_points` and view the expected line of best fit.

```{r simple-linear-lobf, output = TRUE, message = FALSE, fig.align='center', fig.dpi = 400}

regression_2022 <- simple_regression_data %>%
  filter(season == 2022)

teams <- nflreadr::load_teams(current = TRUE)

regression_2022 <- regression_2022 %>%
  left_join(teams, by = c("team" = "team_abbr"))

  ggplot(regression_2022, aes(x = total_yards, y = total_points)) +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed", size = .8) +
  geom_image(aes(image = team_logo_wikipedia), asp = 16/9) +
  scale_x_continuous(breaks = scales::pretty_breaks(),
                     labels = scales::comma_format()) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  labs(title = "**Line of Best Fit: 2022 Season**",
       subtitle = "*Y = total_yards ~ total_points*",
       caption = "*An Introduction to NFL Analytic with R*<br>Brad J. Congelio") +
  xlab("Total Yards") +
  ylab("Total Points") +
  nfl_analytics_theme()

```

The plot shows that based on a simple regression between `total_yards` and `total_points` that several teams - like the Titans, Giants, Packers, Raiders, Jaguars, and the Chiefs - are fitted nearly perfectly with the regression line. Other teams, however, such as the Buccaneers and the Cowboys are well off this line of best fit. To further examine this relationship, we can pass the data into a proper simple linear regression model and start exploring the summary statistics.

```{r lm-model-2022, eval = TRUE, echo = TRUE, output = FALSE}

results_2022 <- lm(total_points ~ total_yards, data = regression_2022)
```

Using the `lm()` function, the $Y$ variable (the dependent) is `total_yards` and the $X$ variable (the predictor) is entered as `total_yards` with the argument that the `data` is coming from the `regression_2022` dataframe stored in the RStudio environment. We can view the results of the regression model by using the `summary()` function.

```{r viewing-regression-summary, eval = TRUE, echo = TRUE, output = TRUE}

summary(results_2022)
```

While residuals are not the only summary statistics to examine in the results, they do provide a broad way to understand how the linear model performed. In this case, the residuals have a wide spread and an inconsistent deviation from zero. While the `median` residual value is the closest to zero at 1.157, it is still a bit too high to safely conclude that the model is making predictions that adequately reflect the actual values. Moreover, both tail ends of the residual values (`Min` and `Max`) are a large negative and positive number, respectively, which is a possible indication that both over- and underestimating a team's `total_points` by statistically significant amount.

However, as mentioned in this chapter's explanation of how to interpret residuals from a model's summary statistics, the widespread and deviation from zero in the results is likely the result of numerous factors outside the model's purview that occur in any one NFL game. To get a better idea of what the residual values represent, we can plot the data and include NFL team logos.

```{r plotting-residual-values, output = TRUE, message = FALSE, fig.align='center', fig.dpi = 400}

regression_2022$residuals <- residuals(results_2022)

ggplot(regression_2022, aes(x = total_yards, y = residuals)) +
  geom_hline(yintercept = 0, color = "black", linewidth = .7) +
  stat_fit_residuals(size = 0.01) +
  stat_fit_deviations(size = 1.75, color = regression_2022$team_color) +
  geom_image(aes(image = team_logo_wikipedia), asp = 16/9, size = .0325) +
  scale_x_continuous(breaks = scales::pretty_breaks(),
                     labels = scales::comma_format()) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +
  labs(title = "**Total Yards & Residual Values**",
       subtitle = "*Y = total_points ~ total_yards*",
       caption = "*An Introduction to NFL Analytic with R*<br>Brad J. Congelio") +
  xlab("Total Yards") +
  ylab("Residual of Total Points") +
  nfl_analytics_theme() +
  theme(panel.grid.minor.y = element_line(color = "#d0d0d0"))
```

With the data visualized, it is clear that the model's `Min` distribution of -71.44 is associated with the Tampa Bay Buccaneers, while the `Max` distribution of 68.08 is the prediction for the total points earned by the Dallas Cowboys. Because a negative residual means that the model's predicted value is too high, and a positive residual means it was too low, we can conclude that the Buccaneers actually scored 71.4 points less than the the results of the model, while the Cowboys scored 68.08 more than predicted.

```{r showing-coefficient-lm, eval = FALSE, echo = TRUE, output = FALSE}

`Coefficients:
              Estimate Std. Error t value      Pr(>|t|)    
(Intercept) -225.03520   65.44927  -3.438       0.00174 ** 
total_yards    0.10341    0.01132   9.135 0.00000000036 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1`

```

The `(Intercept)` of the model, or where the regression line crosses the y-axis, is -225.0350. When working with NFL data, it of course does not make sense that the `(Intercept)` is negative. Given the model is built on a team's total yards and total points, it seems intuitive that the regression line would cross the y-axis at the point of (0,0) as an NFL team not gaining any yards is highly unlike to score any points.

It is important to remember that the linear model attempts to position the regression line to come as close to all the individual points as possible. Because of this, it is not uncommon for regression line to not cross exactly where the x-axis and y-axis meet. Again, contextual factors of an NFL game are not account for in the model's data: strength of the opponent's defense, the quality of special teams play, defensive turnovers and/or touchdowns, field position, etc. can all impact a team's ability to score points without gaining any yardage. The lack of this information in the data ultimately impact the positioning of the line of best fit.

The `total_yards` coefficient represents the slope of the model's regression line. It is this slope that represents how a team's total points are predicted to change with every additional gain of one yard. In this example, the `total_yards` coefficient is 0.10341 - so for every additional yard gained by a team, it is expected to add 0.10341 points to the team's cumulative amount.

The `Std. Error` summary statistic provides guidance on the accuracy of the other estimated coefficients. The `Std. Error` for the model's `(Intercept)` is quite large at 65.44927. Given the ability to resample the data from NFL terms numerous times and then allowing the linear model to predict again, this specific `Std. Error` argues that the regression line will cross the y-axis with 65.44972 of -225.03520 in either direction. Under normal circumstances, a large `Std. Error` for the `(Intercept)` would cause concern about the validity of the regression line's crossing point. However, given the nature of this data - an NFL team cannot score negative points - we should not have any significant concern about the large `Std. Error` summary statistic for the `(Intercept)`.

At 0.01132, the `Std. Error` for the `total_yards` coefficient is small and indicates that the `Estimate` of `total_yards` - that is, the increase in points per every yard gained - is quite accurate. Given repeated re-estimating of the data, the relationship between `total_yards` and `total_points` would vary by just 0.01132, either positively or negatively.

With a `t-value` of 9.135, the `total_yards` coefficient has a significant relationship with `total_points`. The A value of -3.438 indicates that the `(Intercept)` is statistically different from 0 but we should still largely ignore this relationship given the nature of the data.

The model's `Pr(>|t|)` value of highly significant for `total_yards` and is still quite strong for the `(Intercept)`. The value of 0.00000000036 indicates an incredibly significant relationship between `total_yards` and `total_points`.

The linear model's `Residual Standard Error` is 32.04, which means that the average predicted values of `total_points` are 32.04 points different from the actual values in the data. The linear model was able to explain 73.56% of the variance between `total_yards` and `total_points` based on the `multiple R-squared` value of 0.7356. Additionally, the `Adjusted R-squared` value of 0.7268 is nearly identical to the multiple R2, which is a sign that the linear model is not overfitting (in this case because of the simplicity of the data). The model's `F-Statistic` of 83.45 indicates a overall significance to the data, which is backed up by an extremely strong `p-value`.

Based on the summary statistics, the linear model did an extremely good job at capturing the relationship between a team's `total_yards` and `total_points`. However, with residuals ranging from -71.443 to 68.080, it is likely that the model can be improved upon by adding additional information and statistics. However, before providing additional metrics, we can try to improve the model's predictions by including all of the data (rather than just the 2022 season). By including 20-seasons worth of `total_yards` and `total_points`, we are increasing the sample size which, in theory, allows for a reduced impact of any outliers and an improve generalizability.

::: callout-important
Working with 10+ years of play-by-play data can be problematic in that them model, using just `total_yards` and `total_points`, is not aware of changes in the overall style of play NFL. The balance between rushing and passing has shifted, there's been a philosophical shift in the coaching ranks in "going for it" on 4th down, etc. A simple linear regression cannot account for how these shifts impact the data on a season-by-season basis.
:::

The results from including the `total_points` and `total_yards` for each NFL team from 2012-2022 show an improvement of the model, specifically with the residual values.

```{r all-seasons-regression, eval = TRUE, echo = TRUE, output = FALSE}

regression_all_seasons <- simple_regression_data %>%
  select(-season)

all_season_results <- lm(total_points ~ total_yards, data = regression_all_seasons)

summary(all_season_results)
```

Without further testing, the residual values after including 20-seasons worth of data are a bit better. The `Median` is -1.26 which is slightly higher than just one season (`M` = 1.16). The `1Q` and `3Q` distributions are both approximately symmetric around the model's `M` value compared to just the 2022 season regression that results in a deviation between `1Q` and `3Q` (-22.33 and 19.15, respectively). The `Min` and `Max` values of the new model still indicate longtail cases on both ends of the regression line much like the 2022 model found.

::: callout-tip
To further examine the residual values, we can use a **Shapiro-Wilk Test** to test the whether results are normally distributed.

The Shapiro-Wilk Test provides two values with the output: the test statistic (provided as a `W` score) and the model's `p-value`. Scores for `W` can range between 0 and 1, where results closer to 1 meaning the residuals are in a normal distribution. The `p-value` is used make decision on the null hypothesis (that there *is* enough evidence to conclude that there is uneven distribution). In most cases, if the `p-value` is larger than the regression's level of significance (typically 0.05), than you may **reject** the null hypothesis.

We can run the Shapiro-Wilk Test on our 2012-2022 data using the `shapiro.test` function that is part of the `stats` package in R.

```{r running-shapiro-wilk, eval = TRUE, echo = TRUE, output = TRUE}

results_2012_2020 <- residuals(all_season_results)

shapiro_test_result <- shapiro.test(results_2012_2020)

shapiro_test_result

```

The `W` score for the residual is 1, meaning a very strong indication that the data in our model is part of a normal distribution. The `p-value` is 0.8, which is much large than the regression's level of significance (0.05). As a result, we can reject the null hypothesis and again conclude that the data is in a normal distribution.
:::

Using the Shapiro-Wilk Test to confirm the normal distribution of the data is help because, if plotted, the model that covers 20-seasons worth of data is harder to visually interpret than just one season as the plot increases from just 32 data points to 640, with many overlapping.

```{r plotting-all-season-residuals, output = TRUE, message = FALSE, fig.align='center', fig.dpi = 400}

teams <- nflreadr::load_teams(current = TRUE)

regression_all_seasons <- left_join(regression_all_seasons, teams, by = c("team" = "team_abbr"))

regression_all_seasons$residuals <- residuals(all_season_results)

ggplot(regression_all_seasons, aes(x = total_yards, y = residuals)) +
  geom_hline(yintercept = 0, color = "black", linewidth = .7) +
  stat_fit_residuals(size = 2, color = regression_all_seasons$team_color) +
  stat_fit_deviations(size = 1, color = regression_all_seasons$team_color, alpha = 0.5) +
  scale_x_continuous(breaks = scales::pretty_breaks(),
                     labels = scales::comma_format()) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +
  labs(title = "**Total Yards & Residual Values: 2012-2022**",
       subtitle = "*Y = total_points ~ total_yards*",
       caption = "*An Introduction to NFL Analytic with R*<br>Brad J. Congelio") +
  xlab("Total Yards") +
  ylab("Residual of Total Points") +
  nfl_analytics_theme() +
  theme(panel.grid.minor.y = element_line(color = "#d0d0d0"))

```

We can also compare the multiple R2 and adjusted R2 score between the two regression models.

    2012 - 2022 Data:
    Multiple R-squared:  0.683
    Adjusted R-squared:  0.682

    2022 Data
    Multiple R-squared:  0.736
    Adjusted R-squared:  0.727

The regression using just the 2022 data results in a slightly better multiple and adjusted R2 score compared to using data from the last twenty seasons of the NFL. While this does indicate that the model based on the single season is better at defining the relationship between a team's `total_yards` and `total_points` it is essential to remember that there is different underlying patterns in the data as a result of the changing culture in the NFL and, ultimately, the epp and flow of team performance as a result of high levels of parity in the league.

In order to account for this "epp and flow" in both team performance and the changing culture/rules of the NFL, we need to turn to a multiple linear regression in include these additional factors as it is a model that is capable of better accounting for the nuances of NFL data.

#### Multiple Linear Regression

A multiple linear regression is extremely similar to a simple linear regression (both in design and how to create one in RStudio). The main difference, as discussed, is that a multiple linear regression allows for us to include additional predictor variables by using the `+` sign in the model's formula. The inclusion fo these additional predictive variables, in theory, allow the model to compute the more complex relationships in NFL data and improve its final performance.

We will again create our first multiple linear regression with just data from the 2022 season that includes the same predictor (`total_yards`) and response variable (`total_points`). For additional predictors, we must consider what circumstances may lead a team to have high `total_yardage` but an amount of `total_points` that would fall below the model's predicted value. We will include as additional predictors:

1.  **Redzone Efficiency:** provided as a percentage, this is a calculation of how many times a team drove into the red zone and scored. A higher percentage is better.
2.  **Redzone Touchdown Efficiency:** This is the same as redzone efficiency, but includes only the number of red zone trips divided by the total touchdowns scored from the redzone.
3.  **Redzone Field Goal Efficiency:** The same as redzone touchdown efficiency, but with field goals.
4.  **Cumulative Turnovers**: The total number of turnovers during the regular season.
5.  **Defensive Touchdowns**: The number of touchdowns scored by each team's defensive unit.
6.  **Special Teams Touchdowns**: The number of touchdowns scored by special teams (kick/punt returns).

Let's build the multiple regression model for only the 2022 season. You can read in the data using `vroom::vroom`.

```{r getting-m-lm-data, eval = TRUE, echo = TRUE, output = FALSE}

multiple_lm_data <- vroom::vroom("https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/multiple_lm_data.csv")

multiple_lm_data
```

The data for the multiple linear regression has the same four columns as the simple linear regression (`season`, `team`, `total_points`, and `total_yards`). After are the new additional predictors (`rz_eff`, `rz_td_eff`, `rz_fg_eff`, `def_td`, and `spec_tds`).

::: callout-caution
Please note that, of the predictor and response variables, all of the values are in whole number format except for `rz_eff`, `rz_td_eff`, and `rz_fg_eff`. While it is not a problem to include predictors that are on differing scales (in this case, whole numbers and percentages), it may cause difficulty in interpreting the summary statistics. If this is the case, the issue can be resolved by using the `scale()` function to standardize all the data's predictors against one another.
:::

The construction of the multiple linear regression is the same process of the simple linear regression, with the inclusion of additional predictors to the formula using the `+` sign. We are applying a `filter()` to our `multiple_lm_data` to retrieve just the 2022 season to begin.

```{r multiple-linear-regression, eval = TRUE, echo = TRUE, output = TRUE}

multiple_lm_2022 <- multiple_lm_data %>%
  filter(season == 2022)


lm_multiple_2022 <- lm(total_points ~ total_yards + rz_eff + rz_td_eff +
                         rz_fg_eff + total_to + def_td + spec_tds, data = multiple_lm_2022)

summary(lm_multiple_2022)
```

The summary statistic residuals for the multiple linear regression are more evenly distributed towards the mean than our simple linear regression. Based on the residuals, we can conclude that - for 50% of the teams - the model either over or underestimated their `total_points` by just -0.35 (as listed in the `Median` residual). The interquartile range (within the `1Q` and `3Q` quartiles) are both close to the median and the `Min` and `Max` residuals both decreased significantly from our simple linear model, indicating a overall better line of fit.

We can confirm that the multiple linear regression resulted in an even distribution of the residuals by again using a Shapiro-Wilk's Test.

```{r multiple-lm-shapiro, eval = TRUE, echo = FALSE, output = TRUE}

results_lm_2022 <- residuals(lm_multiple_2022)

shapiro.test(results_lm_2022)
```

The results of the Shapiro-Wilk's test (`W = 1` and `p-value = 0.9`) confirm that residuals are indeed evenly distributed. A visualization showcases the model's even distribution of the residuals.

```{r multiple-regression-plot, output = TRUE, message = FALSE, fig.align='center', fig.dpi = 400}

mlm_2022_fitted <- predict(lm_multiple_2022)
mlm_2022_residuals <- residuals(lm_multiple_2022)

plot_data_2022 <- data.frame(Fitted = mlm_2022_fitted, Residuals = mlm_2022_residuals)

plot_data_2022 <- plot_data_2022 %>%
  cbind(teams)

ggplot(plot_data_2022, aes(x = Fitted, y = Residuals)) +
  geom_hline(yintercept = 0, color = "black", linewidth = .7) +
  stat_fit_deviations(size = 1.75, color = plot_data_2022$team_color) +
  geom_image(aes(image = team_logo_espn), asp = 16/9, size = .0325) +
  scale_x_continuous(breaks = scales::pretty_breaks(),
                     labels = scales::comma_format()) +
  scale_y_continuous(breaks = scales::pretty_breaks()) +
  labs(title = "**Multiple Linear Regression Model: 2022**",
       caption = "*An Introduction to NFL Analytic with R*<br>Brad J. Congelio") +
  xlab("Fitted Values") +
  ylab("Residual Values") +
  nfl_analytics_theme() +
  theme(panel.grid.minor.y = element_line(color = "#d0d0d0"))
```

Just as the residual values in the summary statistics indicated, plotting the `fitted_values` against the `residual_values` shows an acceptable spread in the distribution, especially given the nature of NFL data. Despite positive results in the residual values, the summary statistics of the multiple linear regression indicates a significant issue with the data. Within the `Coefficients`, it is explained that one of the items is "not defined because of singularities."

::: callout-important
"Singularities" occur in the data as a result of the dreaded multicollinearity between two or more predictors. The involved predictors were found to have a high amount of correlation between one another, meaning **that one of the variables can be predicted in a near linear fashion with one or more of the other predictive variables**. As a result, it is difficult for the regression model to correctly estimate the contribution of these dependent variables to the response variable.

The model's Coefficients of our multiple linear regression shows `NA` values for the `rz_fg_eff` predictor (the percentage of times a team made a field goal in the red zone rather than a touchdown). This is because `rz_fg_eff` was one of the predictive variables strongly correlated with another but just that it was the one dropped by the regression model to avoid producing flawed statistics as a result of the multicollinearity.

**If you are comfortable producing the lienar regression with `rz_fg_eff`** **being a dropped predictor, that are no issues with that**. However, we can create a correlation plot that allows is to determine which predictors have high correlation values with others. Examining the issue allows us to determine if `rz_fg_eff` is, indeed, the predictive variable we want the regression to drop or if we'd rather, for example, drop `rz_eff` and keep just the split between touchdowns and field goals.

```{r building-corr-matrix, output = TRUE, message = FALSE, fig.align='center', fig.dpi = 400}

regression_corr <- cor(multiple_lm_2022[, c("total_yards", "rz_eff", "rz_td_eff",
                                            "rz_fg_eff", "total_to", "def_td", "spec_tds")])

### melting the regression_corr to prep for ggplot
melted_regression_corr <- melt(regression_corr)

## plotting
ggplot(data = melted_regression_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_distiller(palette = "PuBu", direction = -1, limits = c(-1, +1)) +
  geom_text(aes(x = Var1, y = Var2, label = round(value, 2)), color = "black",
            fontface = "bold", family = "Roboto Condensed", size = 5) +
  labs(title = "Multicollinearity Correlation Matrix",
       subtitle = "Multiple Linear Regression: 2022 Data",
       caption = "**An Introduction to NFL Analytics with R**<br>Brad J. Congelio") +
  nfl_analytics_theme() +
  labs(fill = "Correlation \n Measure", x = "", y = "") +
  theme(legend.background = element_rect(fill = "#F7F7F7"),
        legend.key = element_rect(fill = "#F7F7F7"))
```

Using a correlation plot allows for easy identification of those predictive variables that have high correlation with one another. **The general rule is that two predictors become problematic in the regression model in the coefficient between the two is above 0.7 (or 0.8, given domain knowledge about the context of the data)**.

In our correlation plot, there are two squares in (indicated by the darkest blue color) that have a value greater than 0.7 (or -0.7 in this case, as both strong and negative correlations are capable of producing multicollinearity. The two squares happen to relate to the same relationship between the `rz_fg_eff` and `rz_td_eff` predictors.

Recall that the regression model automatically removed the `rz_fg_eff` from the measured Coefficients. Given the context of the data, I am not sure that is the best decision. Given we are examining the relationship the predictive variables and `total_points`, removing the `rz_fg_eff` variable inherently erases a core source of points in a game of football.

Because of this - and since our `rz_eff` predictor accounts for *both* touchdowns and field goals - I believe we could move forward on rerunning the regression without either `rz_fg_eff` and `rz_td_eff`.
:::

To run the multiple linear regression again, without the predictors relating to red zone touchdown and field efficiency, we will drop both from our `multiple_lm_2022` dataframe, rerun the regression model, and then examine the ensuing summary statistics.

```{r removing-predictor-corr, eval = TRUE, echo = TRUE, output = TRUE}

multiple_lm_2022_edit <- multiple_lm_2022 %>%
  select(-rz_td_eff, -rz_fg_eff)

lm_multiple_2022_edit <- lm(total_points ~ total_yards + rz_eff +
                         total_to + def_td + spec_tds, data = multiple_lm_2022_edit)

summary(lm_multiple_2022_edit)
```

We have certainly simplified the model by removing both `rz_td_eff` and `rz_fg_eff` but the impact of this change is a fair trade off, it seems, to avoid further issues with multicollinearity. Our new adjusted R is still high (0.784), only dropping a bit from the original model that included both predictors (0.807). Both models did well at explaining the amount of variance between the predictors and the response variable. While the `F-statistic` and the `p-value` are strong in both models, it is important to note that the `Residual standard error` dropped from 27 in the original model to 28 in the more simplified version. Given that this value is the average difference between the data's actual values and the predicted equivalents in the regression, both would ideally be smaller.

With multiple linear regression model producing acceptable results over the course of the 2022 season, we can now see if the results remain stable when produced from the course of 2012-2022.

```{r running-lm-all-seasons, eval = TRUE, echo = TRUE, output = TRUE}

multiple_lm_data_all <- multiple_lm_data %>%
  select(-rz_td_eff, -rz_fg_eff, -season)

lm_multiple_all <- lm(total_points ~ total_yards + rz_eff +
                              total_to + def_td + spec_tds, data = multiple_lm_data_all)

summary(lm_multiple_all)
```

The results of the multiple linear regression over data from the 2012-2022 indicates a statistically significant relationship between our predictor variables and a team's total yards. That said, two items are worth further exploration.

1.  The model's Residual standard error increased to 30, as opposed to the values of 27 and 28 from the models built on a single season of data. This means that the model, on average, is over or underpredicting the actual values by 30 total points. To verify that a residual standard error of 30 is not too high given the nature of our data, we can need to evaluate its value against the scale of our data based on the mean and/or median averages of the `total_points` variable. As seen below, the model's RSE as a percentage of the mean is `8.1%` and its percentage of the median is `8.2%`. Given that both values are below 10%, it is reasonable to conclude that the value of the model's residual standard error is statistically small compared to the scale of the `total_points` dependent variable.

    ::: callout-tip
    ```{r checking-scale-against-rse, eval = TRUE, echo = TRUE, output = TRUE}

    total_mean_points <- mean(multiple_lm_data_all$total_points)
    total_points_median <- median(multiple_lm_data_all$total_points)

    rse_mean_percentage <- (30 / total_mean_points) * 100
    rse_median_percentage <- (30 / total_points_median) * 100
    ```
    :::

2.  The `spec_tds` predictor, which is the total number of special teams touchdowns scored by a team, has a `p-value` of 0.61. This high of a `p-value` indicates that the amount of special teams touchdowns earned by a team is not a dependable predictor of the team's total points. Given the rarity of kickoff and punt returns, it is not surprising that the predictor returned a high `p-value`. If we run the regression again, without the `spec_tds` predictive variable, we get results that are nearly identical to the regression model that includes it as a predictor. The only significant difference is a decrease in the `F-statistic` from 398 to 317. Given the small decrease, we will keep `spec_tds` in the model.

::: callout-important
The final step of our multiple linear regression model is feeding it new data to make predictions on.

To begin, we need to create a new dataframe that holds the new predictor variables. For nothing more than fun, let's grab the highest value from each predictive variable between the 2012-2022 season.

```{r creating-predictor-variabes, eval = TRUE, echo = TRUE, output = FALSE}

new_observations <- data.frame(
  total_yards = max(multiple_lm_data$total_yards),
  rz_eff = max(multiple_lm_data$rz_eff),
  total_to = max(multiple_lm_data$total_to),
  def_td = max(multiple_lm_data$def_td),
  spec_tds = max(multiple_lm_data$spec_tds))
```

This hypothetical team gained a total of 7,317 yards in one season and was incredibly efficient in the red zone, scoring 96% of the time. It also scored nine defensive touchdowns and returned a punt or a touchdown to the house four times. Unfortunately, the offense also turned the ball over a whopping total of 41 times.

We can now pass this information into our existing model using the `predict` function and it will output the predicted `total_points` earned by this hypothetical team based on the multiple linear regression model we built with 20 years of NFL data.

```{r prediction-output, eval = TRUE, echo = TRUE, output = FALSE}

new_predictions <- predict(lm_multiple_all, newdata = new_observations)

new_predictions
```

The model determined, based on the new predictor variables provided, that this hypothetical team will score a total of 566 points, which is the second-highest cumulative amount scored by a team dating back to the 2012 season (the 2013 Denver Broncos scored 606 total points). In this situation, the hypothetical team has nearly double the turnovers as the 2013 Bronco (41 turnovers to 21). It is reasonable that providing this hypothetical team a lower number of turnovers would result in it becoming the highest scoring team since 2012.
:::

### Logistic Regressions

Logistic regressions are particularity useful when modeling the relationship between a categorical dependent variable and a given set of predictor variables. While linear models, as covered in the last section, handles data with a continuous dependent variable, logistic regressions are used when the response variable is categorical (whether a team successfully converted a third down, whether a pass was completed, whether a running back fumbled on the play, etc.). In each example, there are only two possible outcomes: "yes" or "no".

Moreover, a logistic regression model does not model the relationship between the predictors and the response variables in a linear fashion. Instead, logistic regressions using the `logistic function` that seeks to mutate the predictors into a values between 0 and 1. Specifically, the formula for a logistic regression is as follows:

$$
P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_nX_n)}}
$$

1.  $P(Y=1|X)$ in the equation is the likelihood of the event $Y=1$ taking place given the provided predictor variables ($X$).
2.  The model's coefficients are represented by $\beta_0 + \beta_1X_1 + \beta_2$.
3.  $X1, X2, ..., Xn$ represent the model's predictor variables.
4.  The $1$ in the formula's numerator results in the model's probability values always being between 0 and 1.
5.  The $1$ in the denominator is a part of the underlying logistic function and ensures that the value is always greater than or equal to 1.
6.  Finally, the $e$ is part of the model's logarithm (or the constant of 2.71828). In short, this function allows the linear combination to be a positive value.

To better explain the formula, let's assume we want to predict the probability of any one NFL team winning a game (again, in a binary `1` for winning, or `0` for losing). These binary outcome is represented by the $Y$. The predictor variables, such as the team's average points scored, the average points allowed by the opposing defense, home-field advantage, and any other statistics likely to be important in making the prediction are represented by $X1, X2, ..., Xn$. The model's coefficients, or the relationship between the predictors and a team's chance of winning, are represented by $\beta_0 + \beta_1X_1 + \beta_2$. In the event that a team's average passing yards is represented by the $B1$ coefficient, a positive number suggests that higher average passing yards results in an increase in the probability of winning. Conversely, a negative coefficient number indicates the statistic has a negative impact on winning probability.

To that end, there are three distinct types of logistic regressions: binary, multinomial, and ordinal. All three allow for working with various types of dependent variables.

**Binary regression models** are used when the dependent variable (the outcome) has just two outcomes (typically presented in binary format - `1` or `0`). Using the logistic function, a binary regression model determines the probability of the dependent event occurring given the predictor variables As highlighted below, a binary regression model can be used to predict the probability that a QB is named MVP at the conclusion of the season. The response variable is a binary (`1` indicating that the player was named MVP and `0` indicating that the player did not win MVP). Various passing statistics can serve as the predictor variables.

A **multinomial regression model** is an extension of the binary model in that it allows for the dependent variable to have more than two unordered categories. For example, a multinomial regression model would be used to predict the type of play a team is going to run (run or pass) based on the predictor variables (down and distance, time remaining, score differential, etc.).

An **ordinal regression model** is used when the dependent variables not only have ordered categories, but the structure and/or order of these categories house meaningful information. Ordinal regression could be used, for example, to possibly predict the severity of a player's injury. The ordered and meaningful categories of the dependent variable can coincide with the severity of the injury itself (mild, moderate, severe, season-ending) while the predictor variables include several other types of categories (the type of contact, the playing surface, what position the player played, how late into the season it occurred, whether the team is coming off a bye week, etc.).

#### Binary Classification

A binary regression model is used when the dependent variable is in binary format (that is, `1` for "yes" or `0` for "no). This binary represents two - and only two - possible outcomes such as a a team converting on third down or not or if a team won or lost the game. Constructing binary regression models allows use to predict the likelihood of the dependent event occurring based on the provided set of predictor variables.

We will build a binary regression model to predict the probability that an NFL QB will win the MVP at the conclusion of the season.

Let's first create a dataframe, called `qb_mvp`, that contains the statistics for quarterbacks that we intuitively believe impact the likelihood of a QB being named MVP.

```{r qb_mvp_model_data, eval = TRUE, echo = TRUE, output = FALSE}

player_stats <- nflreadr::load_player_stats(2000:2022) %>%
  filter(season_type == "REG" & position == "QB") %>%
  filter(season != 2000 & season != 2005 & season != 2006 & season != 2012)

qb_mvp_stats <- player_stats %>%
  group_by(season, player_display_name, player_id) %>%
  summarize(
    total_cmp = sum(completions, na.rm = TRUE),
    total_attempts = sum(attempts, na.rm = TRUE),
    total_yards = sum(passing_yards, na.rm = TRUE),
    total_tds = sum(passing_tds, na.rm = TRUE),
    total_ints = sum(interceptions, na.rm = TRUE),
    sacks = sum(sacks, na.rm = TRUE),
    sack_yards = sum(sack_yards, na.rm = TRUE),
    total_airyards = sum(passing_air_yards, na.rm = TRUE),
    passing_yac = sum(passing_yards_after_catch, na.rm = TRUE),
    total_firstdowns = sum(passing_first_downs, na.rm = TRUE),
    mean_epa = mean(passing_epa, na.rm = TRUE),
    mean_dakota = mean(dakota, na.rm = TRUE),
    total_carries = sum(carries, na.rm = TRUE),
    total_rush_yards = sum(rushing_yards, na.rm = TRUE),
    total_rush_tds = sum(rushing_tds, na.rm = TRUE),
    mean_rush_epa = mean(rushing_epa, na.rm = TRUE))
```

We are using data from over the course of 22 NFL seasons (2000 to 2022), but then removing the 2000, 2005, 2006, and 2012 seasons as the MVP for each was not a quarterback (Marshall Faulk, Shaun Alexander, LaDainian Tomlison, and Adrian Peterson, respectively). To begin building the model, collect QB-specific metric from the `load_player_stats()` function from `nflreadr`, including: total completions, total attempts, total yards, total touchdowns, total interceptions, sacks taken, yards loss on sacks, total air yards, total yards after catch, total first downs, the QB's mean expected points added. the mean DAKOTA (which is the adjusted EPA + CPOE composite), total carries, total rush yards, total rushing touchdowns, and the QB's average EPA earned on rushes.

The data, as collected from `load_player_stats()`, does not contain information pertaining to MVP winners. To include this, we can load a prepared file using data from Pro Football Reference. The data contains two variables: `season` and `player_name` wherein the name represents the player that won that season's MVP. After reading the data in, we can use the `mutate` function to create a new variable called `mvp` that is in binary (`1` representing that the player was the MVP). After, we can merge this data into our `qb_mvp_stats` dataframe. After merging, you will notice that the `mvp` column has mainly `NA` values. As we only brought in `1` in binary, we must set all the `NA` values to `0` to indicate that those players did not win the MVP that season. Lastly, we will limit the number of `total_attempts` to only those quarterbacks with 150 or more in a season.

```{r load-pfr-mvp-data, eval = TRUE, echo = TRUE, output = FALSE}

pfr_mvp_data <- vroom::vroom("https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/pfr_mvp_data.csv")

pfr_mvp_data$mvp <- 1

qb_mvp_stats <- qb_mvp_stats %>%
  left_join(pfr_mvp_data, by = c("season" = "season", "player_display_name" = "player_name"))

qb_mvp_stats$mvp[is.na(qb_mvp_stats$mvp)] <- 0

qb_mvp_stats <- qb_mvp_stats %>%
  filter(total_attempts >= 150)
```

The above results in a dataframe with 723 observations with 17 associated statistics for each quarterback. We can now turn to the construction of the binary regression model.

::: callout-important
If you have not already done so, please install `Tidy Models` and load it.

```{r installing-tidymodels, eval = FALSE, echo = TRUE, output = FALSE}

install.packages("tidymodels")
library(tidymodels)
```
:::

While we will not be building the binary model with the `tidymodels` package, we will be utilizing the associated `rsample` package - which is used to created various types of resamples and classes for analysis - to split our `qb_mvp_stats` data into both a training and testing set.

::: callout-important
Like many of the models to follow in this chapter, it is important to have both a training and testing set of data when performing a binary regression study for three reasons:

1.  **Assess model performance**. Have a trained set of data allows us to evaluate the models performance on the testing set, allowing us to gather information regarding how we can expect the model to handle new data.
2.  **It allow us to avoid overfitting**. Overfitting is a process that occurs when the regression model recognizes the "statistical noise" in the training data but not necessarily the underlying patterns. When this happens, the model will perform quite well on the training data but will then fail when fed new, unseen data. By splitting the data, we can use the withheld testing data to make sure the model is not "memorizing" the information in the training set.
3.  **Model selection**. In the case that you are evaluating several different model types to identify the best performing one, having a testing set allows us to determine which model type is likely to perform best when provided unseen data.
:::

The process of splitting the data into a training and testing set involves three lines of code with the `rsample` package. We first use the `initial_split` function to create a binary split of the data into a training and testing set. Moreover, we use the `strata` argument to conduct what is called "stratified sampling." Because there are very few MVPs in our data compared to those non-MVP players, using the `strata` argument allows us to create a training and test set with a similar amount of MVPs in the data. We then use the `training` and `testing` argument to create our data off of the initial split.

```{r qb-mvp-initial-split, eval = TRUE, echo = TRUE, output = FALSE}

mvp_model_split <- rsample::initial_split(qb_mvp_stats, strata = mvp)
mvp_model_train <- rsample::training(mvp_model_split)
mvp_model_test <- rsample::testing(mvp_model_split)
```

After splitting, the training set (`mvp_model_train`) contains 542 observations while the testing set (`mvp_model_test`) withholds 181 observations. With our data split, we use build our initial model using `mvp_model_train`.

```{r qb-mvp-glm-model, eval = TRUE, echo = TRUE, output = FALSE}

mvp_model <- glm(formula = mvp ~ ., family = binomial, data = mvp_model_train[,-c(1,2,3)])
```

Using `glm`, we construct our model using the `bionomial` argument while also using `[, -c(1,2,3)])` to remove the season, player name, and player ID out of the model. However, after running the model, we receive the following error:

```{r initial-glm-error, eval = FALSE, echo = TRUE, output = FALSE}

`Warning message: glm.fit: fitted probabilities numerically 0 or 1 occurred`
```

While our model still outputted results (into `mvp_model`), the error indicates that we likely have a significant problem with the data. Specifically, because a regression model works on probabilities, the error is alerting that an unknown number of these probabilities are extremely close to 0 or 1, which is often a "too good to be true" scenario. When attempting to diagnose the root cause of the error, there are several likely issues to examine:

1.  **Separation**. Separation within the model occurs when predictor variables (such as total attempts or passing yards) almost perfectly separates those players who won MVP and those who did not. As a result, what is called the "maximum likelihood estimation" fails to converge, producing unevenly distributed residuals.
2.  **Multicollinearity**. Just as we encountered in our above linear regression model (where our redzone touchdown efficiency and redzone field goal efficiency were highly correlated), there is the possibility that two or more of the predictor variables in our binary regression model are also highly correlated.
3.  **Overfitting**. Because we are providing the model 16 predictor variables, it is possible that it has become too complex and the model is capturing the associated "noise" of this complexity.

While it is certainly possible that the model is robust, despite the warning, an examination of the model's summary provides pieces of evidence that there is reason to be skeptical of the probabilities.

```{r glm-model-summary, eval = TRUE, echo = TRUE, output = FALSE}

summary(mvp_model)
```

While the `Median` of the model's residuals looks acceptable, the `Min` and `Max` residuals (-1.92987 and 2.46881) are not symmetrically distributed from zero. This is a possible indication of wide swings in the model's residuals, resulting in unusually confident predictions. As a first step to resolving this issue, we can explore **Separation** by plotting our model's predictor variables against our response variables (the binary `MVP`). In order to view all the predictor variables plotted against the response variable at once (using the `facet_wrap()` function in `ggplot2`), we must first select our desired columns and then pivot our dataframe from "wide" to "long" format.

```{r qb-mvp-stats, eval = TRUE, echo = TRUE, output = FALSE}

qb_mvp_stats_plots <- qb_mvp_stats %>%
  select(total_cmp, total_attempts, total_yards, total_tds, total_ints, sacks, sack_yards,     total_airyards, passing_yac, total_firstdowns, mean_epa, mean_dakota, total_carries,
  total_rush_yards, total_rush_tds, mean_rush_epa, mvp)

qb_mvp_stats_long <- qb_mvp_stats_plots %>%
  pivot_longer(cols = -mvp, names_to = "predictor", values_to = "value")

```

After pivoting the data into long format, we can place the information for each predictor variable into a box plot and then view them together using `facet_wrap()`.

```{r plot-regression-separation, output = TRUE, echo = TRUE, fig.align='center', fig.dpi = 400}

ggplot(qb_mvp_stats_long, aes(x = mvp, y = value, group = mvp)) +
  geom_boxplot(aes(fill = as.factor(mvp))) +
  scale_x_continuous(breaks = seq(0, 1, 1)) +
  scale_y_continuous(breaks = scales::pretty_breaks(),
                     labels = scales::comma_format()) +
  scale_fill_manual(values = c("0" = "#56b4e9", "1" = "#3c8da3")) +
  facet_wrap(~ predictor, scales = "free") +
  labs(title = "**Examining Binary Regression Separation Issues**",
       caption = "*An Introduction to NFL Analytics with R*<br>**Brad J. Congelio**") +
  xlab("MVP Binary") +
  ylab("Metric Value") +
  nfl_analytics_theme() +
  theme(legend.position = "none")
```

When examining the resulting plot of our model's response and predictor variables, we need to look for stark contrasts in the distribution of the two. Because a box plot visualizes several different statistical breakdowns, there are numerous items we must look for in the plot:

1.  **Median**. Each box plot contains a median line which represents the 50th percentile. Of interest to our examination are those instances where the median for our response variable (MVP) is much higher, or lower, than the non-MVP group. This difference is indicative of an issue with that specific variable between the two groups. For example, `mean_dakota`, `mean_epa`, `total_firstdowns`, and `total_cmp` have significantly large spread between the medians. However, give the nature of the data, it is important to trust your intuition. It makes sense that those QBs that won MVP had higher `mean_dakota` and `mean_epa`. However, it is less likely that a QB's total number of completions is as influential. The same argument can be made for `total_attempts`.
2.  **Interquartile Range**. The box for each variable represents the values between the 25th percentile and the 75th percentile of the residuals. The `total_airyards` variable shows a large difference in the size of each box. For those non-MVP players, the large range indicates a larger amount of variability within the group while those QBs that won MVP had less.
3.  **Outliers**. The black points outside the interquartile ranges are the statistical outliers for each variables. In many case, the presence of outliers in one group, but not the other, suggest a difference of the distribution between the two groups.
4.  **Overlap**. If the box plot interquartile ranges are similar, such as with `sack_yards` and `sacks`, it likely means that the predictor variable does not do a good job in explaining the difference between the two groups.
5.  **Skewness**. The opposite of overlap, skewness is concerned with a large spread between the interquartile ranges (such as seen in `total_tds`).

However, before making decisions on removing variables based on separation, we can visualize and inspect any issues of multicollinearity. Using the original `qb_mvp_stats` dataframe, we use the `cor` function on all the gathered predictor variables and then use the `melt` function from `reshape2` to prepare the data for visualization.

```{r glm-multicoll, eval = TRUE, echo = TRUE, output = FALSE}

qb_mvp_corr <- cor(qb_mvp_stats[, c("mean_dakota", "mean_epa", "mean_rush_epa", "passing_yac"                                   ,"sack_yards", "sacks", "total_airyards", "total_attempts",
                                  "total_carries", "total_cmp", "total_firstyards",
                                  "total_ints", "total_rush_tds", "total_rush_yards",
                                  "total_tds", "total_yards")])

qb_mvp_corr_melted <- melt(qb_mvp_corr)

```

```{r qb-mvp-corr-plot, output = TRUE, fig.align='center', fig.dpi = 400}

ggplot(data = qb_mvp_corr_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_distiller(palette = "PuBu", direction = -1, limits = c(-1, +1)) +
  geom_text(aes(x = Var1, y = Var2, label = round(value, 2)), color = "black",
            fontface = "bold", family = "Roboto Condensed", size = 5) +
  labs(title = "Multicollinearity Correlation Matrix",
       subtitle = "QB MVP Binary Regression",
       caption = "**An Introduction to NFL Analytics with R**<br>Brad J. Congelio") +
  nfl_analytics_theme() +
  labs(fill = "Correlation \n Measure", x = "", y = "") +
  theme(legend.background = element_rect(fill = "#F7F7F7"),
        legend.key = element_rect(fill = "#F7F7F7"),
        axis.text.x = element_text(angle = 90))
```

As explained above in our creation of a linear model, two predictors become problematic when the coefficient between the two is above 0.7. In this instance, a grand total of 20 of the model's predictor variables have this problematic relationship:

1.  `total_yards` and `total_attempts` (0.96)
2.  `total_yards` and `total_cmp` (0.98)
3.  `total_yards` and `total_firstdowns` (0.99)
4.  `total_yards` and `total_tds` (0.89)
5.  `total_tds` and `mean_dakota` (0.71)
6.  `total_tds` and `mean_epa` (0.77)
7.  `total_tds` and `total_attempts` (0.82)
8.  `total_tds` and `total_cmp` (0.85)
9.  `total_tds` and `total_firstdowns` (0.90)
10. `total_rush_yards` and `total_carries` (0.94)
11. `total_rush_tds` and `total_carries` (0.71)
12. `total_firstdowns` and `total_attempts` (0.96)
13. `total_firstdowns` and `total_cmp` (0.98)
14. `total_firstdowns` and `total_tds` (0.90)
15. `total_firstdowns` and `total_yards` (0.99)
16. `total_cmp` and `total_att` (0.98)
17. `total_airyards` and `passing_yac` (0.93)
18. `sacks` and `sack_yards` (0.95)
19. `mean_epa` and `mean_dakota` (0.90)
20. `mean_epa` and `total_tds` (0.77)

To start dropping predictor varaibles, we will first start by using problematic correlation before doubling-back to separation issues. Despite providing an overwhelming look at the relationship between our predictor variables, the correlation visualization does suggest a few clear variables to drop.

1.  Because a quarterback's expected points added is used in the calculation of DAKOTA, we can drop the model's `mean_epa` predictor variable as it is "baked" into the `mean_dakota` predictor.
2.  It is unsurprising that `total_attempts` and `total_cmp` are highly correlated, as a quarterback's total completions can only go up with increased attempts. The answer on which to drop is not as clear as `mean_epa`/`DAKOTA` as both predictor variables have further troubled relationships with others. However, our domain knowledge of the NFL should result in us dropping `total_cmps` as a QB's number of completions can be altered by forces outside of their control (such as a wide receiver dropping an easy pass, etc.).
3.  The problematic between `total_airyards` and `passing_yac` is not surprising either. Because yards after catch is inherently a wide receiver statistic, we can drop it and keep `total_airyards`.
4.  A QB's `total_carries` has high correlation with `total_rushing_tds` and `total_rushing_yds` but the latter do not. Because of this, let's drop `total_carries`.
5.  Finally, a the total number of first down passes a QB has maintains a problematic relationship with a multitude of other predictor variables (four). Although `passing_tds` has high correlation with five other variables, our intuition should result in us keeping `passing_tds` and dropping `total_firstdowns`.

After dropping the above predictor variables, we can run the model again and then check for separation issues.

```{r dropping-predictor-variables, eval = TRUE, echo = TRUE, output = FALSE}

qb_mvp_stats_updated <- qb_mvp_stats %>%
  select(total_attempts, total_yards, total_tds, total_ints, sacks, sack_yards,
         total_airyards, mean_dakota, total_rush_yards, total_rush_tds, mean_rush_epa, mvp)

qb_mvp_stats_up_long <- qb_mvp_stats_updated %>%
  pivot_longer(cols = -mvp, names_to = "predictor", values_to = "value")
```

```{r separation-new-variables, output = TRUE, fig.align='center', fig.dpi = 400}

ggplot(qb_mvp_stats_up_long, aes(x = mvp, y = value, group = mvp)) +
  geom_boxplot(aes(fill = as.factor(mvp))) +
  scale_x_continuous(breaks = seq(0, 1, 1)) +
  scale_y_continuous(breaks = scales::pretty_breaks(),
                     labels = scales::comma_format()) +
  scale_fill_manual(values = c("0" = "#56b4e9", "1" = "#3c8da3")) +
  facet_wrap(~ predictor, scales = "free") +
  labs(title = "**Examining Binary Regression Separation Issues: Take 2**",
       caption = "*An Introduction to NFL Analytics with R*<br>**Brad J. Congelio**") +
  xlab("MVP Binary") +
  ylab("Metric Value") +
  nfl_analytics_theme() +
  theme(legend.position = "none")
```

The updated look at our regression model's separation suggest further refinement of our predictor variables. Both `sack_yards` and `sacks` are incredibly symmetrical, indicating that neither statistics is all that useful in predicting whether the QB will be named MVP. The same can be said for `total_rush_tds` and `total_rush_yds`, with the additional issue of the non-MVP group having significant outliers. After removing these four additional predictor variables, we can rerun the regression model.

```{r updated-glm-model, eval = TRUE, echo = TRUE, output = TRUE}

qb_mvp_stats_new <- qb_mvp_stats_updated %>%
  select(-sack_yards, -sacks, -total_rush_tds, total_rush_yards)

qb_mvp_new_split <- rsample::initial_split(qb_mvp_stats_new, strata = mvp)
mvp_model_new_train <- rsample::training(qb_mvp_new_split)
mvp_model_new_test <- rsample::testing(qb_mvp_new_split)

mvp_model_updated <- glm(formula = mvp ~ ., family = binomial, data = mvp_model_new_train)

summary(mvp_model_updated)
```

The results from our updated binary regression model, which removed several of our predictor variables based on correlation and/or separation issues, is much more promising. Not only did the `fitted probabilities numerically 0 or 1 occurred` error disappear, but several of the predictor variables are statistically significant (with `total_tds` and `total_airyards` carrying the most significance in the model). Moreover, the deviance within the model's residuals decreased from our prior attempt (from 36.447 to 29.921).

::: callout-important
It is important to remember that having residuals evenly distributed around zero is not as important in logistic regression compared to linear regression. Because the response variable in a logistic regression is binary, the residuals cannot be normally distributed.

Instead, when using logistic regression, we can examine the `Null deviance` and `Residual deviance`. While a lower value for `Residual deviance` indicates a strong "goodness of fit", it is important to contrast the two values to fully understand how the model is performing.

In our new model, the `Null deviance` is 130.009 and the `Residual deviance` is 29.921. We can calculate the ratio of the residual deviance to the `Null deviance` by dividing the `Residual deviance` by the `Null deviance`. A resulting value closer to 1 suggests that the model is performing no better than the null model, while a lower number indicates a model that is performing better than the null model.

$$
29.921 / 130.009 = 0.23
$$

In our case, the model's ratio of the residual deviance is 0.23 which means the model, as constructed, is performing substantially better the the null model.
:::

With the model performing well on the training data, we can now evaluate the model's performance on the withheld testing data. The below approach uses the `caret` package to evaluate the model's performance.

```{r evaluting-results, eval = TRUE, echo = TRUE, output = TRUE}

qb_mvp_predicted <- predict(mvp_model_updated, mvp_model_new_test, type = "response")

threshold <- 0.5
predicted_classes <- ifelse(qb_mvp_predicted > threshold, 1, 0)

predicted_corr <- confusionMatrix(as.factor(predicted_classes), as.factor(mvp_model_new_test$mvp))

accurary <- predicted_corr$overall["Accuracy"]
precision <- predicted_corr$byClass["Pos Pred Value"]
recall <- predicted_corr$byClass["Sensitivity"]
f1_score <- 2 * (precision * recall) / (precision + recall)

print(predicted_corr)
```

The beginning "Reference" / "Prediction" matrix in the results shows the number of true positives (1), true negatives (175), false positives (4), and false negatives (1). These values are combined into a formula to determine the model's accuracy.

$$
(TP + TN) / (TP + TN + FP + FN) = 0.9724
$$

Based on these preliminary analysis, the model is 97.24% accurate, which is quite high. However, further examination of the model's sensitivity and specificity is less promising. The model's sensitivity score is 0.99, which means it correctly predicted 99.43% of the non-MVP players. However, the specificity - which is the proportion of TP out of the negative instances - indicates that the model correctly identified just 20.0% of the actual MVP players.

The balanced accuracy metric takes overall average between the model's sensitivity and specificity.

$$
(0.99 + 0.20) / 2 = 0.5972.
$$

When considering both the model's sensitivity and specificity, we achieved a balance accuracy of just 59.7%. Moreover, the model's `Kappa` score is just 0.2743, wherein a value closer to 1 indicates a strong correlation between the model's predictions and the actual outcomes, while a number closer to 0 means this correlation is better explained by random chance (think predicting the MVP in our data by random coin flips).

All together, the model is quite good at identifying non-MVP players. However, it struggles to accurately predict MVP winners. This is, of course, the opposite of what we want for this model.

::: callout-important
Indeed, our first attempt at building a binary regression model failed.

And that probably seems strange considering this is a book dedicated to teaching the analytics process using NFL data.

*But this failure is by design.*

You now know how to design a binary regression and, importantly, you are now capable of evaluating the results and understanding how the model's sensitivity and specificity, along with the Kappa score, can help determine any fallacy in an initial high accuracy score (97.2%, in this case).

If we were determined to make this model work, a further dive into exploring variable correlations and separation would be needed as well as the inclusion of new predictive variables to see if any change occurs in the results.

**However, we are going to use this failure to examine another approach to regression in the next section: Penalized Regression Techniques**. By using a ridge, lasso, or elastic net regression, the model will either "shrink" the coefficients of those features found to be less important or by controlling their size against the overall numbers in the model. We will revert back to our original model data (before removing variables based on correlation and separation) to see if using a penalized regression technique can do a better job at removing/handling problematic predictor variables than we did.
:::

#### Multinomial Regression

A multinomial regression model is used when exploring the relationship between predictor variables and a dependent variables that has more than two levels. The model itself ultimately determines the probability of each dependent variable based on the given set of predictive values. Using the softmax function to ensure that the probabilities for each dependent variable total 1, the main assumptions of th model are:

1.  the dependent variable must be categorical with multiple unordered categories
2.  the predictor variables can be a combination of continuous values or categorical
3.  much like all the other models we've worked on, there must not be multicollinearity among the predictor values
4.  the observations must be independent of each other

The specific formula for a multinomial regression is as follows:

$$
P(Y_{i} = k | X_{i}) = \frac{e^{(\beta_{k0} + \beta_{k1}X_{i1} + \beta_{k2}X_{i2} + … + \beta_{kp}X_{ip})}}{\sum_{j=1}^{K} e^{(\beta_{j0} + \beta_{j1}X_{i1} + \beta_{j2}X_{i2} + … + \beta_{jp}X_{ip})}}
$$

1.  $P(Y_{i} = k | X_{i})$ is the likelihood of the observation $i$ belonging to class $k$ given the model's predictive variables
2.  In the formula, $X_{i1}, X_{i2}, ..., X_{ip}$ are the predictive variables
3.  The coefficients are represented by $\beta_{k0}, \beta_{k1}, ...,\beta_{kp}$
4.  Finally, $K$ represents the number of categories the dependent variable has

As an example, let's build a multinomial regression model that predicts the type of offensive play call based on contextual factors of the game. We will build the model around five depending categories:

1.  run inside (noted as `middle` in the play-by-play data)
2.  run outside (noted as `left` or `right` in the play-by-play data)
3.  short pass (passes with air yards under 5 yards)
4.  medium pass (passes with air yards between 6-10 yards)
5.  long passes (passes with air yards of 11+ yards)

The construction of a multinomial regression will allow us to find the probability of each play type based on factors in the game. Such a model speaks directly to coach play-calling tendencies given the situations. To begin, we will gather data from the 2010-2022 NFL regular season. After loading the data, we will select the predictor variables and then conduct some featured engineering (which is, in this case, inspired by a presentation given by Thomas Mock on how to use `tidymodels` to predict a run or pass using binary regression)[@thomas_mock].

```{r multinomial-data-prep, eval = TRUE, echo = TRUE, output = FALSE}

pbp <- nflreadr::load_pbp(2006:2022) %>%
  filter(season_type == "REG")

pbp_prep <- pbp %>%
  select(
    game_id, game_date, game_seconds_remaining, week, season,
    play_type, yards_gained, ydstogo, down, yardline_100, qtr, posteam,
    posteam_score, defteam, defteam_score, score_differential, shotgun,
    no_huddle, posteam_timeouts_remaining, defteam_timeouts_remaining,
    wp, penalty, half_seconds_remaining, goal_to_go, td_prob, fixed_drive, run_location, air_yards) %>%
  filter(play_type %in% c("run", "pass"), penalty == 0, !is.na(down), !is.na(yardline_100)) %>%
  mutate(in_red_zone = if_else(yardline_100 <= 20, 1, 0),
         in_fg_range = if_else(yardline_100 <= 35, 1, 0),
         two_min_drill = if_else(half_seconds_remaining <= 120, 1, 0)) %>%
  select(-penalty, -half_seconds_remaining)

tic()
model_data <- pbp_prep %>%
  group_by(game_id, posteam) %>%
  mutate(run_inside = fcase(
    play_type == "run" & run_location == "middle", 1,
    play_type == "run", 0,
    default = 0),
    run_outside = fcase(
      play_type == "run" & (run_location == "left" | run_location == "right"), 1,
      play_type == "run", 0,
      default = 0),
    short_pass = fcase(
      play_type == "pass" & air_yards <= 5, 1,
      play_type == "pass", 0,
      default = 0),
    medium_pass = fcase(
      play_type == "pass" & air_yards > 5 & air_yards <= 10, 1,
      play_type == "pass", 0,
      default = 0),
    long_pass = fcase(
      play_type == "pass" & air_yards > 10, 1,
      play_type == "pass", 0,
      default = 0),
    run = if_else(play_type == "run", 1, 0),
    pass = if_else(play_type == "pass", 1, 0),
    total_runs = if_else(play_type == "run", cumsum(run) - 1, cumsum(run)),
    total_pass = if_else(play_type == "pass", cumsum(pass) - 1, cumsum(pass)),
    previous_play = if_else(posteam == lag(posteam),
                            lag(play_type), "First play of drive"),
    previous_play = if_else(is.na(previous_play),
                            replace_na("First play of drive"), previous_play)) %>%
  ungroup() %>%
  mutate(across(c(play_type, season, posteam, defteam, shotgun, down, qtr, no_huddle,
                 posteam_timeouts_remaining, defteam_timeouts_remaining, in_red_zone,
                 in_fg_range, previous_play, goal_to_go, two_min_drill), as.factor)) %>%
  select(-run, -pass, -air_yards, -run_location)
toc() ### 1352.5 seconds (22 minutes) using case_when()
      ### 17.69 seconds using fcase()
```

#### Ordinal Regression

nfl player injury severity? could be interesting.

## Penalized Regression Techniques

#### Ridge Regression

examples of ridge regression using nfl data.

#### Lasso Regression

examples of lasso regression using nfl data.

#### Elastic Net Regression

examples of elastic ridge regression using nfl data.

### Generalized Linear and Additive Models (GLM/GAM)

intro to both GLM and GAM here.

#### Poisson Regression (GLM)

example of poisson regression using nfl data.

#### Polynominal Regression(GAM)

example of polynominal regression using nfl data.

## Advanced Modeling Techniques

introduction to advanced modeling techniques in the nfl.

### Clustering

introduction to clustering.

#### K-means Clustering

examples of k-means clustering here.

#### Hierarchical Clustering

examples of hierarchical clustering here.

### Decision Trees/Random Forests

introduction to decision trees/random forests in nfl data.

#### Classification Trees

example of classification tree using nfl data.

#### Regression Trees

example of regression tree using nfl data.

## Creating Our Own Rushing Yards Over Expected Model

Back in January of 2021, Tej Seth posted an article to the [Michigan Football Analytics Society blog](https://mfootballanalytics.com/2021/01/14/creating-a-public-expected-rushing-yards-model/) that outlined his vision for creating a "public expected rushing yards model." The structure of his model, as explained by Tej, was inspired by the prior work of Michael Egle, an honorable mention in both the 2021 and 2023 NFL Big Data Bowl, who previously used the college football equivalent of open-source data ([`cfbfastR`](https://cfbfastr.sportsdataverse.org/)) to create an RYOE model for the collegiate game.[^05-nfl-analytics-advanced-methods-1] In Tej's case, his approach to created an NFL-centric RYOE model culminated with the creation of his [RYOE Shiny App](https://mfbanalytics.shinyapps.io/RYOE/) that allows anybody to explore his RYOE metric by season or team and even through three-way rusher comparisons.

[^05-nfl-analytics-advanced-methods-1]: Thanks to Tej Seth for briefly chatting with me over Twitter regarding the history of both Michael's RYOE model and his own.

Despite a slightly intimidating title, rushing yards over expected is a fantastic entrypoint into exploring model creation and analysis in NFL analytics - in fact, the growing number of "over expected" metrics in the NFL are all great ways to begin learning about and understanding advanced modeling. Robby Greerre, the owner of [nfeloapp.com](https://www.nfeloapp.com/) - a website that provides "data-driven analytics, picks, and predictions for the NFL" - explains that over expected metrics are a increasingly popular avenue in which analysts can "paint a more accurate picture of performance by adjusting familiar statistics like 'completion percentage' or 'yards per rush' for conflating factors like degree of difficulty or game text" [@nfelo].

Some of these metrics, like completion percentage over expected (CPOE), are widely accepted. Specifically, CPOE calculates how likely any quarterback's pass is going to be complete or incomplete compared to other passing attempts. It is considered "widely accepted" because the metric itself is considered "stable" in that the r-squared value retains a strong correlation for individual quarterbacks across several seasons. In fact, as Greerre points out, the r-squared value for CPOE for just one season is 0.226 which is extremely strong based on NFL analytics standards.

On the other hand, RYOE - based on Greerre's analysis - maintains an r-squared value below 0.15 until a running back's fourth season, wherein the average improves to 0.263 (an otherwise stable value). But that does not mean that RYOE is not a metric worth further exploration. The effectiveness of any one metric to account for factors such as degree of difficulty or game text largely relies on our ability to provide adequate feature engineering - specifically, how much relevant data the machine learning model can ingest to begin making predictions.

Because of that, significant machine learning models have been built with information provided by the NFL's Big Data Bowl as it is the one chance that the public receives to feature engineer with the NFL's tracking data (wherein a player's position, speed, direction, etc. is tracked and recorded every 1/10th of a second). Unfortunately, only small windows of data exist from the Big Data Bowl releases and, as a result, we are often required to find creative ways to provide further context to each play/player for the machine learning model.

To showcase this idea, we are going to begin exploring ways to add additional feature engineering to Tej Seth's already fantastic Rushing Yard Over Expected model. While not the most stable metric, as mentioned, the idea of RYOE is generally easy to understand for even the most analyst. Broadly, given what we know about every other rushing play during a specific span of seasons, what is the most likely amount of yards a running back is going to gain on a specific rushing play as predicted by the model on other similar situations?

**That difference is rushing yards over expected**.

Using Tej's Shiny app, we can explore all seasons between 2015 and 2022 for those running backs that had a minimum of 755 rushing attempts.

```{r tej-seth-shiny-results, echo = FALSE, fig.cap = "Credit: Tej Seth"}
knitr::include_graphics(rep("images/ryoe_2015_2022.jpg"))
```

According to Tej's model, since 2015, Nick Chubb of the Cleveland Browns earned - on average - 0.66 over expected. Aaron Jones is closely behind with 0.61 over expected and then a significant drop occurs for the third and fourth players.

To understand how Tej engineered his model and to begin exploring other possible features to feed into the model, we can dive into his [publicly available code](https://github.com/tejseth/RYOE/blob/main/ryoe-2.R).

### Tej Seth's RYOE Model: Under The Hood

::: callout-important
It is important to immediately point out that Tej built his RYOE model using the `xgboost` package whereas we will begin constructing ours using `tidymodels`.

While the underlying eXtreme Gradient Boosting process is largely the same with both approaches, the necessary framework we will construct with `tidymodels` differs *greatly* from the coding used with the `xgboost` package.

The `xgboost` package is a standalone package in R that provide an implementation of the eXtreme Gradient Boosting algorithm. To that end, it offers a highly efficient and flexible way to train gradient boosting models for various machine learning tasks, such as classification, regression, and ranking. The package provides its own set of functions for training, cross-validation, prediction, and feature importance evaluation.

The `tidymodels` package, on the other hand, is a *collection* of R packages that provide a unified framework for modeling and machine learning tasks. It includes several package for data preprocessing, modeling, validation, and evaluation. Because of this, the core goal of the `tidymodels` teams is to offer a consistent syntax and workflow for a wide range of machine learning models. It is a fair comparison to say that `tidymodels` is the `tidyverse` of the machine learning world.
:::

Just like the model we will be building in this chapter, Tej constructed his model via eXtreme Gradient Boosting.

Which may lead to a very obvious question if you are new to machine learning: **what exactly *is* eXtreme Gradient Boosting?**

### eXtreme Gradient Boosting Explained

eXtreme Gradient Boosting (XGBoost) is a powerful machine learning technique that is particularly good at solving supervised machine learning problems, such as classification (categorizing data into classes, for example) and regression (predicting numerical values).

XGBoost can be thought of as an "expert team" that combines the knowledge and skills of multiple "individual experts" to make better decisions or predictions. Each of these "experts" in this context is what we call a decision tree, which is a flowchart structure used for making decisions based on a series of question about the data.

Once provided data, XGBoost seeks to iteratively build a collection of "bad" decision trees and then build an ensemble of these poor ones into a more accurate and robust model. The term "gradient" comes from the fact that the algorithm uses the gradient (or the slope) of the loss function (a measure of how well the model fits the data) to guide the learning process.

## Building a RYOE Model with `tidymodels`

Place holding for the tidymodels code below.

```{r ryoe-model-in-tidy, eval = FALSE, echo = TRUE, output = FALSE}

library(tidyverse)
library(nflverse)
library(tidymodels)
library(tune)
library(bon)

options(scipen = 999)
options(digits = 3)

### reading in NFL play-by-play data from 2016 to 2022
pbp <- nflreadr::load_pbp(2016:2022)

### filtering to just rushing attempts that are not missing any yards_gained
rush_attempts <- pbp %>%
  filter(season_type == "REG") %>%
  filter(rush_attempt == 1, qb_scramble == 0,
         qb_dropback == 0, !is.na(yards_gained)) 

### quickly calculating each defteam's avg. rushing yards allowed per season
def_ypc <- rush_attempts %>%
  filter(!is.na(defteam)) %>%
  group_by(defteam, season) %>%
  summarize(def_ypc = mean(yards_gained))

### joining the defteam avg yards gained into the rushing data
rush_attempts <- rush_attempts %>%
  left_join(def_ypc, by = c("defteam", "season"))

### gathering offensive formation, offensive personnel, defensive personnel, and defenders in box
participation <- nflreadr::load_participation(seasons = 2016:2022) %>%
  select(nflverse_game_id, play_id, possession_team, offense_formation,
         offense_personnel, defense_personnel, defenders_in_box)

### merging participation data into our rushing attempts data
### note: the team match is likely not necessary but I was being careful
rush_attempts <- rush_attempts %>%
  left_join(participation, by = c("game_id" = "nflverse_game_id",
                                  "play_id" = "play_id",
                                  "posteam" = "possession_team"))

### creating a secondary dataframe for joining back in player names
rushing_data_join <- rush_attempts %>%
  group_by(game_id, rusher, fixed_drive) %>%
  mutate(drive_rush_count = cumsum(rush_attempt)) %>%
  ungroup() %>%
  group_by(game_id, rusher) %>%
  mutate(game_rush_count = cumsum(rush_attempt)) %>%
  mutate(rush_prob = (1 - xpass) * 100,
         strat_score = rush_prob / defenders_in_box,
         wp = wp * 100) %>%
  ungroup() %>%
  mutate(red_zone = if_else(yardline_100 <= 20, 1, 0),
         fg_range = if_else(yardline_100 <= 35, 1, 0),
         two_min_drill = if_else(half_seconds_remaining <= 120, 1, 0)) %>%
  select(label = yards_gained, season, week, yardline_100, quarter_seconds_remaining,
         half_seconds_remaining, qtr, down, ydstogo, shotgun, no_huddle,
         ep, wp, drive_rush_count, game_rush_count, red_zone, fg_range, two_min_drill,
         offense_formation, offense_personnel, defense_personnel, defenders_in_box,
         rusher, rush_prob, def_ypc, strat_score, rusher_player_id, posteam, defteam) %>%
  na.omit()

### bringing in pre-aggregated next gen stats
next_gen_stats <- nflreadr::load_nextgen_stats(seasons = 2016:2022, stat_type = "rushing") %>%
  filter(week > 0 & season_type == "REG") %>%
  select(season, week, player_gsis_id,
         against_eight = percent_attempts_gte_eight_defenders, avg_time_to_los)

### merging in next gen stats
rushing_data_join <- rushing_data_join %>%
  left_join(next_gen_stats, by = c("season", "week", "rusher_player_id" = "player_gsis_id")) %>%
  na.omit()

### placing offense personnel positions in individual columns
### new column: extra_ol
rushing_data_join <- rushing_data_join %>%
  mutate(
    ol = str_extract(offense_personnel, "(?<=\\s|^)\\d+(?=\\sOL)") %>% as.numeric(),
    rb = str_extract(offense_personnel, "(?<=\\s|^)\\d+(?=\\sRB)") %>% as.numeric(),
    te = str_extract(offense_personnel, "(?<=\\s|^)\\d+(?=\\sTE)") %>% as.numeric(),
    wr = str_extract(offense_personnel, "(?<=\\s|^)\\d+(?=\\sWR)") %>% as.numeric()) %>%
  replace_na(list(ol = 5)) %>%
  mutate(extra_ol = if_else(ol > 5, 1, 0)) %>%
  mutate(across(ol:wr, as.factor)) %>%
  select(-ol, -offense_personnel)

### doing some as above but for defense personnel
rushing_data_join <- rushing_data_join %>%
  mutate(dl = str_extract(defense_personnel, "(?<=\\s|^)\\d+(?=\\sDL)") %>% as.numeric(),
         lb = str_extract(defense_personnel, "(?<=\\s|^)\\d+(?=\\sLB)") %>% as.numeric(),
         db = str_extract(defense_personnel, "(?<=\\s|^)\\d+(?=\\sLB)") %>% as.numeric()) %>%
  mutate(across(dl:db, as.factor)) %>%
  select(-defense_personnel)

rushing_data_join <- rushing_data_join %>%
  filter(qtr < 5) %>% ### let's remove rushes that took place in OT
  mutate(qtr = as.factor(qtr),
         down = as.factor(down),
         shotgun = as.factor(shotgun),
         no_huddle = as.factor(no_huddle),
         red_zone = as.factor(red_zone),
         fg_range = as.factor(fg_range),
         two_min_drill = as.factor(two_min_drill),
         extra_ol = as.factor(extra_ol))

### going to build model from rushes so will remove identifying information
rushes <- rushing_data_join %>%
  select(-season, -week, -rusher, -rusher_player_id, -posteam, -defteam) %>%
  mutate(across(where(is.character), as.factor))

str(rushes)

#################################
## tidymodels work now
################################
set.seed(1984)

str(rushes)

rushing_split <- initial_split(rushes)
rushing_train <- training(rushing_split)
rushing_test <- testing(rushing_split)
rushing_folds <- vfold_cv(rushing_train)

### creating our xgboost recipe
rushing_recipe <-
  recipe(formula = label ~ ., data = rushing_train) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE)

### creating the model boosting tree specifications
rushing_specs <- boost_tree(
  trees = tune(),
  tree_depth = tune(), 
  min_n = tune(),
  mtry = tune(),
  loss_reduction = tune(),
  sample_size = tune(),
  learn_rate = tune(),
  stop_iter = tune()) %>%
  set_engine("lightgbm", num_leaves = tune()) %>%
  set_mode("regression")

### creating the tuning grid
rushing_grid <- grid_latin_hypercube(
  trees(),
  tree_depth(),
  finalize(mtry(), rushes),
  min_n(),
  num_leaves(),
  loss_reduction(),
  sample_size = sample_prop(),
  learn_rate(),
  stop_iter(),
  size = 5)

### adding everything into the workflow
rushing_workflow <-
  workflow() %>%
  add_recipe(rushing_recipe) %>%
  add_model(rushing_specs)

registerDoSEQ() ### to leave parallel processing
registerDoParallel(cores = 11)
doParallel::stopImplicitCluster()

### tuning the grid with tictoc() running to see how long it takes
tic()
rushing_tune <-
  tune_grid(rushing_workflow, resamples = rushing_folds, grid = rushing_grid, control = control_grid(save_pred = TRUE,
                                                                                                     verbose = TRUE))
toc() ## 344.16 seconds/5 minutes (100 trees, 30 grid)
      ## 5783.15 seconds/ 96 minutes (1000 trees, 60 grid)
      ## 15643.44 seconds / 4.5 hours (tune() trees, 100 grid)

### extracting the best performing hyperparameters from the tuning results
best_params <- rushing_tune %>%
  select_best(metric = "rmse")

### creating a final workflow with this updated model specification
rushing_final_workflow <- rushing_workflow %>%
  finalize_workflow(best_params)

### fitting the workflow on the testing data
final_model <- rushing_final_workflow %>%
  fit(data = rushing_test)

### using final mod to add the predictins to our prior rushing_data_join information
rushing_predictions <- predict(final_model, rushing_data_join)

### creating our projections
ryoe_projs <- cbind(rushing_data_join, rushing_predictions) %>%
  rename(actual_yards = label,
         exp_yards = .pred)

### doing some math to find the league-wide average of mean_ryoe per season
mean_ryoe <- ryoe_projs %>%
  dplyr::group_by(season) %>%
  summarize(nfl_mean_ryoe = mean(actual_yards) - mean(exp_yards))

### merging in the mean_ryoe into the data per season
ryoe_projs <- ryoe_projs %>%
  left_join(mean_ryoe, by = c("season" = "season"))

### taking a player's actual yards minus his expected yards and then weighing it against NFL average per season
ryoe_projs <- ryoe_projs %>%
  mutate(ryoe = actual_yards - exp_yards + nfl_mean_ryoe)

### outputting the results
ryoe_projs %>%
  group_by(rusher) %>%
  summarize(
    rushes = n(),
    yards = sum(actual_yards),
    exp_yards = sum(exp_yards),
    ypc = yards / rushes,
    exp_ypc = exp_yards / rushes,
    avg_ryoe = mean(ryoe)) %>%
  filter(rushes > 1000) %>%
  arrange(-avg_ryoe)
```

fffff

```{r ryoe-r2-plot, eval = FALSE, echo = TRUE, output = FALSE}

ggplot(data = for_plot, aes(x = yards, y = exp_yards)) +
  stat_poly_line(formula = y ~ x + 0, 
                 se = FALSE, color = "black", 
                 linewidth = .8, linetype = "dashed") +
  stat_poly_eq(formula = y ~ x + 0,
               aes(label = after_stat(rr.label), 
                   family = "Roboto Condensed", 
                   size = 9, fontface = "bold"),
               label.y = 1, label.x = 0.4) +
  geom_point(shape = 21, fill = for_plot$team_color, 
             color = for_plot$team_color2, size = for_plot$rushes / 200) +
  scale_x_continuous(breaks = scales::pretty_breaks(),
                     label = scales::comma_format()) +
  scale_y_continuous(breaks = scales::pretty_breaks(),
                     label = scales::comma_format()) +
  nfl_analytics_theme() +
  labs(x = "Actual Rushing Yards",
       y = "Expected Rushing Yards",
       title = "Actual Rushing Yards vs. Expected Rushing Yards",
       subtitle = "2016 - 2022   |   Model: LightGBM",
       caption = "An Introduction to NFL Analytics with R\nBrad J. Congelio") +
  geom_text_repel(data = filter(for_plot, yards >= 4500),
                  aes(label = rusher), box.padding = .8, 
                  point.padding = 1, family = "Roboto Condensed")

```

## Navigating the NFL's Big Data Bowl

lots of place holding for Big Data Bowl material below

Thomas Bliss, a Data Scientist with the NFL, [provided an incredibly helpful list of potential topics](https://www.kaggle.com/competitions/nfl-big-data-bowl-2023/discussion/365497) that can be explored using 2023 Big Data Bowl information:

1.  analyze blocker positioning after the QB leaves the pocket and/or is pressured
2.  analyze blocker ability to hold a defender in place without moving towards the QB
3.  link the rate of false starts to an offensive line's time off the snap
4.  link between QB release time, receiver separation, and offensive line performance

```{r bdb-reading-writing-data, eval = FALSE, echo = TRUE}

### reading all weeks and writing to a parquet file
all_bdb_weeks <- function(dir = file.path('core-data')) {
  paths <- fs::dir_ls(dir, regexp = 'week\\d+')
  all_weeks <-
    paths %>%
    purrr::map_df(vroom::vroom) %>%
    janitor::clean_names() %>%
    arrow::write_parquet(file.path('core-data', 'data.parquet'))
}

all_bdb_weeks()

### reading in all play information provided and writing to a parquet file
read_bdb_plays <- memoise::memoise({function() {
  plays <- file.path('core-data', 'plays.csv') %>%
    readr::read_csv() %>%
    janitor::clean_names() %>%
    arrow::write_parquet(file.path('core-data', 'plays.parquet'))
}})

read_bdb_plays()

### reading in individual game information and writing to a parquet file
read_game_info <- memoise::memoise({function() {
  file.path('core-data', 'games.csv') %>%
    readr::read_csv() %>%
    janitor::clean_names() %>%
    dplyr::mutate(dplyr::across(game_date = lubridate::mdy)) %>%
    arrow::write_parquet(file.path('core-data', 'games.parquet'))
}})

### creating an individual .cvs file for each game in bdb
all.weeks <- read_parquet("./core-data/large-lfs-files/all-weeks-parquet")

all.weeks %>%
  group_by(game_id) %>%
  group_walk(~ write_csv(.x, paste0('tracking_gameId_', .y$game_id, ".csv")))

### writing in team colors, logos
team.colors <- nflfastR::teams_colors_logos %>%
  select(team_abbr, team_color, team_color2, team_logo_espn)
```

fffff

```{r roating-dots-function, eval = FALSE, echo = TRUE}

rotate_the_dots <- function(df) {
  
  if(!"play_direction" %in% names(df)) {
    message("Cannot find play directions. Inferring from offense and defense locations at snap.")
    
    df <- df %>%
      filter(event == "ball_snap", team != "football") %>%
      group_by(game_id, play_id, defensive_team) %>%
      summarize(mean_x = mean(x, na.rm = T)) %>%
      pivot_wider(names_from = defensive_team, values_from = mean_x, names_prefix = "x_") %>%
      ungroup() %>%
      mutate(
        play_direction = 
          ifelse(
            x_1 > x_0,
            "right",
            "left") %>%
          select(game_id, play_id, play_direction) %>%
          inner_join(df, by = c("game_id", "play_id")))
        
  }
  
  df <- df %>%
    mutate(
      to_left = ifelse(play_direction == "left", 1, 0),
      x = ifelse(to_left == 1, 120 - x, x),
      y = ifelse(to_left == 1, 160 / 3 - y, y),
      los_x = 100 - absolute_yardline_number,
      dist_from_los = x - los_x)
  
  if ("o" %in% names(df)) {
    df <- df %>%
      mutate(
        o = ifelse(to_left == 1, o + 180, o),
        o = ifelse(o > 360, 0 - 360, o),
        o_rad = pi * (o / 180),
        o_x = ifelse(is.na(o), NA_real_, sin(o_rad)),
        o_y = ifelse(is.na(o), NA_real_, cos(o_rad)))
  }
  
  if ("dir" %in% names(df)) {
    df <- df %>%
      mutate(
        dir = ifelse(to_left == 1, dir + 180, dir),
        dir = ifelse(dir > 360, dir - 360, dir),
        dir_rad = pi * (dir / 180),
        dir_x = ifelse(is.na(dir), NA_real_, sin(dir_rad)),
        dir_y = ifelse(is.na(dir), NA_real_, cos(dir_rad)),
        s_x = dir_x * s,
        s_y = dir_y * s,
        a_x = dir_x * a,
        a_y = dir_y * a)
  }
  
  return(df)
}

```

ffff

```{r finding-orientation-difference, eval = FALSE, echo = TRUE}

find_o_diff <- function(df, prefix = "qb") {
  
  name_x <- sym(paste0(prefix, "_x"))
  name_y <- sym(paste0(prefix, "_y"))
  
  new_column <- paste0("o_to_", prefix)
  
  df <- df %>%
    mutate(
      dis_x = {{name_x}} - x,
      dis_y = {{name_y}} - y,
      
      tmp = atan2(dis_y, dis_x) * (180 / pi),
      tmp = (360 - tmp) + 90,
      tmp = case_when(tmp < 0 ~ tmp + 360,
                      tmp > 360 ~ tmp - 360,
                      TRUE ~ tmp),
      
      diff = abs(o - tmp),
      
      diff = abs(o - tmp),
      
      !!new_column := pmin(360 - diff, diff)) %>%
        select(-diff, -tmp)
      
    return(df)
}

```

```{r building-convex-hulls, eval = FALSE, echo = TRUE}

########
## READING IN PITTSBURGH VS. BUFFALO - WEEK 1
########

pitt.buff <- arrow::read_parquet("core-data/large-lfs-files/all-weeks-parquet") %>%
  filter(game_id == "2021091201")

########
## READING IN PLAYS FROM PITTSBURGH VS. BUFFALO - WEEK 1
########

pitt.buff.plays <- readr::read_csv("core-data/plays.csv") %>%
  janitor::clean_names() %>%
  filter(game_id == "2021091201")

########
## READING IN GAME INFO FROM PITTSBURGH VS. BUFFALO - WEEK 1
########

pitt.buff.info <- readr::read_csv("core-data/games.csv") %>%
  janitor::clean_names() %>%
  filter(game_id == "2021091201")

########
## COMBING CORE GAME FILE WITH PLAYS, AND THEN BY INFO (TO AVOID MULTIPLE GAME_IDs IN DF)
########

complete.data <- inner_join(pitt.buff, pitt.buff.plays, by = c("game_id" = "game_id", "play_id" = "play_id"))

complete.data <- complete.data %>%
  inner_join(pitt.buff.info, by = c("game_id" = "game_id"))

### ROTATING THE DOTS
complete.data <- rotate_the_dots(complete.data)

########
## MUTATING TO CHARACTER VARIABLE DEFINING WHETHER TEAM IN FRAMES IS ON OFFENSE OR DEFENSE
########

complete.data <- complete.data %>%
  mutate(off_or_def = case_when(
    team == possession_team ~ "offense",
    team != possession_team ~ "defense",
    TRUE ~ "football"))

            ########
            ## CORE CLEANING AND PREP IS COMPLETE
            ########

########
## ADDING IN INFORMATION FROM PLAYERS.CSV TO BUILD CHULLs FOR JUST O-LINE
########

player.info <- readr::read_csv("core-data/players.csv") %>%
  janitor::clean_names() %>%
  select(nfl_id, official_position, display_name)

complete.data <- complete.data %>%
  inner_join(player.info, by = c("nfl_id" = "nfl_id"))

########
## LET'S PICK OUT A FUN PLAY TO WORK WITH
########

one.play <- complete.data %>%  ### BEN PASS TO EBRON FOR 19 YARDS
  filter(play_id == 2209)

########
## NOW LET'S BUILD A CONVEX HULL FOR JUST THE OFFENSIVE LINE
########

ol_chull_order <- one.play %>%
  filter(off_or_def == "offense") %>%
  filter(official_position %in% c("T", "C", "G")) %>%  #### IMPORTANT TO KNOW PERSONNEL PACKAGE HERE: 0 RB, 0 TE, 5 WR
  select(frame_id, x, y) %>%
  chull

ol_chull_order <- c(ol_chull_order, ol_chull_order[1])

ol_chull_coords <- one.play %>%
  filter(off_or_def == "offense") %>%
  select(frame_id, x, y) %>%
  slice(ol_chull_order)

ol_chull_poly <- sp::Polygon(ol_chull_coords, hole = F)
ol_chull_area <- ol_chull_poly@area

########
## NOW LET'S PLOT IT
########

one.play %>%
  the_dots(
    animated = TRUE,
    orientation = FALSE,
    convex = TRUE,
    segment_length = 6,
    segment_size = 3,
    dot_size = 4,
    animated_h = 4,
    animated_w = 8,
    animated_res = 150
  )

```
