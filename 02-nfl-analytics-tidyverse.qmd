# Wrangling NFL Data in the `tidyverse`

```{r setup-ch2, include = FALSE}

source("R/book-functions.R")

library(tidyverse)
library(nflverse)

pbp <- nflreadr::load_pbp(2022)
```

## Downloading R and RStudio

## Installing Necessary Packages

## The `tidyverse` and Its Verbs

The `tidyverse` is a collection of R packages designed for data manipulation, visualization, and analysis. It was developed by Hadley Wickham, the Chief Scientist at RStudio, and a varied team of contributors. The goal of the `tidyverse` is to provide a consistent, easy-to-understand set of functions and syntax for working with data in R.

The core principle of the `tidyverse` is "tidy data," which is the development team's belief in creating a standard way of organizing data sets so that they can be easily manipulated, visualized, and analyzed. To that end, a "tidy" data set is one that is comprised of observations (rows) and variables (columns) with each variable being a distinct piece of information and each observation being a unit of analysis.

Installing and loading the `tidyverse` results eight of the core packages automatically being loaded and ready to use:

1.  **dplyr:** "dplyr provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges."
2.  **tidyr:** "tidyr provides a set of functions that help you get to tidy data. Tidy data is data with a consistent form: in brief, every variable goes in a column, and every column is a variable."
3.  **readr:** "readr provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf). It is deigned to flexibly parse many types of data found in the wild, while still cleanly failing when data unexpectedly changes."
4.  **purrr:** "purrr enhances R's functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. Once you master the basic concepts, purrr allows you to replace many for loops with code that is easier to write and more expressive."
5.  **tibble**: "tibble is a modern re-imagining of the data frame, keeping what time has proven to be effective, and throwing out what it has not. Tibbles are data.frames that are lazy and surly; they do less and complain more forcing you to confront problems earlier, typically leading to cleaner, more expressive code."
6.  **stringr:** "stringr provides a cohesive set of functions designed to make working with strings as easy as possible. It is built on top of stringi, which uses the ICU C library to provide fast, correct implementations of common string manipulations."
7.  **forcats:** "forcats provides a suite of useful tools that solve common problems with factors. R uses factors to handle categorical variables, variables that have a fixed and known set of possible values."
8.  **ggplot2:** "ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map the variables to aesthetics, what graphical primitives to use, and it takes care of the details" [@tidyverse].

Aside from the core eight packages, the `tidyverse` will also install a multiple of other packages such as `rvest` (for web scraping), `readxl` (for reading Excel sheets in the RStudio environment), `lubridate` (a very powerful tool for working with times and dates), and `magrittr` (the package that provides the pipe `%>%`). As well, prior versions of the `tidyverse` utilized the `modelr`. Modeling is now handled in the `tidyverse` by the `tidymodels` package.

## The Flow of the `tidyverse`

The underlying design of coding in the `tidyverse`, aside from the `dplyr` verbs are both the assignment statement (`<-`) and the pipe (`%>%`). Please note, as mentioned in the book's Preface, that I still use the pipe (`%>%`) that is part of the `magrittr` package and not the native pipe operator (`|>`) included in the 4.1 release of R. The choice of pipe operator you use is your decision to make, as either will work seamlessly within the examples and activities provided in this book.

As I explain to my Sports Analytics students, the language and flow of the `tidyverse` can seem like a foreign language at first. But, it is important that you stick with it because, sooner rather than later, the light bulb above your head will go off. Before detailing the in's and out's of the `tidyverse` in the below section, let's first dissect an example of the `tidyverse` workflow.

```{r example-tidyverse, eval = FALSE, echo = TRUE, output = FALSE}

pbp <- nflreadr::load_pbp(2022) %>%
  filter(posteam == "PHI" & rush == 1) %>%
  group_by(rusher) %>%
  summarize(success_rate = mean(success))
  
```

The given example involves multiple iterations of the `tidyverse` paradigm. At the outset of my Sport Analytics course, when introducing the concepts of the `tidyverse`, I emphasize that it is possible to "talk your way through" the process from the beginning to your end goal (especially once you have you have a more comprehensive understanding of the `dplyr` verbs, which are expounded upon in the subsequent section). The following stepwise method illustrates this using the above example of code:

1.  We first create a dataframe, denoted by `pbp`, by utilizing the `load_pbp` function from the `nflreadr` package. To talk through this, you can say "`pbp`is an copy of `nflreadr::load_pbp(2022`." While R language purists may laugh at teaching the `tidyverse` in such language, it does indeed work. Going forward, the assignment operator (`<-`) simply implies that "*something is*." In this case, our `pbp` dataframe *is* the containers for the play-by-play data we are collecting from `nflreadr`.
2.  We then move into our first pipe operator (`%>%`). Again, R language purist will likely develop a eye twitch upon reading this, but I explain to my students that the pipe operator serves as a *"... and then"* command. In terms of the "talk it out" method above, the flow would be: "`pbp` is a copy of `nflreadr::load_pbp(2022)` and then ..."
3.  After the pipe operator (or the first "and then ..." command), we move into our first `dplyr` verb. In this case, we are using the `filter` verb to select just the Philadelphia Eagles as the offensive team and just offensive plays that are rush attempts. With another pipe operator, we are including a second "*... and then"* command.
4.  To finish the example, we are grouping by each individual rusher on the Eagles "*and then"* summarize the average success rate for each rusher.

**To put it together, "talking it out" from beginning to end results in:**

"First, create a dataframe called `pbp` that is a copy of `nflreadr::load_pbp(2022)` *and then* `filter` for all instances where the `posteam` is `PHI` and `rush == 1` *and then* `group_by` each individual rusher, *and then* summarize the average success rate for each rusher into a new column titled `success_rate`."

To showcase this visually, the "talking through" method is inputted into the example code below:

```{r talk-through-it-method, eval = FALSE, echo = TRUE, output = FALSE}

pbp <- "is" nflreadr::load_pbp(2022) %>% "... and then"
  filter(posteam == "PHI" & rush == 1) %>% "... and then"
  group_by(rusher) %>% "... and then"
  summarize(success_rate = mean(success))

```

## Working With NFL Data and the `dplyr` Verbs

Of the packages nestled within the `tidyverse`, `dplyr` is perhaps the most important in terms of wrangling and cleaning data. As mentioned above, `dplyr` is a powerful tool for data manipulation in R as it provides a key set of functions, known as verbs, that are designed to be easy to user and understand. The verbs can be used to filter, group, summarize, rearrange, and transform all types of data sets. For those just starting their NFL analytics endeavors in the R programming language, the following four `dplyr` verbs are perhaps the most important. Specific examples of working with these verbs, as well as others, follow below.

1.  `filter`: the `filter` verb allows you to subset data based on certain criteria. For example, you can use `filter()` to keep only those rows in a data set where a certain variable meets a certain conditions (ie., more than 100 completed passes). Moreover, the `filter` verb can be used in conjunction with logical operators such as `&` and `|` to create more complex criteria.
2.  `group_by`: the `group_by()` verb allows you to group a data set by one or more variables. It is a useful tool when you want to perform an operation on each group, such as calculating a summary statistic (ie, intended air yards per quarterback) or when creating a plot.
3.  `summarize`: the `summarize()` verb allows you to reduce a data set to a single summary value. The `summarize()` verb is often used in conjunction with the `group_by` function, allowing you to group the data by one or more variables. The `summarize()` verb allows for a wide range of summary statistics, including means, medians, standard deviations, and more. You can also use it to calculate custom summary statistics.
4.  `` `mutate` ``: the `mutate` verbs allows you to create new variables within your data while also preserving existing ones.

### NFL Data and the `filter()` verb

ffffff

<center>

| Logical Operator |          Meaning          |
|:----------------:|:-------------------------:|
|       `==`       |         equal to          |
|       `!=`       |       not equal to        |
|       `<`        |         less than         |
|       `<=`       |   less than or equal to   |
|       `>`        |       greater than        |
|       `>=`       | greater than or equal to  |
|       `!`        |            not            |
|       `&`        |            and            |
|       `|`        |            or             |
|      `%in%`      |         includes          |
|     `is.na`      | checks for missing values |

</center>

### NFL Data and the `group_by()` verb

As mentioned above, the `group_by` verb allows you to group data by one or more specific variables in order to conducted, among other actions, summary statistics. To showcase how `group_by` is used within the `nflverse` data, let's first gather the 2022 regular season statistics and then use the `summarize` verb to get the average success rate on rushing plays.

As well, we immediately make use of the `filter` function to do sort the data : (1.) we first instruct to filter the data to include just those instances where the `play_type` equals `run`, (2.) we then say it must also be `play == 1`, meaning there was no penalty or other interruption that "cancelled out" the play, and (3.) we lastly pass the argument that the `down` cannot be missing by using `!is.na` as a missing down is indicative of a two-point conversion attempt.

```{r loading-data, eval = TRUE, echo = FALSE, output = FALSE}

pbp <- nflreadr::load_pbp(2022)
```

```{r success-rate-without-group, output = TRUE}

rushing_success_ungrouped <- pbp %>%
  filter(play_type == "run" & play == 1 & !is.na(down)) %>%
  summarize(success_rate = mean(success))

rushing_success_ungrouped
```

Without including the `group_by` verb within the above code, the output is the average success rate for rushing plays for all 32 NFL teams, wherein success rate is the percentage of rushing plays that resulted in an EPA above zero. In this case, approximately 43% of NFL rushes had a positive success rate.

That said, we are interested in examining the success rate by team, not league-wide average. To do so, we add the `posteam` variable into the `group_by` verb.

```{r success-rate-with-grouping, output = TRUE}

rushing_success_grouped <- pbp %>%
  filter(play_type == "run" & play == 1 & !is.na(down)) %>%
  group_by(posteam) %>%
  summarize(success_rate = mean(success)) %>%
  arrange(-success_rate)

rushing_success_grouped %>%
  slice(1:10)
```

In the above example, we have added the offensive team into the `group_by` verb, while also arranging the data in descending order by `success_rate`, and then used `slice` to gather just the ten teams with the highest rushing success rate. The Philadelphia Eagles led the NFL in rushing success rate during the 2022 NFL regular season at 52.3%. By removing the `slice` function in the above example, we can see that Tampa Bay maintained the worst rushing success rate in the league at 37.3%.

While determining the rushing success rate of teams is interesting, we can also determine the same metric for individual running backs as well. To do so, we simply replace the variable in the `group_by` verb. In the below example, we replace the `posteam` variable with the `rusher` variable to see which running backs have the highest success rate.

```{r running-back-success, output = TRUE}

running_back_success <- pbp %>%
  filter(play_type == "run" & play == 1 & !is.na(down)) %>%
  group_by(rusher) %>%
  summarize(success_rate = mean(success)) %>%
  arrange(-success_rate)

running_back_success %>%
  slice(1:10)
```

The output, unfortunately, is not all that helpful. Because we did not use the `filter` verb to stipulate a minimum number of rushing attempts, the output is saying that - for example, Daniel Bellinger, a tight end, has among the most successful rushers in the league with a 100% rushing success rate. To correct this, we must add a second metric to our `summarize` verb (we will call it `n_rushes`) and then use the `filter` verb afterwards to include a minimum number of rushes required to be included in the final output.

As well, we will provide an additional argument in the first `filter` verb that stops the output from including any rushing attempt that does not include the running back's name. The `n_rushes()` in the `summarize` verb allows use to now include the number of attempts, per individual rusher, that fall within the first `filter` parameter. Afterwards, we include a second `filter` argument to include just those rushers with at least 200 attempts.

```{r running-back-success-min, eval = TRUE, echo = TRUE, output = TRUE}

running_back_success_min <- pbp %>%
  filter(play_type == "run" & play == 1 & !is.na(down) & !is.na(rusher)) %>%
  group_by(rusher) %>%
  summarize(success_rate = mean(success), n_rushes = n()) %>%
  filter(n_rushes >= 200) %>%
  arrange(-success_rate)

running_back_success_min %>%
  slice(1:10)
```

Unsurprisingly, Miles Sanders - a running back for the Eagles, who lead the NFL in team success rate - is the leader in rushing success among individual players with 49% of his attempts gaining positive EPA.

### NFL Data and the `summarize()` verb

As we've seen, the `summarize` function can be used to find summary statistics based whichever option we pass to it via the `group_by` verb. However, it can also be used to create new metrics built off data included in the `nflverse` play-by-play data.

For example, let's examine which teams were the most aggressive on 3rd and short passing attempts during the 2022 season. Of course, determining our definition of both what "short" is on 3rd down and "aggressive" is quite subjective. For the purposes of this example, however, let's assume that 3rd and short is considered 3rd down with five or less yards to go and that "aggressive" is a quarterback's air yards being to, at minimum, the first-down marker.

Must like our above examples working with rushing success rate, we begin constructing the metric with the `filter` argument. In this case, we are filtering for just pass plays, we want the down to equal 3, the yards to go to be equal to or less than 5, we want it to be an official play, and we do not want it to be missing the down information. After the initial `filter` process, we include the `posteam` variable within our `group_by` verb.

In our `summarize` section, we are first getting the total number of times each team passed the ball on 3rd down with no more than five yards to go. After, we are creating a new `aggressiveness` column that counts the number of times a quarterback's air yards were, at minimum, the required yards for a first down. Next, we create another new column titled `percentage` that takes `aggressiveness` and divides it by `total`.

```{r team-aggressiveness, eval = TRUE, echo = TRUE, output = TRUE}

team_aggressiveness <- pbp %>%
  filter(play_type == "pass" & down == 3 & ydstogo <= 5 & play == 1 & !is.na(down)) %>%
  group_by(posteam) %>%
  summarize(total = n(),
            aggressiveness = sum(air_yards >= ydstogo, na.rm = TRUE),
            percentage = aggressiveness / total) %>%
  arrange(-percentage)

team_aggressiveness %>%
  slice(1:10)
```

The Las Vegas Raiders, based on our definitions, are the most aggressive passing team in the league on 3rd and short as just over 83% of their air yards were at - or past - the required yardage for a first down. On the other end of the spectrum, the New York Giants were the least aggressive team during the 2022 regular season, at 49.1%.

### NFL Data and the `mutate()` verb

In the our example above working with the `summarize` verb, our output includes only the information contained in our `group_by` and then whatever information we provided in the `summarize()` (such as `total`, `aggressiveness`, and `percentage`).

What if, however, you wanted to create new variables and then `summarize()` those? That is where the `mutate` verb is used.

As an example, let's explore individual quarterback's average completion percentage over expected for specific air yard distances. To start, we can attempt to do this simply by including both `passer` and `air_yards` in the `group_by` verb.

```{r qb-cpoe-no-mutate, eval = TRUE, echo = TRUE, output = FALSE}

airyards_cpoe <- pbp %>%
  group_by(passer, air_yards) %>%
  summarize(avg_cpoe = mean(cpoe, na.rm = TRUE))
```

Your output is going to include the `avg_cpoe` for each quarterback at each and every distance of `air_yards`. Not only is it difficult to find meaning in, but it would prove to be difficult - if not impossible - to visualize with `ggplot`. To correct this issue, we must use the `mutate` verb.

Rather than `summarize` the completion percentage over expected for each distance of `air_yards`, we can use the `mutate` verb to bundle together a grouping of distances. In the below example, we are using the `mutate` verb to create a new variable titled `ay_distance` using the `case_when` verb.

```{r qb-cpoe-mutate, eval = TRUE, echo = TRUE, output = FALSE}

airyards_cpoe_mutate <- pbp %>%
  filter(!is.na(cpoe)) %>%
  mutate(
    ay_distance = case_when(
      air_yards < 0 ~ "Negative",
      air_yards >= 0 & air_yards < 10 ~ "Short",
      air_yards >= 10 & air_yards < 20 ~ "Medium",
      air_yards >= 20 ~ "Deep")) %>%
  group_by(passer, ay_distance) %>%
  summarize(avg_cpoe = mean(cpoe))
```

With the `air_yards` data now binned into four different groupings, we can examine quarterbacks at specific distances.

```{r qb-cpoe-medium, output = TRUE}

airyards_cpoe_mutate %>%
  filter(ay_distance == "Medium") %>%
  arrange(-avg_cpoe) %>%
  slice(1:10)
```

## Core Skills for Tidy Data

### Importing and Examining Data

### Dealing with Missing Data

### Changing Variable Types

To start, let's examine data from Sports Info Solutions by reading in the data from this book's Git repository using the `Vroom` package.

```{r sis-boom-bust, eval = TRUE, echo = TRUE, output = FALSE}

sis_data <- vroom::vroom("https://raw.githubusercontent.com/bcongelio/nfl-analytics-with-r-book/origin/example_data/csv/sis_boom_bust.csv")
```

In this example, we want to prepare the data for visualization in Chapter 4. Specifically, we want to compare the the `Boom%` and the `Bust%` for each quarterback in the data, wherein the `Boom%` correlates to any pass attempt by the quarterback that resulted in an expected points add (EPA) of at least 1 and `Bust%` is any pass attempt that resulted in an EPA of at least -1.

To start, let's first implement what we learned in the above `Importing and Examining Data` section to clean and format the column names into a `tidy` structure, select the relevant columns for our pending visualization in Chapter 4, and then determine the data type for each column provided by SIS:

```{r sis-boom-bust-examine, eval = TRUE, echo = TRUE, output = TRUE}

sis_data <- sis_data %>%
  janitor::clean_names() %>%
  select(player, team, att, boom_percent, bust_percent)

str(sis_data)
```

After preparing the data, we can see that four columns (`player`, `team`, `boom_percent`, and `bust_percent`) are listed as a `character` data type while each quarterback's number of attempts (`att`) is listed as a numeric. While we do want both player and team names to be a character-based datatype, we need both the boom and bust percentage for each quarterback to be a numeric value rather than a character.

Moreover, the values in each `boom_percent` and `bust_percent` include a percentage sign at the tail-end.

Because of this, we have two issues we need to correct in order to eventually bring the data over to Chapter 4 for visualization:

1.  correctly changing the variable type for `boom_percent` and `bust_percent`
2.  removing the `%` from the end of each value in `boom_percent` and `bust_percent`

Much like many things while working in the R programming language, there exists more than one way to tackle the above issues. In this specific case, we will first look at a method that uses the `stringr` package before switching the columns from `character` to `numeric`. After, we will use a much easier method to do both at one time.

::: callout-note
It is important to note here that I do not necessarily endorse using the first method below to change variable types and to drop the percentage sign, as the second option is ideal.

However, it is important to see how both methods work as there could be a case, somewhere down the road when you are exploring and preparing data yourself, that the first option is the *only* or *best* option.

As mentioned, there are typically multiple ways to get to the same endpoint in the R programming language. Showcasing both methods below simply provides you more tools in your toolkit for tidying data.
:::

#### Method #1: Using `stringr` and Changing Variables

In the below example, we are creating an "example" database titled `sis_data_stringr` from our current iteration of SIS data and then pipe into the `mutate` verb (covered in more depth in the below `Creating New Variables` section). It is within this `mutate` verb that we can drop the percentage sign.

We first indicate that we are going to `mutate` our already existing columns (both `boom_percent` and `bust_percent`). After, we use an `=` sign to indicate that the following string is the argument to create or, in our case, edit the existing values in the column.

We then use the `str_remove` function from the `stringr` package to locate the `%` sign in each value and remove it.

Afterwards, we dive into base R (that is, not `tidyverse` structure) and use the `as.numeric` function to change both of the boom and bust columns to the correct data type (using the `$` sign to notate that we are working on just one specific column in the data.

```{r stringr-example-variables, eval = TRUE, echo = TRUE, output = TRUE}

sis_data_stringr <- sis_data %>%
  mutate(boom_percent = str_remove(boom_percent, "%"),
         bust_percent = str_remove(bust_percent, "%"))

sis_data_stringr$boom_percent <- as.numeric(as.character(sis_data_stringr$boom_percent))
sis_data_stringr$bust_percent <- as.numeric(as.character(sis_data_stringr$bust_percent))

str(sis_data_stringr)

```

Our output using `str()` now shows that `att`, `boom_percent`, and `bust_percent` are now all three numeric and, moreover, the tailing percentage sign in each boom and bust value is now removed. At this point, the data is prepped and properly structured for the visualization process.

#### Method #2: Using the `parse_number` Function in `readr`

While the above example using `stringr` and base R is suitable for our needs, we can use the `readr` package - and its `parse_number` function - to achieve the same results in a much less verbose manner.

```{r readr-examples-variables, eval = TRUE, echo = TRUE, output = TRUE}

sis_data <- sis_data %>%
  mutate(boom_percent = readr::parse_number(boom_percent),
         bust_percent = readr::parse_number(bust_percent))

str(sis_data)
```

The above example, using `readr`, is quite similar to the first version that used `stringr` in that we are using the `mutate` verb to edit our existing `boom_percent` and `bust_percent` columns. In this case, however, we are replacing our `stringer` argument with the `parse_number` function from the `readr` package. When finished, you can see in the `str()` output that not only are both boom and bust correctly listed as numeric, but the `parse_number` function automatically recognized and dropped the tailing percentage sign.

While both examples get us to the same endpoint, using the `readr` package allows us to get there with less work.

### Creating New Variables

### Writing Tidy Functions

### Merging Multiple Sets of Data
